---
marp: true
paginate: true
style: |
  section .red {
    color: red;
  }
  section img {
    object-fit: contain;
  }
---

<center><span style="font-size:90px; font-weight:700;">AQA</span></center>
<center>试图找出 <b>可解释</b> 版本</center>

---
<center>黎叔没跟我 cue 过组会时间 TAT （会在校蹲到二月初，本周有的话还能爬来）</center>

<center>上次黎叔说重点放在“可解释性” + 性能接近 SOTA</center>

<center>于是越查资料越自闭</center>

<img src="./img/crying.JPG" style="max-height:400px; margin-top:30px;"/>

---
# 目录

## Part A: 对四篇 paper 的复现尝试

## Part B: 对除 PECoP 外解决方案 “可解释性” 的小结

## Part C: 对毕设的不靠谱想法

  - 试图基于 IRIS 提出的方法进行魔改

    > Interpretable Rubric-Informed Segmentation for Action Quality Assessment

---

# Part A: 复现结果

---
<!-- footer: "Part A: 复现结果" -->

# 1 CoRe

### 1.1 AQA-7

> CoRe + GART

| Type | Sp.Corr | R-l2 (x100) |
| :---: | :-----: | :-------------: |
| 论文数据 | 0.8401 | 2.12 |
| 训练 60 Epoch | 0.840793 | 0.8214 |

---

# 1 CoRe (Cont.)

### 1.2 MTL-AQA

> CoRe + GART (with DD label)

| Type | Sp.Corr | R-l2 (x100) |
| :---: | :-----: | :-------------: |
| 论文数据 | 0.9512 | 0.260 |
| 作者公布的 Model | 0.953281 | 0.2463 |
| 训练 117 Epoch | 0.951908 | 0.2904 |

---

# 2 TSA-Net

## FR-FS

| Type | Acc.(论文) | Acc.(复现) |
| :---: | :-----: | :---: |
| Plain-Net | 94.23 | 95.19 |
| TSA-Net | 98.56 | 98.08 |

---

### 3 TPT

- 训练

  已经把 bath_size 降到 1 了还是内存不足 aaa

- 测试

  本来想把 nfs4-p1 下面学长您训练好的模型拿来测试一下

  但作者仓库里的 `test script` 至今仍是 TODO

---

# 4 PECoP

- 代码默认支持的数据库是 `PD4T`（需要填一个奇妙的 request form 申请表）

  然后这东西要求 students need a faculty member to submit a request on their behalf

- 尝试在 `MTL-AQA` 上进行训练

  但由于代码仅支持格式为 `[sample_name, action_label, num_frames, num_p]` 的数据标签，遂 abandon

  > AQA-7 同理（缺少对应格式的数据标签）

---
<!-- footer: "" -->

# Part B: (上述方案) 可解释性

> - 毕设的侧重点在 “可解释性” 来着
> - 但好像真的没有人在 AQA 上做可解释性（痛苦面具）

---
<!-- footer: "Part B: (上述方案) 可解释性" -->

1. CoRe
    - 使用 Grad-CAM 对作为主要判断依据的 region 进行可视化
2. TSA-Net
    - 可视化了 VOT 算法对运动员主体 tracking box 的识别结果
    - 拉踩 AlphaPose 的识别结果
3. TPT
    - 使用热力图对 Cross Attention 中各 Clip 获得的 attention responses 进行可视化
    - <span class="red">各 Part 使用相同的 MLP 预测 relative</span>
---
<!-- footer: "" -->

# Part C: 有想法，但不多

---
<!-- footer: "Part C: 有想法，但不多" -->

## 1 拉踩上述方案

1. CoRe: 使用 I3D 对 <span class="red">整个视频</span> 进行特征提取，无法定位两个视频具有较大差异的 <span class="red">具体时段</span> 

2. TSA-Net: Paper 本身没有可视化 Self-Attention Map，但可视化之后也无法定位两个视频具有较大差异的 <span class="red">具体时段</span> (?)

3. TPT: 
    - 需要手动指定 Query 的维度（需要划分的 Part 数量）
    - 使用同一 MLP 为各 Part 生成 Relative Representation，无法区分不同 Part 的侧重点
    - 最终使用 MLP 对 fusion 后的特征直接回归计算，不好定位具体时段
---

## 2 唐突引入文章

> IRIS: Interpretable Rubric-Informed Segmentation for Action Quality Assessment **(ACM-IUI 2023)** [[Paper\]](https://arxiv.org/pdf/2303.09097.pdf)

因为标题带 `Interpretable`，所以爬去看了

---

## 2.1 IRIS 创新点：Rubric-Informed

- 论文只解决“花样滑冰短节目”的 AQA 问题（仅使用了 MTL-Skate dataset）

- 所谓的 `Rubric-Informed`: 花样滑冰的评分由 各个动作得分 + 节目总体评分 构成

  - (TES) 各动作得分 = Difficult Degree (Base) + GOE (Offset)
  - (PCS) 总体评分（玄学）

  => 最终得分就是上述各项的代数和

---

## 2.2 IRIS: Approach

受花样滑冰项目评分规则 (Rubric) 影响，作者提出了以下模型：

1. 预测各动作的 Relative Score

   1. 使用 Multi-Scale TCN 对视频进行 Segmentation，并为每一帧打上 `Jump, Spin, Step Sequence, Transition` 标签中的一个
   2. 对四种标签各训练一个 CNN 模型，用于预测对应动作种类的 <span style="color:blue;">GOE</span>
   3. 输出最终 *动作得分* (<span style="color:blue;">下图中的 TES</span>) = DD + GOE 

---

2. 预测节目总体评分 (<span style="color:blue;">下图中的PCS</span>)

   使用 multi-task CNN 预测 *节目的总体评分* 中的各 subscore

3. 输出最终评分：前两个大项的代数和 (<span style="color:blue;">Score = TES + PCS</span>)

<img src="/Users/shen/Desktop/AQA/notebook/site/assets/IRIS visual.png" style="max-height:400px;">

---

## 2.3 IRIS 存在的问题

- 性能其实不太好（作者声称的 Sp. Corr = 0.82）

- 作者声称该方法具有泛化性，但只在 MIT-Skate Dataset 上进行验证

  - 由于数据较少，标签本身打得比较粗略（没有细分不同的跳跃/旋转动作类别）

  - 数据集较老，只包含2012年以前的比赛视频

- 直接用传统 CNN 预测基于 I3D 特征提取结果预测 GOE => 不好解释
    本质上也是直接对一整个 Clip 进行 Regression => 没有采用对比学习

- 代码和数据集都没有开源
    => 作者用来训练 Segmentation 模型的数据集是手工标注动作起始帧的
---

## 3 目前对毕设的思路

> 尝试缝合，并开始哭泣

- 打算和 IRIS 一样先聚焦于 “花样滑冰短节目” 这个下游任务

> 由于 基于骨架 / 基于姿态估计 的方法在上述论文中都是被拉踩的对象

- 计划：<u>先通过 I3D + 对比学习 定位差异最大的动作片段，再用骨架模型找差异</u>

  - 预测：使用 3D-CNN 提取特征（提高精度 orz）

  - 解释：使用 基于骨架 / 基于姿态估计 的方法强行解释

---

### Approach: 基于 IRIS 改动

1. 以 16 帧为一个 Clip（互不重叠，区分于 TPT），使用 I3D 提取特征

2. 使用 MS-TCN 为 Clip 打标签（`Jump, Spin, Step Sequence, Transition`之一）

    => MS-TCN 可能造成 over-segmentation，参考 IRIS 的方案对相邻区间进行合并

3. (?) 对每一种类型训练一个 MLP 来计算 relative score
    > 或者采用 CoRe 中提出的 Group-Aware Regression Tree

4. 选择 最好&最差 的两个 segment 进行具体分析 + 可视化

    > <span class="red">TODO：如何在两个 长度/采样频率 不同的视频中衡量同一种类动作的差异</span>
    > 目前基于骨架模型的 “行为识别” 基本是 时空图卷积-分类（可解释性又寄了）

---

4.[续] input 中 最好&最差 的片段记为 “对象A”
  1. 挑选具有相同 Different Degree 的 Segment 中 <u>得分最高的片段</u> 记为 “对象B”
  2. 将 A 和 B 放缩到相同长度（帧数）
      > 目前是打算把较长的一段中间抽掉几帧
  3. 从放缩后的 A & B 中检测骨架数据（关键点），并对骨架数据归一化、两片段骨架初始中心坐标对齐
      - 若 A&B 骨骼 <u>中心点的 水平/垂直 位移</u> 差异超过阈值，标红
        > 由于相机本身也在移动，大概得把横向位移忽略掉
      - 若 A&B 骨骼 <u>其他关键点构成的向量</u> 两帧之间的夹角差异超过阈值，标红

---

# Fin

- 然后就莫得想法了 TAT

- 在尝试看：

  - [Learning Semantics-Guided Representations for Scoring Figure Skating (TMM 2023)](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10301591&tag=1)
    企图通过语义分析拯救一下可解释性（感觉也不太妙）

  - [Skating-Mixer: Multimodal MLP for Scoring Figure Skating (AAAI 2023)](https://arxiv.org/pdf/2203.03990.pdf)
    提出了新的数据集 FS1000（比 MIT-Skate 包含更新的花样滑冰比赛视频）
    多模态：同时考虑了 视觉信息 & 音频信息