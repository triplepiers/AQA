{"config":{"lang":["en"],"separator":"[\\s\\u200b\\u3000\\-\u3001\u3002\uff0c\uff0e\uff1f\uff01\uff1b]+","pipeline":["stemmer"]},"docs":[{"location":"","title":"AQA: \u52a8\u4f5c\u8d28\u91cf\u8bc4\u4f30","text":"I'm MAAAAAAAD<pre><code>\u6211\u89c9\u5f97\u6211\u5feb\u98a0\u4e86\uff0c\u6bcf\u5929\u770b RGB-Based \u548c Skeleton-Based \u5de6\u53f3\u4e92\u640f\n\n\u7136\u540e\uff0c\u627e\u4e0d\u51fa\u4e00\u4e2a\u517c\u5177\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\u7684\u65b9\u6cd5\n</code></pre> <ul> <li>\u674e\u6e0a\u660e \u7684 GitHub \u4ed3\u5e93\uff1aAwesome AQA</li> </ul>"},{"location":"#1-requirement","title":"1 Requirement","text":"<p>Explainable learning for sports movement optimization</p> <p>\u5229\u7528\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u5bf9\u4f53\u80b2\u52a8\u4f5c\u8fdb\u884c\u6253\u5206\u4ee5\u5b9e\u73b0\u8fd0\u52a8\u5458\u6c34\u5e73\u7684\u63d0\u5347\u662f\u4e00\u4e2a\u503c\u5f97\u7814\u7a76\u7684\u95ee\u9898\uff1a</p> <ul> <li> <p>\u7136\u800c\uff0c\u7531\u4e8e\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u5dee\uff0c\u5bfc\u81f4\u6253\u5206\u53ea\u80fd\u505a\u5230\u77e5\u5176\u7136\u4e0d\u77e5\u5176\u6240\u4ee5\u7136\u3002</p> </li> <li> <p>\u672c\u8bfe\u9898\u63d0\u51fa \u4f53\u80b2\u52a8\u4f5c\u4f18\u5316\u7684\u53ef\u89e3\u91ca\u5b66\u4e60\uff1a</p> <p>\u5c1d\u8bd5\u7814\u7a76\u4e00\u79cd\u53ef\u89e3\u91ca\u7684\u6253\u5206\u6a21\u578b\uff0c\u4f7f\u8fd0\u52a8\u5458\u66f4\u5bb9\u6613\u7406\u89e3\u9700\u8981\u6539\u8fdb\u7684\u6280\u672f\u70b9\uff0c\u63d0\u9ad8\u8fd0\u52a8\u6210\u7ee9\u3002</p> </li> </ul>"},{"location":"#2-references","title":"2 References","text":""},{"location":"#2018","title":"2018","text":"<ul> <li> <p>ST-GCN: Spatial Temporal Graph Convolutional Networks for Skeleton-Based Action Recognition</p> <p>(AAAI 2018) [Code]</p> <ul> <li>\u57fa\u4e8e\u52a8\u6001\u9aa8\u9abc\u7684 \u52a8\u4f5c\u8bc6\u522b</li> </ul> </li> </ul>"},{"location":"#2021","title":"2021","text":"<ul> <li> <p>CoRE: Group-aware Contrastive Regression for Action Quality Assessment</p> <p>(ICCV 2021) [Paper] [Code]</p> <ul> <li> <p>\u4f7f\u7528 Grad-Cam \u5bf9\u5355\u5e27\u56fe\u50cf\u7684\u91cd\u70b9\u8fdb\u884c\u53ef\u89c6\u5316</p> </li> <li> <p>\u4f7f\u7528 I3D \u5bf9\u6574\u4e2a\u89c6\u9891\u5377\u79ef\u4e00\u6b21\uff0c\u7136\u540e\u4e22\u5230 GART \u91cc\u8fdb\u884c\u56de\u5f52\u8ba1\u7b97 \\(\\Delta s\\)</p> </li> </ul> </li> <li> <p>TSA-Net: Tube Self-Attention Network for Action Quality Assessment </p> <p>(ACM-MM 2021 Oral) [Paper] [Code]</p> <ul> <li> <p>\u8054\u5408\u4e4b\u524d\u7684\u8bba\u6587\u300aFineGym\u300b\u4e00\u8d77\u6279\u5224\u4e86 AlphaPose \u4e0d\u80fd\u6b63\u786e\u5b9e\u73b0 Pose Estimation</p> <p>\u5728\u672c\u6587\u4e2d\u91c7\u7528\u4e86 Tracking Box \u7b97\u6cd5 VOT</p> </li> <li> <p>\u4f7f\u7528 Self-Attention \u673a\u5236\u5229\u7528\u4e0a\u4e0b\u6587\u4fe1\u606f</p> </li> </ul> </li> <li> <p>A Survey of Video-based Action Quality Assessment</p> </li> </ul>"},{"location":"#2022","title":"2022","text":"<ul> <li> <p>TPT: Action Quality Assessment with Temporal Parsing Transformer [Code]</p> <ul> <li> <p>\u4f7f\u7528\u70ed\u529b\u56fe\u5bf9 cross attention \u8fdb\u884c\u53ef\u89c6\u5316\uff08\u6807\u51fa high attention responses\uff09\u7684 video clip</p> </li> <li> <p>\u4f7f\u7528\u7c7b Transformer \u67b6\u6784\u805a\u7126\u4e8e\u7ec6\u7c92\u5ea6\u7279\u5f81\uff08Part\uff09</p> <ul> <li> <p>\u57fa\u4e8e\u82e5\u5e72\u4e2a overlap \u7684 Clips \u751f\u6210 Part Representations\uff08\u957f\u5ea6\u4e00\u81f4\uff09</p> </li> <li> <p>\u5047\u8bbe\u6240\u6709\u89c6\u9891\u90fd\u53ef\u4ee5\u6309\u7167\u76f8\u540c\u7684\u987a\u5e8f\u8fdb\u884c\u9636\u6bb5\u5207\u5206\uff08\u5728 Diving \u4e2d\u53ef\u884c\uff09\uff0c\u5177\u4f53\u8868\u73b0\u4e3a queries \u7684\u603b\u6570</p> </li> </ul> </li> <li> <p>\u5bf9\u6bcf\u4e2a Part \u4f30\u8ba1 relative pairwise representation \uff0c\u5e76\u6700\u7ec8\u4f7f\u7528 AVG Pooling \u8fdb\u884c\u6574\u5408</p> <p>\u6240\u6709 Part \u5171\u4eab\u540c\u4e00\u4e2a MLP\uff08\u591a\u5c42\u611f\u77e5\u673a\uff09</p> </li> <li> <p>\u6700\u7ec8\u4f7f\u7528\u4e24\u5c42 MLP \u57fa\u4e8e relative pairwise representation \u9884\u6d4b relatiave score</p> </li> </ul> </li> </ul>"},{"location":"#2023","title":"2023","text":"<ul> <li> <p>PECoP: Parameter Efficient Continual Pretraining for Action Quality Assessment </p> <p>(WACV 2023 Oral) [Paper] [Code]</p> <ul> <li> <p>\u4e3a I3D Module \u5f15\u5165 3D-Adapter \u8fdb\u884c Continual Pretraining</p> <ul> <li> <p>\u66f4\u597d\u7684\u5b66\u4e60 domain-specific \u4efb\u52a1\u3001\u51cf\u5c11\u9700\u8981\u8c03\u6574\u7684\u53c2\u6570</p> </li> <li> <p>\u4f7f\u7528 VSPP (transform-based) \u4f5c\u4e3a SSL Pretext Task</p> </li> </ul> </li> <li> <p>\u8bc1\u660e\u5728 R3D \u4e2d\u5f15\u5165 3D-Adapter \u540c\u6837\u6709\u6548</p> </li> </ul> </li> <li> <p>FSPN: Fine-Grained Spatio-Temporal Parsing Network for Action Quality Assessment </p> <p>(TIP 2023) [Paper]</p> </li> <li> <p>IRIS: Interpretable Rubric-Informed Segmentation for Action Quality Assessment </p> <p>(ACM-IUI 2023) [Paper]</p> </li> <li> <p>HGCN: Hierarchical Graph Convolutional Networks</p> <p>(IEEE 2023) </p> </li> </ul>"},{"location":"FigureSkating/","title":"FigureSkating","text":""},{"location":"FigureSkating/#datasets","title":"DataSets","text":"<ul> <li> <p>MIT-Skate</p> </li> <li> <p>Fis-V</p> <p>S-LSTM + M-LSTM (with C3D) \u540c\u6837\u5c06 TES \u548c PCS \u56de\u5f52\u4f5c\u4e3a\u72ec\u7acb\u4efb\u52a1</p> <p>\u53ef\u89e3\u91ca\u6027</p> <p>S-LSTM \u7684 attention weight \u53ef\u4ee5\u8868\u660e\u4e0d\u540c clip \u5bf9\u6700\u7ec8\u9884\u6d4b\u503c\u7684\u8d21\u732e\u7a0b\u5ea6</p> <ul> <li> <p>S-LSTM \u7528\u4e8e\u5c40\u90e8\u4fe1\u606f\u5efa\u6a21:</p> <ul> <li>\u5e38\u89c1\u65b9\u6848\u4f7f\u7528\u6700\u5927\u6216\u5e73\u5747\u6c60\u5316\u8fd0\u7b97\u7b26\u5c06 C3D \u7279\u5f81\u5408\u5e76\u4e3a\u89c6\u9891\u7ea7\u8868\u793a</li> <li> <p>\u7136\u800c\uff0c\u5e76\u975e\u6240\u6709\u89c6\u9891\u7247\u6bb5/\u5e27\u5bf9\u56de\u5f52\u6700\u7ec8\u5206\u6570\u7684\u8d21\u732e\u76f8\u540c</p> <p>use self-attentive embedding scheme to generate the video-level representations.</p> </li> </ul> </li> <li> <p>M-LSTM\uff08\u591a\u5c3a\u5ea6\u5377\u79ef\u8df3\u8dc3\uff09\u91c7\u7528\u4e86\u51e0\u4e2a\u5177\u6709\u4e0d\u540c\u6838\u5927\u5c0f\u7684\u5e76\u884c1D\u5377\u79ef\u5c42\uff0c\u5b66\u4e60\u4ee5\u591a\u4e2a\u5c3a\u5ea6\u5efa\u6a21\u987a\u5e8f\u4fe1\u606f</p> <p>\u901a\u8fc7\u8fd9\u79cdM-LSTM\u67b6\u6784\uff0c\u6211\u4eec\u5b9e\u9645\u4e0a\u53ef\u4ee5\u517c\u987e\u4e24\u5168\u5176\u7f8e \u2014\u2014 \u591a\u5c3a\u5ea6\u5377\u79ef\u7ed3\u6784\u53ef\u4ee5\u4ece\u89c6\u9891\u4e2d\u63d0\u53d6\u5c40\u90e8\u548c\u5168\u5c40\u7279\u5f81\u8868\u793a</p> <ul> <li>\u4fee\u8ba2\u540e\u7684\u8df3\u8dc3LSTM\u53ef\u4ee5\u9ad8\u6548\u5730\u8df3\u8fc7/\u4e22\u5f03\u5b66\u4e60\u5c40\u90e8\u548c\u5168\u5c40\u4fe1\u606f\u4e2d\u4e0d\u91cd\u8981\u7684\u5197\u4f59\u4fe1\u606f</li> <li>\u4e0d\u540c\u5c3a\u5ea6\u7684\u6700\u7ec8LSTM\u8f93\u51fa\u4ecd\u7136\u88ab\u4e32\u8054\u5e76\u901a\u8fc7\u975e\u7ebf\u6027\u5168\u8fde\u63a5\u5c42\u8fdb\u884c\u5b66\u4e60\u56de\u5f52</li> </ul> </li> </ul> </li> <li> <p>RFSJ: a Replay Figure Skating Jumping dataset</p> <p>Fis-V \u628a\u91cd\u64ad\u7247\u6bb5\u526a\u6389\u4e86\uff0c\u4f46\u662f\u8fd9\u91cc\u4fdd\u7559\u4e86</p> </li> <li> <p>FR-FS: \u5224\u5b9a\u662f\u4e0d\u662f\u6454\u5012\u4e86</p> </li> <li> <p>2023 FineFS: Fine-grained Figure Skating dataset</p> <ul> <li> <p>\u7ec6\u7c92\u5ea6\u7684\u5206\u6570\u6807\u7b7e\uff0c\u4ece\u7c97\u5230\u7ec6\u7684\u6280\u672f\u5b50\u52a8\u4f5c\u7c7b\u522b\u6807\u7b7e\uff0c\u6280\u672f\u5b50\u52a8\u4f5c\u7684\u5f00\u59cb\u548c\u7ed3\u675f\u65f6\u95f4</p> <p>\u6700\u7c97\u7c92\u5ea6\u662f Spin, Jump, StepSeq\uff08\u6ca1\u6709\u5355\u72ec\u5212\u5206\u63a5\u7eed\u6b65\uff0c\u4f46\u4e5f\u5207\u6210\u4e867\u6bb5\uff09</p> <p>\u751a\u81f3\u7ed9\u6bcf\u4e2a\u52a8\u4f5c\u6807\u4e86 BV &amp; GOE\uff08\u6211\u54ed\u6b7b\uff09</p> </li> <li> <p>Video swin transformer (VST) \u63d0\u53d6\u7684\u89c6\u9891\u7279\u5f81 =&gt; \u5bf9\u6bd4 I3D \u548c C3D \u540e\u9009\u7528</p> </li> <li> <p>\u4f7f\u7528 MHForemer \u63d0\u53d6 \u7684 17-joints 3D \u9aa8\u67b6\u6a21\u578b\uff08\u5e76\u7ecf\u8fc7\u540e\u7eed\u624b\u52a8\u4fee\u6b63\uff09</p> </li> </ul> <p></p> <p>\u8bba\u6587 \u63d0\u51fa\u7684 \u5bf9 TES \u548c PCS \u5206\u522b\u8fdb\u884c\u4e86\u56de\u5f52\u9884\u6d4b</p> <ul> <li>\u4f7f\u7528 ACM-Net \u9010\u4e2a\u786e\u5b9a clip \u4e2d\u662f\u5426\u5305\u542b\u6280\u672f\u52a8\u4f5c\uff08\u4ec5\u5229\u7528\u89c6\u9891\u7ea7\u522b\u6807\u7b7e\u6765\u5b9a\u4f4d\u52a8\u4f5c\u5b9e\u4f8b\u7684\u65f6\u95f4\u8fb9\u754c\u5e76\u8bc6\u522b\u76f8\u5e94\u7684\u52a8\u4f5c\u7c7b\u522b\uff09</li> <li> <p>Uncertainty Score Disentanglement (USD)\uff1a\u4e0d\u786e\u5b9a\u6027\u5206\u6570\u89e3\u7f20\u3002</p> <p>\u5b66\u4e60\u72ec\u7acb\u7684\u9762\u5411PCS\u548c\u9762\u5411TES\u7684\u7279\u5f81\uff0c\u5e76\u5229\u7528\u8fd9\u4e9b\u7279\u5f81\u6765\u9884\u6d4bPCS\u548cTES\u3002</p> </li> <li> <p>\u901a\u8fc7 Encoder + \u7ebf\u6027\u56de\u5f52\u5668\u5206\u522b\u9884\u6d4b\u5206\u503c\uff08\u53c2\u8003\u4e86 uncertainty regression \u601d\u8def\uff09</p> </li> <li> <p>benchmarks: FineFS &amp; Fis-V \u4e0a TES &amp; PCS \u7684 Spearman corr.</p> </li> </ul> </li> </ul>"},{"location":"FigureSkating/#short-program","title":"Short-Program \u77ed\u8282\u76ee","text":"<p>\u77ed\u8282\u76ee \u89c4\u5b9a\u4e86\u52a8\u4f5c\u7ec6\u8282\u5185\u5bb9\uff1a3\u79cd\u8df3\u8dc3 + 3\u79cd\u65cb\u8f6c + 1\u79cd\u6b65\u6cd5 + \u63a5\u7eed\u6b65</p> <ul> <li>Spin \u4e3b\u8981\u5c31\u4e09\u79cd\uff1a\u8e72\u8e1e\u65cb\u8f6c\uff08sit spin\uff09\u3001\u71d5\u5f0f\u65cb\u8f6c\uff08camel spin\uff09\u548c\u76f4\u7acb\u65cb\u8f6c\uff08upright spin\uff09</li> <li>\u5b9a\u7ea7\u6b65\u6cd5\u53ef\u4ee5\u5206\u4e3a\uff1a\u8f6c\u4f53\u6b65\u6cd5\u548c\u6ed1\u884c\u6b65\u6cd5\uff0c\u6bd4\u8d5b\u4e2d\u88c1\u5224\u8981\u6839\u636e\u8fd0\u52a8\u5458\u7684\u52a8\u4f5c\u7f16\u6392\u548c\u4e34\u573a\u53d1\u6325\u786e\u5b9a\u7b49\u7ea7\u3002</li> <li>\u7f16\u6392\u6b65\u6cd5\u5305\u62ec\u4e09\u7c7b\uff1a\u6ed1\u884c\u6b65\u6cd5\u3001\u65cb\u8f6c\u6b65\u6cd5\u548c\u8df3\u8f6c\u6b65\u6cd5\u3002\uff08\u4f46\u5149\u662f\u6ed1\u884c\u6b65\u6cd5\u5c31\u4e00\u5806\uff09</li> </ul> \u7537\u5355 \u5973\u5355 \u65f6\u95f4 2min40s \\(\\pm\\) 10s 2min40s \\(\\pm\\)  10s Jump-1 \u4e24\u5468/\u4e09\u5468\u534a\u8df3\u8dc3 \u4e24\u5468/\u4e09\u5468\u534a\u8df3\u8dc3 Jump-2 \u5305\u542b\u6b65\u6cd5\u7684\u4e09\u5468\u8df3 \u5305\u542b\u6b65\u6cd5\u7684\u4e09\u5468\u8df3 Jump-3 3+2\u4ee5\u4e0a\u7684\u7ec4\u5408\u8df3 3+2\u4ee5\u4e0a\u7684\u7ec4\u5408\u8df3 Spin-1 \u8df3\u63a5\u65cb\u8f6c \u8df3\u63a5\u65cb\u8f6c Spin-2 \u4e00\u6b21\u6362\u811a\u7684\u71d5\u5f0f/\u8e72\u8f6c \u5f13\u8eab\u65cb\u8f6c Spin-3 \u4e00\u6b21\u6362\u811a\u7684\u7ec4\u5408\u65cb\u8f6c \u4e00\u6b21\u6362\u811a\u7684\u7ec4\u5408\u65cb\u8f6c \u6b65\u6cd5 \u4e24\u5957\uff0810\u8d77\u4e3a1\u5957\uff09 \u4e00\u5957\u666e\u901a + \u4e00\u5957\u87ba\u65cb\uff0810\u8d77\u53d6\u6d88\uff09"},{"location":"FigureSkating/#grading-policy","title":"Grading Policy","text":"<p>9 \u540d\u88c1\u5224\u5bf9 TES &amp; PCS \u8fdb\u884c\u6253\u5206\uff1a</p> <ul> <li>\u6bcf\u4e2a\u6280\u672f\u52a8\u4f5c\u7684\u8bc4\u5206 \\(score \\in [-5, +5]\\)\uff0c\u8be5\u52a8\u4f5c\u6700\u540e\u7684 \\(GOE = (\\sum^9_{i=1} score_i - MAX - MIN) * 0.1 * BV\\)</li> </ul> \\[ Score = TES_{\u6280\u672f\u5206}( BV_{\u57fa\u7840\u5206} + GOE_{\u6267\u884c\u5206} ) + PCS_{\u8282\u76ee\u5206} - \u6263\u5206\u9879 \\]"},{"location":"rep/","title":"\u590d\u73b0\u7ed3\u679c","text":""},{"location":"rep/#1-core","title":"1 CoRe","text":"Source Code <ul> <li> <p>Src</p> <ul> <li> <p>GitHub repo for Group-aware Contrastive Regression for Action Quality Assessment</p> </li> <li> <p>Kinetics pretrained I3D model</p> </li> </ul> </li> <li> <p>Dataset</p> <p>\u5f53\u524d\u4ee3\u7801\u4ec5\u652f\u6301\u5728 MTL-AQA / AQA-7 \u4e0a\u8fdb\u884c\u8bad\u7ec3</p> <ul> <li> <p>MTL-AQA</p> <ul> <li> <p>Original</p> </li> <li> <p>Prepared Dataset: \u767e\u5ea6\u4e91 (<code>smff</code>)</p> </li> </ul> </li> <li> <p>AQA-7</p> <ul> <li> <p>Original</p> <pre><code>mkdir AQA-Seven &amp; cd AQA-Seven\nwget http://rtis.oit.unlv.edu/datasets/AQA-7.zip\nunzip AQA-7.zip\n</code></pre> </li> <li> <p>Prepared Dataset: \u767e\u5ea6\u4e91 (<code>65rl</code>)</p> </li> </ul> </li> <li> <p>JIGSAWS</p> </li> </ul> </li> <li> <p>Prerained CoRe</p> <ul> <li>for MTL-AQA</li> </ul> </li> </ul> Prepareation <ul> <li> <p>\u670d\u52a1\u5668: <code>10.214.211.135</code></p> </li> <li> <p>\u51c6\u5907\u5de5\u4f5c\uff1a</p> <ul> <li> <p>\u9002\u7528\u4e8e <code>NVIDIA RTX A6000</code> \u7684 Pytorch \u73af\u5883</p> <pre><code>pip3 install torch==1.10.0+cu113 torchvision==0.11.1+cu113 torchaudio==0.10.0+cu113 -f https://download.pytorch.org/whl/cu113/torch_stable.html\n</code></pre> </li> <li> <p>\u8865\u5145\u4f9d\u8d56 <code>pyyaml</code> &amp; <code>torch_videovision</code></p> <pre><code>pip install git+https://github.com/hassony2/torch_videovision\n</code></pre> </li> <li> <p>\u4e0b\u8f7d\u9884\u8bad\u7ec3 I3D backbone</p> <pre><code>wget https://github.com/hassony2/kinetics_i3d_pytorch/raw/master/model/model_rgb.pth\n</code></pre> </li> <li> <p><code>~/CoRe/MTL-AQA/new/new_total_frames_256s</code> \u8f6f\u94fe\u63a5\u81f3 <code>/nfs4-p1/zzy/AQA/CoRe-MTL/*</code></p> </li> <li> <p>\u5c06 <code>/nfs4-p1/zzy/AQA</code> \u4e2d\u7684 Seven.tar \u590d\u5236\u81f3\u7528\u6237\u8def\u5f84\uff0c\u5e76\u4f7f\u7528 <code>tar -xf [tarFile]</code> \u89e3\u538b\uff0c\u540e\u521b\u5efa\u8f6f\u94fe\u63a5</p> </li> </ul> </li> </ul>"},{"location":"rep/#_2","title":"\u6a21\u578b\u8bad\u7ec3","text":"<ul> <li> <p>On MTL-AQA</p> <pre><code>bash ./scripts/train.sh 0,1 MTL try\n</code></pre> <ul> <li>\u8dd1\u4e86\u4e24\u5929\u591a\uff0c\u7136\u540e\u5728\u7b2c 118 Epoch \u56e0\u4e3a\u5fd8\u8bb0\u63d2\u7535\u7136\u540e\u5bc4\u4e86\uff0c\u8fd9\u662f Training Log</li> </ul> <p> \u7b2c\u4e00\u6761 log\uff0c\u6709\u4e00\u79cd\u201c\u5b69\u5b50\u751f\u4e86\u201d\u7684\u7f8e\u611f</p> <ul> <li> <p>giao \u6211\u4ee5\u4e3a\u65ad\u4e86\u5c31\u6ca1\u4e86\uff0c\u8bfb\u4e86\u4e0b src \u53d1\u73b0\u8fd8\u662f\u6709\u4e2d\u95f4\u5b58\u6863\u7684\uff08\u4f60\u4e0d\u8f93\u51fa\u8c01\u77e5\u9053\u554a\uff09</p> <p>\u6a21\u578b\u5728 <code>~/CoRe/experiments/CoRe_RT/MTL/try/best.pth</code></p> </li> </ul> </li> <li> <p>On AQA-7</p> <p>\u4e3a\u4e86\u80fd\u5feb\u70b9\u8dd1\u5b8c\uff0c\u628a MAX_EPOCH \u6539\u6210 60 \u4e86</p> <pre><code>bash ./scripts/train.sh 0,1 Seven try --Seven_cls 1\n</code></pre> <ul> <li>\u4fee\u6539\u4e86 MAX_EPOCH \u8dd1\u4e86 60 \u4e2a Epoch\uff0c\u8fd9\u662f Training Log</li> </ul> </li> </ul>"},{"location":"rep/#_3","title":"\u6a21\u578b\u9a8c\u8bc1","text":"<ul> <li> <p>On MTL-AQA</p> <ol> <li> <p>\u4f5c\u8005\u7684\u9884\u8bad\u7ec3\u6a21\u578b</p> <pre><code>bash ./scripts/test.sh 0 MTL try --ckpts ./MTL_CoRe.pth\n</code></pre> \u9a8c\u8bc1\u7ed3\u679c<pre><code>ckpts @ 59 epoch( rho = 0.9541, L2 = 25.6865 , RL2 = 0.0024)\n\n[TEST] correlation: 0.953281, L2: 26.893236, RL2: 0.002463\n</code></pre> </li> <li> <p>\u81ea\u5df1\u8dd1\u4e86 117 \u4e2a Epoch \u4e2d\u9053\u5d29\u5f82\u7684\u6a21\u578b (2fcd)</p> <pre><code># \u53ea\u662f\u6539\u4e86\u4e00\u4e0b\u53c2\u6570\u8def\u5f84\nbash ./scripts/test.sh 0 MTL try --ckpts ./experiments/CoRe_RT/MTL/try/best.pth\n</code></pre> \u9a8c\u8bc1\u7ed3\u679c<pre><code>ckpts @ 108 epoch( rho = 0.9534, L2 = 30.5425 , RL2 = 0.0028)\n\n[TEST] correlation: 0.951908, L2: 31.714521, RL2: 0.002904\n</code></pre> </li> </ol> </li> <li> <p>On AQA-7 (\u81ea\u5df1\u8dd1\u4e86 60 \u4e2a Epoch \u7684\u6a21\u578b)</p> <pre><code>bash ./scripts/test.sh 0 MTL try --ckpts ./experiments/CoRe_RT/Seven/try/best.pth\n</code></pre> \u9a8c\u8bc1\u7ed3\u679c<pre><code>ckpts @ 58 epoch( rho = 0.8681, L2 = 63.5571 , RL2 = 0.0064)\n\n[TEST] correlation: 0.840793, L2: 89.697052, RL2: 0.008214\n</code></pre> </li> </ul>"},{"location":"rep/#2-tsa-net","title":"2 TSA-Net","text":"Source Code <ul> <li> <p>Src</p> <ul> <li> <p>GitHub repo for TSA-Net</p> </li> <li> <p>Kinetics pretrained I3D model (i3dm)</p> </li> </ul> </li> <li> <p>Dataset: FR-FS (star)</p> <p>\u65b0\u7684\u4e00\u5929\u4ece\u53d1\u73b0\u5b66\u957f\u6ca1\u4e0b\u8fc7\u8fd9\u4e2a dataset \u5f00\u59cb\u5d29\u6e83</p> </li> </ul> Prepareation <ul> <li> <p>\u670d\u52a1\u5668: <code>10.214.211.137</code></p> </li> <li> <p>\u51c6\u5907\u5de5\u4f5c\uff1a</p> <ul> <li> <p>\u5728 <code>~/dataset</code> \u4e2d\u4e0b\u8f7d FR-FS \u6570\u636e\u96c6\uff0c\u5e76\u5efa\u7acb\u8f6f\u94fe\u63a5\u81f3 <code>/TSA/data/FRFS</code></p> </li> <li> <p>\u5728 <code>~/TSA/data</code> \u8def\u5f84\u4e0b\uff0c\u4e0b\u8f7d\u9884\u8bad\u7ec3 backbone</p> </li> <li> <p><code>AttributeError: module 'distutils' has no attribute 'version'</code></p> <p>tensordBoard \u7248\u672c\u4e0e setuptool \u4e0d\u5339\u914d\uff0c\u91cd\u88c5 <code>setuptools==58.0.4</code></p> </li> <li> <p><code>Descriptors cannot be created directly.</code></p> <p>\u5c06 protobuf \u964d\u7ea7\u81f3 3.20.X</p> </li> </ul> </li> </ul>"},{"location":"rep/#_4","title":"\u6a21\u578b\u8bad\u7ec3","text":"\u4f7f\u7528 nohup \u8fdb\u884c\u540e\u53f0\u8bad\u7ec3<pre><code># train TSA-Net &gt;&gt; TSA.log\nnohup python test.py --gpu 0 --pt_w ~/TSA/Exp/TSA-USDL/best.pt --TSA &gt;&gt; TSA.log  2&gt;&amp;1 &amp;\n# train Plain-Net &gt;&gt; plain.log\nnohup python test.py --gpu 0 --pt_w ~/TSA/Exp/USDL/best.pt &gt;&gt; plain.log  2&gt;&amp;1 &amp;\n</code></pre> <p>\u8bad\u7ec3\u7ed3\u675f\u540e\u5c06\u5728\uff1a<code>~/TSA/Exp/TSA-USDL</code> \u548c <code>~/TSA/Exp/USDL</code> \u4ea7\u751f\u5bf9\u5e94\u7684 <code>best.pt</code>\uff08\u5176\u5b9e\u8fd8\u6709 <code>train.log</code>\uff09</p>"},{"location":"rep/#_5","title":"\u6a21\u578b\u9a8c\u8bc1","text":"WTM \u7b11\u6b7b\uff0c\u4ed6\u8fd9\u4e2a test \u7684\u4ee3\u7801\u6709 bug <p>\u53e6\u5916\u5b83 <code>README</code> \u91cc\u7ed9\u7684\u6d4b\u8bd5\u4ee3\u7801\u662f\u4e0d\u5bf9\u7684: </p> <p><code>weight_logger</code> \u7528\u7684\u4e0d\u662f\u76f8\u5bf9 CWD \u7684\u8def\u5f84 &amp; <code>train.py</code> \u751f\u6210\u7684\u6a21\u578b\u540e\u7f00\u662f <code>.pt</code> (\u800c\u4e0d\u662f <code>.pth</code>)</p> <ul> <li> <p>Bug Tip: <code>AttributeError: 'Namespace' object has no attribute 'type'</code></p> </li> <li> <p>Fix:</p> <pre><code># \"/home/syf/TSA/test.py\", line 206\n# from\nbase_logger = get_logger(f'{args.model_path}/{args.type}.log', args.log_info)\n\n# to (\u968f\u4fbf\u8d77\u5565\u540d\u90fd\u884c)\nbase_logger = get_logger(f'{args.model_path}/test.log', args.log_info)\n</code></pre> </li> </ul> \u7528 nohup \u6302\u540e\u53f0<pre><code>nohup python test.py --gpu 0 --pt_w Exp/TSA-USDL/best.pth --TSA &gt;&gt; TSA.log  2&gt;&amp;1 &amp;\nnohup python test.py --gpu 0 --pt_w Exp/USDL/best.pth &gt;&gt; plain.log  2&gt;&amp;1 &amp;\n</code></pre> <p>\u4e0b\u9762\u662f\u9a8c\u8bc1\u65f6\u7684\u901a\u7528\u53c2\u6570\uff1a</p> <pre><code>dp: False                           weight_decay: 1e-05\nnum_workers: 8                      num_epochs: 20\ntest_batch_size: 8                  temporal_aug: 0\nstd: 5                              lr: 0.0001\n</code></pre> <p>\u9a8c\u8bc1\u7ed3\u679c</p> <p> Model Accuracy TSA-Net 98.08% Plain-Net 95.19% <p></p>"},{"location":"rep/#3-tpt","title":"3 TPT","text":"Source Code <ul> <li> <p>Src</p> <ul> <li>Github repo for TPT</li> </ul> </li> <li> <p>\u51c6\u5907\u5de5\u4f5c</p> <p>\u56e0\u4e3a\u7528\u7684\u662f CoRe \u7684\u540c\u6b3e\u6570\u636e\u96c6\uff0c\u6240\u4ee5\u5728 135 \u90a3\u53f0\u670d\u52a1\u5668\u4e0a</p> <p>\u597d\u7684\u56e0\u4e3a\u663e\u5361\u5185\u5b58\u4e0d\u591f\uff0c\u6362\u5230 137 \u4e86</p> <ol> <li> <p>\u5728\u9879\u76ee\u6839\u76ee\u5f55\u4e0b\u521b\u5efa\u8def\u5f84 <code>[TPT_root]/data/</code></p> </li> <li> <p>\u6309\u7167\u6307\u5b9a\u683c\u5f0f\u521b\u5efa\u8f6f\u94fe\u63a5</p> @ [TPT_root]/data<pre><code>ln -s /home/syf/CoRe/MTL-AQA/model_rgb.pth ./model_rgb.pth\nln -s /nfs4-p1/zzy/AQA/AQA-TPT/data_preprocessed ./data_preprocessed\n</code></pre> </li> <li> <p>\u8865\u5145\u4f9d\u8d56 <code>einops, pandas, tensorboard, tqdm, matplotlib, seaborn</code></p> </li> <li> <p>\u8bbe\u7f6e\u73af\u5883\u53d8\u91cf <code>export PYTORCH_CUDA_ALLOC_CONF=\"max_split_size_mb:2048\"</code></p> <p>2048 \u8fd8\u662f\u4e0d\u591f</p> </li> </ol> </li> </ul>"},{"location":"rep/#_6","title":"\u6a21\u578b\u8bad\u7ec3","text":"<ul> <li> <p>On MTL-AQA</p> <pre><code># \u9ed8\u8ba4\u7aef\u53e3 29500 already in use\uff0c\u6b64\u5904\u6307\u5b9a\u7aef\u53e3 29501\n# \u9010\u6e10\u653e\u5f03\u591a\u5361\u8bad\u7ec3 =&gt; \u6211\u8d85\uff0c\u52a8\u4e86\uff01\u6211\u6c38\u8fdc\u7231 137 \u8fd9\u53f0\u673a\u5b50\nnohup python -u -m torch.distributed.launch --nproc_per_node=1 --master_port 29501 train_pairencode1_decoder_1selfatt_self8head_ffn_sp_new.py --epoch_num=250 --dataset=MLT_AQA --bs_train=2 --bs_test=2 --use_pretrain=False --num_cluster=5 --margin_factor=3.2 --encode_video=False --hinge_loss=True --multi_hinge=True --d_model=512 --d_ffn=512 --exp_name=sp_new_103_5_3 &gt;&gt; MTL.log  2&gt;&amp;1 &amp;\n</code></pre> <p>\u964d\u5230 batch_size = 2 \u8fd8\u662f\u5185\u5b58\u4e0d\u591f\uff0c\u4ed9\u901d\u4e86</p> </li> </ul>"},{"location":"rep/#_7","title":"\u6a21\u578b\u9a8c\u8bc1","text":"<p>\u4f5c\u8005\u6682\u672a\u63d0\u4f9b test script\uff0c\u8e6d\u4e00\u4e0b\u9694\u58c1 CoRe \u7684</p> <ul> <li> <p>On MTL-AQA</p> <pre><code># @ \uff5e/CoRe\npython test.py --gpu 0 --pt_w [path_to_model] --TSA\n</code></pre> </li> </ul>"},{"location":"rep/#4-pecop","title":"4 PECoP","text":"Source Code <ul> <li> <p>Src</p> <ul> <li> <p>GitHub repo for PECoP</p> </li> <li> <p>Kinetics pretrained I3D</p> </li> </ul> </li> <li> <p>Dataset</p> <ul> <li> <p>PD4T (\u9700\u8981\u586b\u5199\u7533\u8bf7\u8868orz)</p> </li> <li> <p>MTL-AQA (CoRe ver.): \u8f6f\u94fe\u63a5\u81f3 <code>~/dataset/MTL-AQA</code></p> </li> </ul> </li> </ul> Prepareation <ul> <li> <p>\u670d\u52a1\u5668: <code>10.214.211.137</code></p> </li> <li> <p>\u51c6\u5907\u5de5\u4f5c\uff1a</p> <ul> <li> <p>\u8865\u5145\u4f9d\u8d56 <code>tensorboardX, torch_videovision, ptflops</code></p> <pre><code>pip install git+https://github.com/hassony2/torch_videovision\npip install ptflops\n</code></pre> </li> <li> <p>\u5728 <code>~/PECoP</code> \u8def\u5f84\u4e0b\uff0c\u4e0b\u8f7d\u9884\u8bad\u7ec3 I3D backbone\uff08\u540c CoRe\uff09</p> </li> </ul> </li> </ul>"},{"location":"rep/#_8","title":"\u6a21\u578b\u8bad\u7ec3","text":"<ul> <li> <p>On MTL-AQA</p> <p>\u4fee\u6539\u4e86 <code>data_list</code> (\u6570\u636e\u6807\u7b7e\u6587\u4ef6\u8def\u5f84) &amp; <code>rgb_prefix</code> (\u6570\u636e\u96c6\u8def\u5f84)</p> <pre><code>python train.py --bs 16 --lr 0.001 --height 256 --width 256 --crop_sz 224 --clip_len 32 --rgb_prefix /home/syf/dataset/MTL-AQA/ --data_list /home/syf/dataset/MTL-AQA/final_annotations_dict_with_dive_number.pkl\n</code></pre> <p>\u7f3a\u5c11\u5fc5\u8981\u7684 list \u6587\u4ef6</p> <p><code>./datasets/ucf101.py</code> \u4e2d\u89c4\u5b9a\u5bf9\u5e94\u7684 list (UTF-8)\u6587\u4ef6\u683c\u5f0f\u4e3a:</p> <pre><code>sample_name(str)   action_label(int)    num_frames(int)    num_p(str)\n</code></pre> <p>\u56e0\u4e3a MTL-AQA \u7f3a\u5c11\u5bf9\u5e94\u683c\u5f0f\u7684\u6587\u4ef6\uff0c\u6682\u65f6\u6ca1\u6cd5\u52a8</p> </li> </ul>"},{"location":"sum/Constrastive/","title":"Pair-wise \u5bf9\u6bd4\u5b66\u4e60","text":""},{"location":"sum/Constrastive/#2021-core","title":"2021: CoRe","text":""},{"location":"sum/Constrastive/#1-abstract","title":"1 Abstract","text":"<p>\u5bf9\u6bd4\u5b66\u4e60 Constrastive Learning</p> <p>\u5b66\u4e60\u4e00\u4e2a representation space\uff0c\u901a\u8fc7\u6bd4\u8f83\u4e24\u4e2a\uff08same class\uff09\u6837\u672c\u5728 representation space \u4e2d\u7684\u8ddd\u79bb\u6765\u8861\u91cf\u5176 \u8bed\u4e49\u8054\u7cfb (semantic relationship)</p> <p>\u56e0\u4e3a\u53ea\u6709\u540c\u7ec4\u7684\u624d\u80fd\u76f4\u63a5\u5bf9\u6bd4\uff0c\u6240\u4ee5\u9700\u8981 Group-aware</p> <p>\u672c\u6587\u8bad\u7ec3\u6a21\u578b\u7528\u4e8e \u56de\u5f52\u9884\u6d4b relative score \u5e76\u5bf9\u4e24\u4e2a video \u7684 score \u8fdb\u884c\u5bf9\u6bd4\uff0c\u4ece\u800c\u5b66\u4e60\u5176 diff \u7528\u4e8e\u6700\u7ec8\u5206\u6570\u4f30\u8ba1</p> <ul> <li> <p>\u73b0\u6709\u5de5\u4f5c\uff1a</p> <p>\u4ece single video \u56de\u5f52\u5f97\u5230\u4e00\u4e2a quality score\uff08\u9488\u5bf9\u5355\u4e00\u89c6\u9891\u7684\u56de\u5f52\u4efb\u52a1\uff09\uff0c\u9762\u4e34\u4e09\u4e2a\u6311\u6218\uff1a</p> <ol> <li> <p>Score Label \u662f\u7531\u4eba\u7c7b\u88c1\u5224\u7ed9\u51fa\u7684\uff0c\u5176\u4e3b\u89c2\u8bc4\u5224\u4f1a\u7ed9\u5206\u6570\u4f30\u8ba1\u9020\u6210\u6781\u5927\u56f0\u96be</p> <p>suffter a lot from large inter-video score variation</p> </li> <li> <p>video \u4e4b\u95f4\u7684\u5dee\u5f02\u662f\u5341\u5206\u5fae\u5c0f\u7684\uff1a\u8fd0\u52a8\u5458\u5f80\u5f80\u5728\u76f8\u4f3c\u7684\u73af\u5883\u4e0b\u4f5c\u51fa\u7c7b\u4f3c\u7684\u52a8\u4f5c</p> </li> <li> <p>\u5927\u591a\u6570\u89e3\u51b3\u65b9\u6848\u4f7f\u7528 Spaerman's Rank \u8fdb\u884c\u8bc4\u4f30\uff0c\u8fd9\u79cd\u65b9\u6cd5\u53ef\u80fd\u4e0d\u80fd\u6b63\u786e\u53cd\u5e94\u8bc4\u4f30\u8868\u73b0</p> </li> </ol> </li> <li> <p>\u521b\u65b0\u70b9\uff1a</p> <ul> <li> <p>\u57fa\u4e8e reference to another video that has shared attributes \u8fdb\u884c\u5206\u6570\u9884\u6d4b\uff08\u4e0d\u662f\u76f4\u63a5\u4ece\u5355\u4e2a\u89c6\u9891\u4e0b\u624b\uff09</p> </li> <li> <p>\u63d0\u51fa\u4e86 CoRe\uff1a</p> <p>\u901a\u8fc7 pare-wise comparision\uff0c\u5f3a\u8c03\u89c6\u9891\u4e4b\u95f4\u7684 diff\uff0c\u5e76\u5f15\u5bfc\u6a21\u578b\u5b66\u4e60\u8bc4\u4f30\u7684 key hint</p> \\[     \\text{Video} \\rightarrow Model \\rightarrow \\text{Relative Score Space} \\] </li> <li> <p>\u901a\u8fc7 GART \u5c06\u4f20\u7edf\u7684\u5206\u6570\u56de\u5f52\u9884\u6d4b\u5212\u5206\u4e3a\u4e24\u4e2a\u5b50\u95ee\u9898\uff1a</p> <p>group-aware regression tree</p> <ol> <li> <p>coarse-to-fine classification: \u5224\u65ad \u201c\u597d\u574f\u201d \u7684\u4e8c\u5206\u7c7b</p> <p>\u5c06 relative score \u7a7a\u95f4\u5212\u5206\u6210\u591a\u4e2a\u76f8\u79bb\u533a\u95f4\uff0c\u968f\u540e\u901a\u8fc7\u4e8c\u53c9\u6811\u5c06 relavtive score \u5212\u5206\u5230\u5bf9\u5e94\u7684 group</p> </li> <li> <p>small interval regression</p> <p>\u5728 relative score \u88ab\u9884\u6d4b\u7684 class \u5185 \u9884\u6d4b final score</p> </li> </ol> </li> <li> <p>\u63d0\u51fa Relative L2-distance\uff0c\u5728\u6a21\u578b\u8bc4\u4f30\u4e2d\u8003\u8651\u7ec4\u95f4\u5dee\u5f02</p> </li> </ul> </li> </ul>"},{"location":"sum/Constrastive/#2-relative-works","title":"2 Relative Works","text":"<ul> <li> <p>Gordan\uff1a\u4f7f\u7528\u9aa8\u67b6\u8f68\u8ff9\u8bc4\u4f30\u4f53\u64cd\u8df3\u8dc3\u52a8\u4f5c\u8d28\u91cf</p> </li> <li> <p>Pirsiavash\uff1a</p> <ul> <li> <p>\u79bb\u6563\u4f59\u5f26\u53d8\u6362\uff08DCT\uff09\u88ab\u7528\u4e8e\u4ece body pose \u56fe\u50cf\u5230 input feature \u7684\u7f16\u7801</p> </li> <li> <p>SVR \u88ab\u7528\u4e8e\u4ece feature \u5230 final score \u7684\u6620\u5c04</p> </li> </ul> </li> <li> <p>Parmar\uff1a\u4f7f\u7528 C3D \u5bf9\u89c6\u9891\u6570\u636e\u8fdb\u884c\u7f16\u7801\u4ee5\u83b7\u5f97 spatio-temporal features</p> </li> <li> <p>Xu\uff1a\u4f7f\u7528\u4e24\u4e2a LSTM \u6765\u5b66\u4e60 multi-scale feature</p> </li> <li> <p>Pan\uff1a</p> <ul> <li> <p>\u4f7f\u7528 spatial &amp; temporal relation graphs\uff0c\u5728\u5176\u76f8\u4ea4\u5904\u5efa\u6a21\uff08\uff1f\uff09</p> </li> <li> <p>\u4f7f\u7528 I3D \u63d0\u53d6 spatio-temporal features</p> </li> </ul> </li> <li> <p>Tang\uff1a\u63d0\u51fa USDL \u6765\u964d\u4f4e\u4eba\u7c7b\u88c1\u5224\u6253\u5206\u9020\u6210\u7684\u6b67\u4e49</p> <p>uncertainty-aware score distribution learning</p> </li> </ul>"},{"location":"sum/Constrastive/#3-approach","title":"3 Approach","text":""},{"location":"sum/Constrastive/#1-contrastive-regression","title":"1 Contrastive Regression","text":"<ul> <li> <p>Problem Formulation</p> <ul> <li> <p>\u5927\u591a\u6570\u65b9\u6cd5\u5c06 AQA \u89c6\u4e3a\u4e00\u4e2a\u56de\u5f52\u95ee\u9898\uff1a\u8f93\u5165\u5305\u542b\u76ee\u6807\u52a8\u4f5c\u7684 video\uff0c\u8f93\u51fa\u5bf9\u5e94\u7684 quality score</p> </li> <li> <p>\u90e8\u5206\u65b9\u6cd5\u63d0\u51fa\u4e86 difficulty-score\uff08\u5df2\u77e5\uff09\uff0c\u6700\u7ec8\u7ed3\u679c\u53d8\u4e3a qulity * difficulty</p> </li> </ul> <p>\u7531\u4e8e\u89c6\u9891\u5f80\u5f80\u5728\u76f8\u4f3c\u7684\u73af\u5883\u4e0b\u62cd\u6444\uff0c\u6a21\u578b\u5f88\u96be\u4ece\u5fae\u5c0f\u7684\u53d8\u5316\u4e2d\u5b66\u4e60\u5de8\u5927\u7684\u5206\u6570\u5dee\u5f02</p> <ul> <li> <p>\u91cd\u65b0\u5b9a\u4e49\u95ee\u9898\u4e3a\uff1a</p> <p>Regress relative score between the Input Video \\(v_m=\\{F_m^i\\}_{i=1}^{L_m}\\) &amp; an Exemplar \\(v_n=\\{F_n^i\\}_{i=1}^{L_n}\\) with Score Label \\(s_n\\)</p> </li> <li> <p>\u6211\u4eec\u53ef\u4ee5\u5c06\u56de\u5f52\u95ee\u9898\u5199\u4e3a\u5982\u4e0b\u5f62\u5f0f\uff1a</p> \\[ \\hat{s}_m = R_{\\Theta}(F_W(v_m), F_W(v_n)) + s_n \\] <p>\u5176\u4e2d \\(R_{\\Theta}\\) \u4e3a\u56de\u5f52\u6a21\u578b\uff0c\\(F_W\\) \u4e3a\u7279\u5f81\u63d0\u53d6\u6a21\u578b\uff08\u53c2\u6570\u76f8\u540c\u7684I3D\uff09</p> <p>\u8ba1\u7b97 CurInput &amp; Exampler \u4e4b\u95f4\u7684\u504f\u5dee\u503c\uff0c\u7136\u540e\u57fa\u4e8e Exampler \u7684\u5206\u6570\u8fdb\u884c\u751f\u6210\u8bc4\u5206</p> </li> </ul> </li> <li> <p>Exemplar-Based Score Regression</p> <p>\u5982\u4f55\u6311\u9009\u5177\u6709\u53ef\u6bd4\u6027\u7684 Exampler</p> <ol> <li> <p>\u4f7f\u7528 I3D \u6a21\u578b\u5bf9 input &amp; exampler \u8fdb\u884c\u7279\u5f81\u63d0\u53d6\uff0c\u5f97\u5230 \\(f_m, f_n\\)</p> </li> <li> <p>\u5c06\u4e24\u8005\u7279\u5f81\u4e0e exampler score \u8fdb\u884c\u805a\u5408\uff1a\\(f_{(n,m)} = concat([f_n, f_m, \\frac{s_n}{\\epsilon}])\\)</p> <p>\\(\\epsilon\\) \u662f\u4e00\u4e2a norm constant\uff0c\u7528\u4e8e\u786e\u4fdd \\(\\frac{s_n}{\\epsilon} \\in [0,1]\\)</p> </li> <li> <p>\u4f7f\u7528\u56de\u5f52\u6a21\u578b\u5bf9 \\(pair(n,m)\\) \u7684 score-diff \u8fdb\u884c\u9884\u6d4b\uff1a\\(\\Delta s = R_{\\Theta}(f_{(n,m)})\\)</p> </li> </ol> </li> </ul>"},{"location":"sum/Constrastive/#2-group-aware-regression-tree","title":"2 Group-Aware Regression Tree","text":"<ul> <li> <p>contrastive regression \u5b58\u5728\u7684\u95ee\u9898</p> <p>\u6bd4\u8f83\u56de\u5f52\u53ef\u4ee5\u9884\u6d4b relative score \\(\\Delta s\\)\uff0c\u4f46\u901a\u5e38\u5177\u6709\u8f83\u5927\u7684\u503c\u57df</p> <p>=&gt; \u76f4\u63a5\u9884\u6d4b \\(\\Delta s\\) \u5177\u6709\u4e00\u5b9a\u56f0\u96be</p> </li> <li> <p>GART \u5b9e\u73b0\u5206\u6cbb</p> <ol> <li> <p>\u5c06 \\(\\Delta s\\) \u7684\u503c\u57df\u5212\u5206\u4e3a 2<sup>d</sup> \u4e2a non-overlapping class</p> </li> <li> <p>\u5efa\u7acb \\(d-1\\) \u5c42\u7684 \u4e8c\u53c9\u56de\u5f52\u6811\uff08\u6709 2<sup>d</sup> \u4e2a leaf nodes\uff09</p> </li> <li> <p>\u5206\u7c7b</p> <p>\u6574\u4e2a\u8fc7\u7a0b\u9075\u5faa coarse-to-fine manner\uff1a\u7b2c\u4e00\u5c42\u51b3\u5b9a input \u6bd4 exampler \u597d/\u574f\uff0c\u5e76\u5728\u540e\u7eed\u5177\u4f53\u91cf\u5316 better/worse \u7684\u7a0b\u5ea6</p> </li> </ol> </li> <li> <p>Tree Architecture</p> <ul> <li> <p>Input\uff1a</p> <ul> <li> <p>Root Node: \u5c06\u805a\u5408\u7279\u5f81 \\(f_{(n,m)}\\) \u653e\u8fdb\u591a\u5c42\u795e\u7ecf\u7f51\u7edc MLP\uff0c\u5e76\u5c06 \\(\\text{MLP}(f_{(n,m)})\\) \u4f5c\u4e3a GART \u7684\u8f93\u5165</p> </li> <li> <p>Other Nodes: \u7236\u8282\u70b9\u7684\u8f93\u51fa</p> </li> </ul> </li> <li> <p>Output:</p> <ul> <li> <p>Internal Nodes: \u4ea7\u751f\u901a\u5411\u5de6\u53f3\u5b50\u8282\u70b9\u7684\u6982\u7387</p> <p>\u901a\u5411\u67d0\u4e2a\u5177\u4f53 leaf node \u7684\u6982\u7387 = \\(\\prod p_{\\text{nodes in path}}\\)</p> </li> <li> <p>Leaf Nodes: \\(sigmoid(P_{\\text{leaf}}) \\in [0,1]\\) \u4ee3\u8868\u4e86 Input \u548c\u5bf9\u5e94 class \u7684 score-diff</p> </li> </ul> </li> </ul> </li> <li> <p>\u8fb9\u754c\u5212\u5206\u7b56\u7565</p> <p>\u7b49\u957f\u5212\u5206 class \u4f1a\u5bfc\u81f4 unbalance</p> <ol> <li> <p>\u6536\u96c6\u6d4b\u8bd5\u96c6\u4e2d\u6240\u6709 pair \u7684 score-diff\uff1a \\(\\delta = [\\delta_1, ... ,\\delta_T]\\)\uff0c\u5bf9\u5176\u8fdb\u884c\u5347\u5e8f\u6392\u5217\u540e\u5f97\u5230 \\(\\delta^*\\)</p> </li> <li> <p>\u7ed9\u5b9a n_class = \\(R\\)\uff0c\u9075\u5faa\u4ee5\u4e0b\u7b56\u7565\u5f97\u5230\u5404\u7ec4\u8fb9\u754c \\(G^r = (min^r, max^r)\\):</p> \\[ \\begin{align*}     min^r &amp;= \\delta^*\\left[\\lfloor(T-1) \\times \\frac{r-1}{R}\\rfloor\\right] \\\\     max^r &amp;= \\delta^*\\left[\\lfloor(T-1) \\times \\frac{r}{R}\\rfloor\\right] \\end{align*} \\] <p>\u5176\u4e2d\uff0c\\(\\delta^*[i]\\) \u8868\u793a\u6570\u7ec4\u4e2d\u7684\u7b2c i \u4e2a\u5143\u7d20</p> </li> </ol> </li> <li> <p>\u4f18\u5316\u7b56\u7565</p> <p>GART \u5305\u542b classification &amp; in-class regression\uff0c\u6240\u4ee5\u603b\u7684 Cost \u8981\u62c6\u6210\u4e24\u90e8\u5206\u3002</p> <p>\u5f53 pair \\(\\delta\\) \u7684 score-diff \u88ab\u5206\u7c7b\u81f3 Class i \u65f6:</p> <ul> <li> <p>\u5c06 One-Hot \u5206\u7c7b\u6807\u7b7e\u7684\u5bf9\u5e94\u4f4d\u7f6e\u7f6e 1: \\(l[i] = 1\\) </p> </li> <li> <p>\u53e6 regression \u6807\u7b7e \\(\\sigma_i = \\frac{\\delta - min^i}{max^i - min^i}\\)</p> </li> </ul> <p>\u6b64\u65f6\uff0c\u6bcf\u4e2a pair \u90fd\u62e5\u6709 One-Hot Classification Label \\(l\\) &amp; Regression Label \\(\\sigma\\):</p> \\[ \\begin{align*} J_{cls} &amp;= - \\sum_{r=1}^{R}(l_r \\log(P_r) + (1 - l_r)\\log(1-P_r))\\\\ J_{reg} &amp;= \\sum_{r=0}^R(\\hat{\\sigma}_r - \\sigma_r)^2, \\text{where } l_r=1\\\\ J &amp;= J_{cls} + J_{reg} \\end{align*} \\] <p>\u5176\u4e2d \\(P_r, \\hat{\\sigma}_r\\) \u5206\u522b\u662f Leaf Probability &amp; Regression Result</p> </li> <li> <p>\u524d\u5411\u63a8\u5bfc\uff08\u4ec5 GART \u90e8\u5206\uff09</p> \\[ R_{\\Theta}(f_{(n,m)}) = \\hat{\\sigma}_{r^*} (max^{r^*}-min^{r^*}) + min^{r^*} \\] <p>\\(r^*\\) \u662f\u5177\u6709 Max Probability \u7684 class \u7f16\u53f7</p> <p>Multi-Exampler Voting Strategy</p> <p>\u5bf9\u4e8e\u7ed9\u5b9a\u8f93\u5165 \\(v_{\\text{test}}\\)\uff0c\u9009\u5b9a \\(M\\) \u4e2a exampler \\(\\{v^m_{\\text{train}}\\}_{m=1}^M\\) \u53ca\u5176\u5bf9\u5e94\u5206\u6570\u6807\u7b7e \\(\\{s^m_{\\text{train}}\\}_{m=1}^M\\)</p> <p>Voting \u7b56\u7565\u53ef\u4ee5\u4f7f\u7528\u5982\u4e0b\u516c\u5f0f\u8fdb\u884c\u63cf\u8ff0\uff1a</p> \\[     \\begin{align*}         \\hat{s}^m_{\\text{test}} &amp;= R_{\\Theta}(F_W(v_{\\text{test}}, v_{\\text{train}}^m)) + s_{\\text{train}}^m \\\\         \\hat{s}_{\\text{test}} &amp;= \\frac{1}{M} \\sum \\hat{s}_{\\text{test}}^m     \\end{align*} \\] </li> </ul>"},{"location":"sum/Constrastive/#4-evaluation-protocol","title":"4 Evaluation Protocol","text":"<ol> <li> <p>Spearman's Rank Correlation </p> <p>\u4e3a\u4e86\u80fd\u548c\u4e4b\u524d\u7684\u5de5\u4f5c\u8fdb\u884c\u6bd4\u8f83\uff0c\u6b64\u5904\u91c7\u7528\u4e86 Spearman's Rank Correlation \u4f5c\u4e3a\u8bc4\u4f30\u6807\u51c6\uff1a</p> \\[ \\rho = \\frac{\\sum(p_i-p)(q_i-q)}{\\sqrt{\\sum(p_i-\\overline{p})^2 \\sum(q_i-\\overline{q})^2}} \\] <p>\u5176\u4e2d \\(p,q\\) \u5206\u522b\u8868\u793a\u6837\u672c\u5728\u4e24\u4e2a\u5e8f\u5217\u4e2d\u7684 ranking</p> </li> <li> <p>Fisher\u2019s z-value</p> <p>\u7528\u4e8e\u8bc4\u4f30 across-action \u7684\u5e73\u5747\u8868\u73b0</p> </li> <li> <p>Relative L2-Distance</p> <p>\u5bf9\u4e8e\u7279\u5b9a\u52a8\u4f5c\u7684 max/min score\uff0cR-l<sub>2</sub> \u9075\u5faa\u5982\u4e0b\u5b9a\u4e49\uff1a</p> \\[ R-l_2(\\theta) = \\frac{1}{K} \\sum\\left(\\frac{|s_k - \\hat{s}_k|}{s_{max} - s_{min}}\\right)^2 \\] <p>\u5176\u4e2d \\(s_k,\\hat{s}_k\\) \u5206\u522b\u8868\u793a\u7b2c k \u4e2a\u6837\u672c\u7684 ground-truth \u548c \u9884\u6d4b\u5f97\u5206</p> <p>\u4f20\u7edf\u7684 l<sub>2</sub> \u8ddd\u79bb\u5bf9\u4e8e\u4ece\u5c5e\u4e8e\u4e0d\u540c class \u7684 action \u662f\u65e0\u610f\u4e49\u7684\uff1b</p> <p>\u76f8\u6bd4\u4e8e Spearman's Rank Correlation \u6ce8\u91cd RANK\uff0cR-l<sub>2</sub> \u66f4\u52a0\u91cd\u89c6\u5177\u4f53\u6570\u503c</p> </li> </ol>"},{"location":"sum/Constrastive/#2022-tpt","title":"2022: TPT","text":""},{"location":"sum/Constrastive/#1-abstract_1","title":"1 Abstract","text":"<ul> <li> <p>\u5148\u524d\u7684 SOTA \u65b9\u6cd5</p> <p>\u4f7f\u7528 ranking-based pairwise comparison\uff0c\u6216 regression-based methods </p> <p>=&gt; \u901a\u8fc7\u5bf9 backbone \u8f93\u51fa\u505a global Pooling\uff0c\u57fa\u4e8e \u6574\u4e2a\u89c6\u9891 (holistic video) \u8fdb\u884c regression / ranking</p> <p>\u9650\u5236\u4e86\u5bf9 \u7ec6\u7c92\u5ea6\u00b7\u7c7b\u5185\u5dee\u5f02 (fine-grained intra-class variation) \u7684\u6355\u6349</p> </li> <li> <p>\u521b\u65b0\u70b9</p> <p>regression-based\uff0c\u53ef\u4ee5\u5728\u907f\u514d part-level \u76d1\u7763\u7684\u60c5\u51b5\u4e0b\u5b66\u4e60\u66f4\u7ec6\u7c92\u5ea6\u7684\u7279\u5f81</p> <ol> <li> <p>\u4f7f\u7528 Temporal Parsing Transfomer \u5c06 \"\u6574\u4f53\u7279\u5f81(holistic)\" \u62c6\u89e3\u4e3a temporal part-level \u5f62\u5f0f</p> <p>\u5c06 Diving \u62c6\u89e3\u4e3a approach -&gt; take off -&gt; flight \u7b49\u591a\u4e2a\u9636\u6bb5</p> <ul> <li> <p>\u4f7f\u7528\u4e00\u7cfb\u5217 queries \u6765\u8868\u793a \u7279\u5b9a\u52a8\u4f5c\u7684 atomic temporal patterns</p> </li> <li> <p>\u5728 Decoding \u9636\u6bb5\uff0c\u5c06 frames \u8f6c\u6362\u4e3a \u957f\u5ea6\u56fa\u5b9a\u7684\u00b7\u6309\u7167\u65f6\u5e8f\u6392\u5217\u7684 part representations</p> </li> <li> <p>\u57fa\u4e8e (relative pairwise) part representations \u548c \u5bf9\u6bd4\u56de\u5f52 \u5f97\u5230\u6700\u7ec8\u7684 Quality Score</p> </li> </ul> </li> <li> <p>\u63d0\u51fa\u4e86\u4e24\u79cd\u65b0\u7684 Loss Function \u7528\u4e8e Decoder</p> <p>\u56e0\u4e3a\u5f53\u524d\u7684\u6570\u636e\u96c6\u90fd\u6ca1\u6709\u63d0\u4f9b temporal part-level labels / partitions</p> <ol> <li> <p>Ranking Loss: Cross Attenion \u9636\u6bb5\u5b66\u4e60\u7684\u53c2\u6570\u7b26\u5408\u65f6\u5e8f</p> </li> <li> <p>Sparsity Loss: \u8ba9\u5b66\u4e60\u7684 part representation \u66f4\u5177 \"\u5224\u522b\u6027(discriminative)\"</p> </li> </ol> </li> </ol> </li> </ul> <p>Temporal Action Parsing: \u7ec6\u7c92\u5ea6\u52a8\u4f5c\u8bc6\u522b</p> <ul> <li>Zhang: Temporal Query Network adopted query-response functionality</li> <li>Dian: TransParser</li> </ul> <p>\u4e0a\u8ff0\u5de5\u4f5c\u5747\u805a\u7126\u4e8e \"frame-level\" \u7684\u7279\u5f81\u589e\u5f3a\uff0c\u800c\u672c\u6587\u5219\u4fa7\u91cd\u4e8e\u63d0\u53d6 \"\u66f4\u5177\u5907\u8bed\u4e49\u4fe1\u606f\u7684\u00b7part representation\"</p>"},{"location":"sum/Constrastive/#2-approach","title":"2 Approach","text":""},{"location":"sum/Constrastive/#overview","title":"Overview","text":"<ol> <li> <p>\u4f7f\u7528\u6ed1\u52a8\u7a97\u53e3\u5c06 input \u5212\u5206\u4e3a \\(T \\times M \\text{frames}\\) \u7684 \u91cd\u53e0 Clips</p> </li> <li> <p>\u5bf9\u4e8e\u5206\u5272\u5f97\u5230\u7684 Clips\uff0c\u4f7f\u7528 I3D (backbone) \u5904\u7406\u5f97\u5230 \\(V = \\{v_t \\in \\mathbb{R}^D\\}_{t=1}^T\\)</p> <ul> <li> <p>\\(D\\) \u4e3a feature dimension</p> </li> <li> <p>\u6bcf\u4e2a \\(v_t\\) \u7ecf\u7531\u7a7a\u95f4\u4e0a\u7684 Average Pooling \u5f97\u5230\uff08\u672c\u6a21\u578b\u5e76\u4e0d\u5173\u6ce8 spatial patterns\uff09</p> </li> </ul> </li> <li> <p>Contrastive Regression</p> <ol> <li> <p>Clip-Level \\(V\\) \\(\\rightarrow\\) Temporal Part-Level representations \\(P =\\{p_k\\in \\mathbb{R}^d\\}_{k=1}^K\\)</p> <p>\\(d\\) \u4e3a Part Feature-dimension, \\(K\\) \u4e3a query \u603b\u6570</p> <p>\u64cd\u4f5c\u5206\u522b\u5bf9 Input &amp; Exampler \u8fdb\u884c\uff0c\u751f\u6210 \\(P, P_0\\)</p> </li> <li> <p>\u57fa\u4e8e Part-aware Contrastive Regressor \u8ba1\u7b97 Part-wise Relative representations</p> </li> <li>Fuse Part-aware representation \u4ee5\u9884\u6d4b Relative Score \\(\\Delta s = R(P,P_0)\\)</li> <li>Final Score = \\(s + \\Delta s\\)\uff08\\(s\\) \u4e3a exampler \u7684\u5b9e\u9645\u5206\u6570\uff09</li> </ol> </li> </ol>"},{"location":"sum/Constrastive/#temporal-parsing-transformer","title":"Temporal Parsing Transformer","text":"<p>\u8f93\u5165\uff1aClip-Level Representation \\(V\\)\uff0c\u8f93\u51fa\uff1aquries (Part Representation)</p> <p>\u533a\u522b\u4e8e DETR \u7684 Transformer \u67b6\u6784</p> <ol> <li> <p>\u56e0\u4e3a Encoder \u4e0d\u80fd\u63d0\u9ad8\u672c\u65b9\u6cd5\u7684\u51c6\u786e\u6027\uff0c\u672c\u65b9\u6cd5 \u4ec5\u5305\u542b Decoder\uff1a</p> <p>\u53ef\u80fd\u662f\u56e0\u4e3a\uff1a</p> <ul> <li> <p>clip-level self-attention smooths the temporal representations</p> </li> <li> <p>\u672c\u65b9\u6cd5 cannot decode part presentations without part labels</p> </li> </ul> </li> <li> <p>\u5728 cross attention block \u6dfb\u52a0\u4e86\u53c2\u6570 \\(temperature\\) \u4ee5\u63a7\u5236\u5185\u79ef\u7684\u653e\u5927\uff08\uff1f\uff09</p> </li> <li> <p>\u6ca1\u6709\u628a\u4f4d\u7f6e\u4fe1\u606f\uff08clip id\uff09embed \u5230\u8f93\u5165\u6570\u636e\u4e2d</p> <p>=&gt; query \u7528\u4e8e\u8868\u793a atomic patterns \u800c\u975e\u4f5c\u4e3a\u65f6\u95f4\u951a\u70b9</p> </li> </ol> <p>\u5bf9\u4e8e \\(i^{th}\\) Decoder Layer\uff0c\u6709\uff1a</p> <ul> <li> <p>Decoder Part Feature \\(p^{(i)}_k \\in \\mathbb{R}^d\\)</p> </li> <li> <p>Learnable Atomic Patterns \\(q_k \\in \\mathbb{R}^d\\)</p> </li> <li> <p>embedded Clip Rrpresentation \\(v_t \\in \\mathbb{R}^d\\)</p> </li> </ul> <p>\u5148\u7528 \\(p^{(i)}_k + q_k\\) \u5f97\u5230 query\uff0c\u968f\u540e\u548c \\(v_t\\) \u505a cross attention \u5f97\u5230\u8f93\u51fa \\(\\alpha_{k,t}\\)</p> \\[ \\alpha_{k,t} = \\frac{     \\exp{((p_k^{(i)} + q_k)^T \\cdot \\frac{v_t}{\\tau})} }{     \\sum_{j=1}^T \\exp{((p_k^{(i)} + q_k)^T \\cdot \\frac{v_j}{\\tau})} } \\] <ul> <li>\\(\\alpha_{k,t}\\) \u8868\u793a query<sup>k</sup> \u548c clip<sup>t</sup> \u4e4b\u95f4\u7684 attention value</li> <li>\\(\\tau \\in \\mathbb{R}\\) \u662f\u53ef\u8c03\u53c2\u6570\uff0c\u7528\u4e8e\u653e\u5927\u5185\u79ef\u4ee5\u4f7f\u5f97 attention \u66f4\u52a0 discriminative.</li> </ul> <p>\u7531\u4e8e\u672c\u6a21\u5757\u7684\u76ee\u7684\u5728\u4e8e\uff1a\u5c06 Clip Repre \u805a\u5408\u5230 Part Repre \u4e2d\uff0c\u6211\u4eec\u9700\u8981\u6839\u636e\u5982\u4e0b\u7b56\u7565\u5bf9 Part Repre \u8fdb\u884c\u66f4\u65b0\uff1a</p> \\[ p^{(i)}_k += \\sum_{j=1}^{T} \\alpha_{k,j} v_j + p_k^{(i)} \\]"},{"location":"sum/Constrastive/#part-aware-contrastive-regression","title":"Part-Aware Contrastive Regression","text":"<p>\u5728 Temporal Parsing Transformer \u6a21\u5757\uff0c\u6211\u4eec\u5df2\u7ecf\u6210\u529f\u5c06\u8f93\u5165\u8f6c\u5316\u4e3a Part Repre \\(P = \\{p_k\\}\\);</p> <p>\u6b64\u65f6\u6211\u4eec\u9700\u8981\u5bf9 Input \u548c Exampler \u7684 \\(P,P_0\\) \u8fdb\u884c\u6bd4\u8f83\uff0c\u5e76\u751f\u6210 Relative Score \\(\\Delta s\\)</p> <p>\u6211\u4eec\u53ef\u4ee5\u5206\u522b\u8ba1\u7b97\u6bcf\u4e2a Part \u7684\u76f8\u5bf9\u5206\u6570\uff0c\u6700\u540e\u628a\u4ed6\u4eec fuse \u5230\u4e00\u5757\u513f</p> <p>\u5bf9\u4e8e \\(k^{th}\\) Part\uff0c\u6211\u4eec\u901a\u8fc7 \u591a\u5c42\u611f\u77e5\u673a(MLP) \\(f_r(.)\\) \u751f\u6210\u5bf9\u5e94\u7684 relative pairwise representation \\(r_k \\in \\mathbb{R}^d\\)\uff1a</p> <p>\u6240\u6709\u7684 Parts \u5171\u7528\u4e00\u4e2a MLP</p> \\[ r_k = f_r(\\text{concat}([P_k;P_k^0])) \\] <p>\u4e3a\u4e86\u63d0\u9ad8\u51c6\u786e\u6027\uff0c\u6b64\u5904\u4f7f\u7528 Group-aware Regression strategy \u751f\u6210 Relative Score \\(\\Delta s\\)\uff1a</p> <ul> <li> <p>\u5bf9 Training Set \u4e2d\u6240\u6709\u53ef\u80fd\u7684 pair \u751f\u6210 \\(B\\) \u4e2a \\(\\Delta s\\) \u53d6\u503c\u533a\u95f4\uff08\u7c7b\u4f3c CoRe Tree \u91cc\u7684\u533a\u95f4\u975e\u504f\u533a\u95f4\u5212\u5206\u7b56\u7565\uff09</p> </li> <li> <p>\u751f\u6210 One-Hot Label \\(\\{l_n\\}\\)\uff0c\u8868\u793a \\(\\Delta s[i]\\) \u6240\u5904\u7684\u533a\u95f4\u7f16\u53f7</p> </li> </ul> <p>\u5bf9 Input Video \u7684\u9884\u6d4b\uff1a</p> <ol> <li>\u5bf9 Relative Part Repre \\(\\{r_k\\}\\) \u4f7f\u7528 Average Pooling</li> <li>\u4f7f\u7528 2 * 2-Layer MLP \u5bf9\u8f93\u5165\u89c6\u9891\u7684 classification label \\(l\\) &amp; \u56de\u5f52\u7ed3\u679c \\(\\gamma\\) \u8fdb\u884c\u9884\u6d4b</li> </ol>"},{"location":"sum/Constrastive/#3-optimization","title":"3 Optimization","text":"<ul> <li> <p>\u5047\u8bbe\uff1a\u6bcf\u4e00\u7c7b\u52a8\u4f5c\u90fd\u53ef\u4ee5\u6309\u7167\u76f8\u540c\u987a\u5e8f\u8fdb\u884c\u9636\u6bb5\u5207\u5206\uff0c\u5e76\u901a\u8fc7 transformer query \u8fdb\u884c\u8868\u793a</p> </li> <li> <p>\u5728 Cross Attention \u9636\u6bb5\uff0c\\(k^{th}\\) query \u7684 attention center \\(\\\\overline{\\alpha}_k\\)\uff1a</p> \\[ \\overline{\\alpha}_k = \\sum_{t=1}^T t \\cdot \\alpha_{k,t} \\in [1,T] \\] <ul> <li>\\(\\{\\alpha_{k,t}\\}\\) \u662f\u5df2\u7ecf normalized \u7684 attention responses</li> <li>\\(k^{th}\\) query \u5bf9\u4e8e\u6240\u6709 clip \u7684 attention \u603b\u548c\u4e3a 1\uff0c\u5373 \\(\\sum_{t=1}^T \\alpha_{k,t} = 1\\)</li> </ul> </li> </ul>"},{"location":"sum/Constrastive/#cross-attention-block","title":"Cross Attention Block","text":"<ol> <li> <p>Ranking Loss</p> <p>\u4e3a\u4e86\u9f13\u52b1\u5404 query \u805a\u7126\u4e8e different temporal region\uff0c\u6211\u4eec\u5bf9 attention center \u4f7f\u7528 Ranking Loss</p> <p>\u7406\u60f3\u60c5\u51b5\u4e0b\uff0cPart Repre \u5728\uff08\u540c\u7c7b\uff09\u4e0d\u540c\u89c6\u9891\u4e0b\u6709 \u76f8\u540c\u65f6\u5e8f</p> \\[ L_{rank} = \\sum_{k=1}^{K-1} \\max{(0,\\ \\overline{\\alpha}_k - \\overline{\\alpha}_{k+1} + m)} + \\max{(0,\\ 1-\\overline{\\alpha}_1 + m)} + \\max{(0,\\ \\overline{\\alpha}_k - T + m)} \\] <p>\\(m\\) \u662f\u7528\u4e8e\u63a7\u5236\u60e9\u7f5a\u529b\u5ea6\u7684\u8d85\u53c2\u6570</p> <ul> <li> <p>\u7b2c\u4e00\u9879\u7528\u4e8e\u786e\u4fdd\u987a\u5e8f \\(\\overline{\\alpha}_k \\lt \\overline{\\alpha}_{k+1}\\) \u6210\u7acb</p> </li> <li> <p>\u540e\u4e24\u9879\u5206\u522b\u7528\u4e8e\u786e\u5b9a\u9996\u4f4d\u987a\u5e8f \\(\\overline{\\alpha}_0 = 1\\) \u548c \\(\\overline{\\alpha}_{k+1} = T\\) \u6210\u7acb \uff08\\(\\overline{\\alpha}_k \\in [1,T]\\)\uff09</p> </li> </ul> </li> <li> <p>Sparsity Loss</p> <p>\u9f13\u52b1\u6bcf\u4e00\u4e2a query \u805a\u7126\u4e8e\u9760\u8fd1 center \\(\\mu_k\\) \u7684\u90a3\u4e9b\u5207\u7247\uff1a</p> \\[ L_{sparsity} = \\sum_{k=1}^K\\sum_{t=1}^T|t - \\overline{\\alpha}_k| \\cdot \\alpha_{k,t} \\] </li> </ol>"},{"location":"sum/Constrastive/#contrastive-regressor","title":"Contrastive Regressor","text":"<p>\u57fa\u4e8e\u6bd4\u8f83\u5b66\u4e60\u7684\u56de\u5f52\u5668\u9700\u8981\u9884\u6d4b \u5206\u7ec4\u6807\u7b7e \\(l\\) &amp; \u76f8\u5bf9\u504f\u5dee \\(\\gamma\\)\uff0c\u6211\u4eec\uff1a</p> <ul> <li> <p>\u5bf9\u5404\u7ec4\u4f7f\u7528 BCE Loss</p> \\[ L_{cls} = - \\sum_{n=1}^N [l_n \\log{(\\vec{l}_n)} + (1-l_n) \\log{(1 - \\vec{l}_n)}] \\] </li> <li> <p>\u5bf9\u7531 ground-truth \u751f\u6210\u7684 interval \u4fe1\u606f\u4f7f\u7528 Square Error</p> \\[ L_{reg} = \\sum_{n=1}^N (\\gamma_n \\ \\vec{\\gamma}_n)^2, \\text{ where } l_n=1 \\] </li> </ul>"},{"location":"sum/Constrastive/#overall-training-loss","title":"Overall Training Loss","text":"\\[ L = \\lambda_{cls} L_{cls} + \\lambda_{reg} L_{reg} + \\lambda_{rank} \\sum_{i=1}^L L^i_{rank} + \\lambda_{sparsity} \\sum_{i=1}^L L^i_{sparsity} \\]"},{"location":"sum/Constrastive/#2022-pcln","title":"2022: PCLN","text":"<p>Pairwise Contrastive Learning Network</p> End-to-End \u6a21\u578b <p>\u6307\u76f4\u63a5\u8f93\u5165 \u539f\u59cb\u6570\u636e(raw data)\uff0c\u8f93\u51fa\u6700\u7ec8\u7ed3\u679c\u7684\u6a21\u578b</p> <ul> <li> <p>\u7ecf\u5178\u7684 Machine Learning \u65b9\u6cd5</p> <ul> <li> <p>\u9700\u8981\u4eba\u5de5\u8fdb\u884c feature engineering\uff0c\u901a\u8fc7\u5148\u9a8c\u7ecf\u9a8c\u4eba\u5de5\u7684\u5c06 raw data \u5904\u7406\u6210 feature\uff0c\u968f\u540e\u8f93\u5165\u6a21\u578b</p> </li> <li> <p>\u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b\uff0c\u7279\u5f81\u63d0\u53d6\u7684\u597d\u574f\u4f1a\u4e25\u91cd\u5f71\u54cd\u9884\u6d4b\u8d28\u91cf\uff08\u751a\u81f3\u6bd4\u5b66\u4e60\u7b97\u6cd5\u672c\u8eab\u9020\u6210\u7684\u5f71\u54cd\u66f4\u5927\uff09</p> </li> <li> <p>\u6b64\u65f6\u7684 ML \u65b9\u6cd5\u7531\u591a\u4e2a \u72ec\u7acb\u6a21\u5757 \u6267\u884c\uff0c\u6bd4\u5982 NLP = \u5206\u8bcd + \u8bcd\u6027\u6807\u6ce8 + \u8bcd\u6cd5\u5206\u6790 + \u8bed\u4e49\u5206\u6790 ...</p> </li> </ul> </li> <li> <p>\u591a\u5c42\u795e\u7ecf\u7f51\u7edc</p> <ul> <li> <p>\u53ef\u4ee5\u62df\u5408\u4efb\u610f\u975e\u7ebf\u6027\u51fd\u6570\uff0c\u6a21\u578b\u53ef\u4ee5\u81ea\u884c\u901a\u8fc7\u53cd\u5411\u4f20\u8f93\u5bf9 feature \u8fdb\u884c\u6355\u6349</p> </li> <li> <p>\u8fd9\u4f7f\u5f97 End-to-End \u6a21\u578b\u4e2d\u95f4\u7684\u7ec6\u8282\u90fd\u65e0\u9700\u4eba\u5de5\u5e72\u9884\uff08\u4e22\u56fe\u7247\u8fdb\u53bb\u5c31\u884c\u4e86\uff09</p> </li> </ul> </li> </ul>"},{"location":"sum/Constrastive/#1-introduction","title":"1 Introduction","text":"<ul> <li> <p>Previous Works: AQA in sports</p> <ul> <li> <p>common solution: \\(\\text{Regression}(\\text{video}) \\rightarrow \\text{Score}\\)</p> <p>=&gt; \u5ffd\u7565\u4e86 subtle diffs\uff0c\u4e5f\u6ca1\u6709\u8003\u8651\u540c\u5206\u4f46\u4e0d\u540c\u96be\u5ea6\u7684\u60c5\u51b5</p> </li> <li> <p>\u4e00\u79cd\u542f\u53d1\u5f0f\u7684\u89e3\u51b3\u65b9\u6848\uff1atrain a specific model to learn diffs between videos</p> <p>\u53d7 pairwise Learning to Rank (LTR) \u601d\u60f3\u7684\u542f\u53d1\uff08\u4e00\u5bf9\u5bf9\u6bd4\u8f83\u6570\u636e\uff0c\u7136\u540e\u6392\u540d\uff09</p> <ul> <li>\u4ee5 video-pairs \u4e3a\u8f93\u5165\uff08\u53ef\u4ee5\u6269\u5927\u8bad\u7ec3\u96c6 \\(N \\rightarrow C_N^2\\)\uff09</li> <li>\u4ee5\u76f8\u5bf9\u5206\u6570 relative score = <code>s1 - s2</code> \u4f5c\u4e3a\u6807\u7b7e\uff08\u9884\u6d4b\u76ee\u6807\uff09</li> </ul> </li> </ul> </li> <li> <p>\u672c\u6587\u5de5\u4f5c</p> <ul> <li> <p>\u63d0\u51fa\u4e86 BasicRegression + PCLN \u7684\u7aef\u5230\u7aef\u6a21\u578b\uff0c\u5bf9 Video-Pair \u8fdb\u884c\u7f16\u7801\uff0c\u5bf9 relative score \u8fdb\u884c\u9884\u6d4b</p> <p>PCLN \u662f\u4e00\u4e2a\u65b0\u9896\u7684\u3001LTR-based \u6a21\u578b\uff0c\u7528\u4e8e\u5b66\u4e60\u4e24\u4e2a\u89c6\u9891\u95f4\u7684\u5dee\u5f02</p> </li> <li> <p>\u5b9a\u4e49\u4e86\u65b0\u7684 consistency constraint \u7528\u4e8e\u8bad\u7ec3\u6a21\u578b\uff08\u5e73\u8861 PCLN &amp; BasicRgression\uff09</p> </li> <li> <p>\u5728 Test \u9636\u6bb5\uff0c\u53ea\u6709 BasicRegression \u6a21\u5757\u53c2\u4e0e\uff08PCLN \u4e0d\u7528\u52a8\uff09</p> </li> <li> <p>benchmarks: AQA-7, MTL-AQA</p> </li> </ul> </li> </ul>"},{"location":"sum/Constrastive/#2-related-works","title":"2 Related Works","text":""},{"location":"sum/Constrastive/#a-regression-based-aqa","title":"a) Regression-based AQA","text":"<p>\u57fa\u4e8e input data \u7684\u683c\u5f0f\u53ef\u4ee5\u88ab\u5206\u4e3a\u4e24\u7c7b\uff1a</p> <ol> <li> <p>Skeleton-based</p> <ul> <li>Pirsiavash\uff1aDCT\uff08\u7279\u5f81\u63d0\u53d6\uff09+ SVR\uff08\u56de\u5f52\u9884\u6d4b\uff09</li> <li>Pan\uff1a<code>Human joint Learning = joint-Common + joint-Difference</code> Modules</li> </ul> </li> <li> <p>Appearance-based</p> <p>to: acquire more detailed appearance-based info</p> <ul> <li>Parmar: C3D\uff08\u7279\u5f81\u63d0\u53d6\uff09+ SVR &amp; LSTM\uff08\u56de\u5f52\u9884\u6d4b\uff09</li> <li>Xiang: P3D\uff08\u5206\u6bb5\u7279\u5f81\u63d0\u53d6\uff09+ fuse\uff08\u6574\u5408\uff09 + Regression\uff08\u56de\u5f52\u9884\u6d4b\uff09</li> <li>Tang: Uncertainty-aware Score Distribution Learning</li> <li>Dong: (Multioke Hidden Substages) Learning &amp; Fusion Network</li> </ul> </li> </ol>"},{"location":"sum/Constrastive/#b-pairwise-ranking-aqa","title":"b) Pairwise Ranking AQA","text":"<ul> <li> <p>Doughty: A Supervised Deep Ranking Method</p> <p>a rank-specific attention Module\uff0c\u7528\u4e8e\u5206\u522b\u5904\u7406 higher/lower skills parts</p> </li> <li> <p>Yu: Group-aware Regression Tree\uff08\u66ff\u6362\u4f20\u7edf Regression \u6a21\u5757\uff09</p> </li> <li> <p>Jain: \u4f7f\u7528 Binary Classification Network \u6765\u5b66\u4e60\u4e24\u4e2a\u89c6\u9891\u95f4\u7684 similarity</p> <p>\u5c06 input video &amp; expert video(\u8bad\u7ec3\u96c6\u4e2d\u5f97\u5206\u6700\u9ad8\u7684) \u88ab\u4e00\u8d77\u8f93\u5165\u6a21\u578b\uff0c\u4f46\u662f\uff1a</p> <ul> <li>\u53ea\u6709\u4e00\u4e2a expert sample \u4e0d\u80fd\u9002\u5e94\u52a8\u4f5c\u7684\u590d\u6742\u6027\u548c\u89c6\u9891\u7684\u591a\u6837\u6027</li> <li>\u4f7f\u7528\u4e86 2-stage \u7b56\u7565\uff0c\u4e0d\u80fd\u4fdd\u8bc1\u5728 Binary-Classification \u9636\u6bb5\u5b66\u4e60\u7684\u53c2\u6570\u540c\u6837\u9002\u7528\u4e8e Score-Regression \u9636\u6bb5</li> </ul> </li> </ul>"},{"location":"sum/Constrastive/#3-approach_1","title":"3 Approach","text":""},{"location":"sum/Constrastive/#question-formulation","title":"Question Formulation","text":"<ul> <li> <p>Input: </p> <ul> <li> <p>\u5047\u8bbe\u8f93\u5165\u7684\u89c6\u9891\u5171\u6709 \\(L\\) \u5e27\uff0c\u5219\u53ef\u5c06\u8f93\u5165\u89c6\u9891\u8bb0\u4e3a \\(V = \\{v_i\\}_{i=1}^L\\)</p> </li> <li> <p>PCLN \u7528\u4e8e\u5b66\u4e60\u4e24\u4e2a\u89c6\u9891\u4e4b\u95f4\u7684 diff\uff0c\u4ece\u800c\u8fdb\u884c\u66f4\u51c6\u786e\u7684\u9884\u6d4b\u3002\u5176\u8f93\u5165 \\(&lt;V_i, V_j&gt;\\) \u6709\u968f\u673a\u62bd\u6837\u4ea7\u751f\uff0c\u5171\u6709 \\(C(n,2)\\) \u79cd</p> </li> </ul> </li> <li> <p>Prediction: \u4f7f\u7528 \\(\\Theta(\u00b7)\\) \u8868\u793a\u5206\u6570\u9884\u6d4b\u51fd\u6570\uff0c\u5219\u6574\u4e2a\u6a21\u578b\u7684\u529f\u80fd\u53ef\u4ee5\u6982\u62ec\u4e3a</p> \\[ \\hat{S} = \\Theta(V) \\] <p>\u4f46\u5fc5\u4e0d\u53ef\u80fd\u8fd9\u4e48\u7b80\u5355\uff0c\u672c\u6587\u7684\u76ee\u7684\u5c31\u662f\u627e\u5230\u4e00\u4e2a\u66f4\u6709\u6548\u7684 Regression Function \\(\\Theta(\u00b7)\\)</p> <p>\u6700\u62bd\u8c61\u7684\u662f\uff1a\u8fd9\u4e2a\u65b9\u6cd5\u91cc \\(\\hat{S}\\) \u548c \\(\\Delta S\\) \u662f\u7531 BasicReg \u548c PCLN \u6a21\u5757 \u72ec\u7acb\u9884\u6d4b \u7684</p> <p>\u5c06\u672c\u6587\u63d0\u51fa\u7684\u7b97\u6cd5\u8bb0\u4e3a \\(\\Upsilon(\u00b7)\\)\uff0c\u90a3\u4e48\u6574\u4f53\u8fc7\u7a0b\u53ef\u4ee5\u91cd\u5199\u4e3a\uff1a</p> \\[ [\\hat{S}_i, \\hat{S}_j, \\Delta S] = \\Upsilon(V_i, V_j) \\] </li> </ul>"},{"location":"sum/Constrastive/#feature-extraction","title":"Feature Extraction","text":"<p>\u5bf9\u4e8e video-pair \\(&lt;V_i, V_j&gt;\\)\uff0c\u7406\u8bba\u4e0a\u6709\u4e24\u79cd\u7279\u5f81\u63d0\u53d6\u65b9\u6848\uff1a</p> <ol> <li> <p>3D-Conv-based: require short clip with fixed-length</p> <p>\u4ece clip \u4e2d\u63d0\u53d6\u7684\u7279\u5f81\u5e76\u4e0d\u7a33\u5b9a\uff0c\u4f1a\u5bfc\u81f4\u9884\u6d4b\u7ed3\u679c\u5b58\u5728\u8f83\u5927\u6ce2\u52a8</p> </li> <li> <p>\u2705 Temporal Encoder-based</p> </li> </ol> <p>\u672c\u6587\u4f7f\u7528\u7684 Temporal Encoder-based \u65b9\u6cd5\uff1a</p> \\[ f_i = \\text{TemporalEncoder}(\\text{ResNet}(V_i)),\\ i=p,q \\] <p>\u5bf9\u89c6\u9891 \\(V_i,V_j\\) \u4f7f\u7528 weight-sharing Model \u5206\u522b\u63d0\u53d6\u7279\u5f81</p> <ol> <li>\u4f7f\u7528 backbone\uff08ResNet\uff09 \u5bf9 each frame \u8fdb\u884c\u7279\u5f81\u63d0\u53d6</li> <li> <p>\u4f7f\u7528 Temporal Encoder Network \u5bf9 featureSeq \u7684\u65f6\u5e8f\u4fe1\u606f\u8fdb\u884c\u7f16\u7801</p> <ul> <li>Temporal Encoder Network \u7531\u4e24\u4e2a EncodingBlock \u5806\u53e0\u800c\u6210\u3002</li> <li>\u5355\u4e2a EncodingBlock = <code>1*1</code> temporalConv + ActivationFunc + maxPooling</li> </ul> </li> </ol>"},{"location":"sum/Constrastive/#score-regression","title":"Score Regression","text":"<ul> <li> <p>\u4f7f\u7528 Fully-Connexted Network \u8fdb\u884c\u56de\u5f52\u8fd0\u7b97</p> <p>\u8be5 FC network \u5171\u5305\u542b\u4e09\u4e2a\u5168\u8fde\u63a5\u5c42\uff1a<code>D * 4096</code>, <code>4096 * 2048</code>, <code>2048 * 1</code></p> </li> <li> <p>\u6211\u4eec\u53ef\u4ee5\u7528\u4ee5\u4e0b\u7684\u516c\u5f0f\u63cf\u8ff0 Regression \u8fc7\u7a0b\uff1a</p> \\[ \\hat{S}_i = \\text{Regression}(f_i),\\ i=p,q \\] </li> </ul>"},{"location":"sum/Constrastive/#pcln-model","title":"PCLN Model","text":"<p>\u5927\u591a\u6570\u4f53\u80b2\u6d3b\u52a8\u662f\u5728 similar background \u4e0b\u5b8c\u6210\u7684</p> <p> \u4f7f\u7528 Teomporal-Encoded Features \\(f_p, f_q\\) \u4f5c\u4e3a\u8f93\u5165 </p> <ol> <li> <p>\u5bf9\u4e8e\u5355\u4e2a\u8f93\u5165\u89c6\u9891\uff08\u7684 temporal encoded feature\uff09\u4f7f\u7528 1D-Conv</p> <p>further encode feature \u4ee5\u6355\u6349\u62bd\u8c61\u7a0b\u5ea6\u66f4\u9ad8\u7684 action info</p> \\[ f'_i = \\text{ReLU}(w_{(0)} \\otimes f_i + b_{(0)}),\\ i=p,q \\] <p>\\(w_{(0)}\\) \u662f 1D-Conv \u53c2\u6570\uff0c\\(\\otimes\\) \u662f Conv \u64cd\u4f5c\uff0c\\(b_{(0)}\\) \u662f\u5bf9\u5e94\u7684 bias vec</p> </li> <li> <p>\u4f7f\u7528\u77e9\u9635\u4e58\u6cd5\u8fde\u63a5\u4e24\u4e2a\u89c6\u9891\u7684 feature matrix</p> \\[ f_{(0)} = f'_p \\circ f'_q \\] </li> <li> <p>\u5bf9 \\(f_{(0)}\\) \u4f7f\u7528 \u5806\u53e0\u7684 [2D-Conv \u5c42 + MaxPooling] \u8fdb\u884c\u5904\u7406</p> <p>\u672c\u6587\u9a8c\u8bc1\u5b9e\u9a8c\u4e2d\u4f7f\u7528\u4e86 2 \u5c42 2D-Conv</p> <p>\u8bb0 i-th \u5c42 2D-Conv \u7684\u8f93\u51fa\u4e3a \\(f'_{(i)}\\)\uff0c\\((w_{(i)}, b_{(i)})\\) \u4e3a\u8be5\u5c42\u7684 ConvParams &amp; BiasVec</p> \\[ \\begin{align*}     f'_{(i)} &amp;= \\text{ReLU}(w_{(i)} \\otimes f_{(i-1)} + b_{(i)}) \\\\     f_{(i)} &amp;= \\text{MaxPool}(f'_{(i)}) \\end{align*} \\] </li> <li> <p>\u4f7f\u7528 4Layers-MLP\uff08\u591a\u5c42\u611f\u77e5\u673a\u673a\uff09\u9884\u6d4b relative score \\(\\Delta S\\)\uff0cDense \u5c42\u7684\u8282\u70b9\u6570\u5206\u522b\u4e3a <code>[64, 32, 8, 1]</code></p> \\[ \\Delta S = \\text{MLP}(f_{(\\text{last})}) \\] </li> </ol>"},{"location":"sum/Constrastive/#4-evaluation","title":"4 Evaluation","text":"<p>\u672c\u6587\u603b\u5171\u4f7f\u7528\u4e86 3 \u4e2a constraint \u6765\u540c\u65f6\u63d0\u5347\u5bf9 \\(\\Delta S\\) &amp; \\(\\hat{S}\\) \u9884\u6d4b\u7684\u51c6\u786e\u5ea6\uff1a</p> <ol> <li> <p>Loss for Basic-Regression</p> <p>AQA \u95ee\u9898\u6700\u672c\u8d28\u7684\u9700\u6c42\uff1a\u83b7\u5f97\u66f4\u51c6\u786e\u7684 \\(\\hat{S}\\) -&gt; \u6700\u5c0f\u5316\u5747\u65b9\u8bef\u5dee\uff08\u4e24\u4e2a\u89c6\u9891\u5206\u5f00\u7b97\uff09</p> \\[ \\mathcal{L}_{bs} = \\frac{1}{2} \\sum_{i=p,q}^N(\\hat{S}_i - S)^2 \\] </li> <li> <p>Loss for PCLN</p> <p>\u540c\u6837\u7684\uff0c\u6211\u4eec\u9700\u8981\u66f4\u51c6\u786e\u7684 \\(\\Delta S\\)</p> \\[ \\mathcal{L}_{ds} = (\\Delta S - |S_i - S_j|)^2 \\] </li> <li> <p>Consistency between PCLN &amp; BasicRegression: \u9650\u5236 \\(\\Delta S = |\\hat{S}_i - \\hat{S}_j|\\)</p> \\[ \\mathcal{L}_{rs} = (\\Delta S - |\\hat{S}_i - \\hat{S}_j|)^2 \\] </li> </ol> <p>\u4e0d\u540c\u4e8e\u5148\u524d\u7684\u65b9\u6cd5\uff1a\u5728 test \u9636\u6bb5\u76f4\u63a5\u4f7f\u7528 Basic Regression \u9884\u6d4b\u6700\u7ec8\u5206\u6570</p>"},{"location":"sum/GCN/","title":"\u56fe\u5377\u79ef\u795e\u7ecf\u7f51\u7edc GCN","text":""},{"location":"sum/GCN/#2023-hierarchical-graph-convolutional-networks","title":"2023: Hierarchical Graph Convolutional Networks","text":"<p>\u5c42\u7ea7\u5377\u79ef\u795e\u7ecf\u7f51\u7edc</p>"},{"location":"sum/GCN/#1-abstract","title":"1 Abstract","text":"<ul> <li> <p>\u5f53\u524d\u5de5\u4f5c\u5b58\u5728\u7684\u95ee\u9898\uff1a\u5c06 video \u5212\u5206\u4e3a\u7b49\u957f\u7684 clip\uff0c\u53ef\u80fd\u4f1a\u5bfc\u81f4\u4e24\u79cd\u95ee\u9898</p> <ul> <li> <p>intra-clip confusion</p> </li> <li> <p>inter-clip incoherence</p> </li> </ul> </li> <li> <p>\u672c\u6587\u521b\u65b0\u70b9\uff1aHGC</p> <ul> <li> <p>\u901a\u8fc7 clip refinement\uff08\u5207\u7247\u7ec6\u5316\uff09\u6765\u7ea0\u6b63\u8bed\u4e49\u4fe1\u606f\u5185\u7684\u6df7\u4e71\uff0c\u751f\u6210 <code>shot</code> \u4f5c\u4e3a\u57fa\u672c\u52a8\u4f5c\u5355\u5143</p> <p>\u89e3\u51b3\u4e86 intra-clip confusion \u95ee\u9898</p> </li> <li> <p>\u6784\u5efa scene graph\uff1a\u5c06\u8fde\u7eed\u7684\u4e00\u7ec4 <code>shot</code> \u7ec4\u5408\u6210\u6709\u610f\u4e49\u7684 <code>scene</code>\uff0c\u4ece\u800c\u6355\u6349 <code>local dynamics</code></p> <ul> <li><code>scene</code> \u53ef\u4ee5\u88ab\u89c6\u4e3a\u7ed9\u5b9a\u52a8\u4f5c\u7684\u53e6\u4e00\u79cd \u201c\u8fc7\u7a0b\u63cf\u8ff0\u201d</li> <li>\u89e3\u51b3\u4e86 inter-clip incoherence \u95ee\u9898</li> </ul> </li> <li> <p>\u901a\u8fc7\u987a\u5e8f\u805a\u5408 scenes \u83b7\u53d6 video-level representation\uff0c\u6700\u7ec8\u901a\u8fc7\u56de\u5f52\u9884\u6d4b\u5f97\u5230\u5206\u6570</p> </li> <li> <p>\u9a8c\u8bc1\u6570\u636e\u96c6\uff1aAQA-7\uff0cMTL-AQA\uff0cJIGSAWS</p> </li> </ul> </li> </ul>"},{"location":"sum/GCN/#2-relative-works","title":"2 Relative Works","text":""},{"location":"sum/GCN/#21-aqa","title":"2.1 AQA","text":""},{"location":"sum/GCN/#_1","title":"\u4e3b\u8981\u6d41\u6d3e","text":"<ol> <li> <p>Pose-Based\uff08\u65e9\u671f\uff09\uff1a\u4f7f\u7528 pose-based feature \u56de\u5f52\u9884\u6d4b\u5206\u6570</p> <ul> <li> <p>\u5e38\u89c1 pipeline\uff1a<code>location tracking -- \u7279\u5f81\u63d0\u53d6 -- \u5206\u6570\u9884\u6d4b</code></p> </li> <li> <p>\u4f18\u52bf\uff1a\u53ef\u4ee5\u63d0\u53d6 position / speed / direction \u7b49\u53ef\u5177\u4f53\u91cf\u5316\u7684\u7279\u5f81</p> </li> <li> <p>\u52a3\u52bf</p> <ul> <li> <p>\u5728\u4f53\u80b2\u8fd0\u52a8\u9886\u57df\u8fdb\u884c\u59ff\u6001\u4f30\u8ba1\u6bd4\u8f83\u56f0\u96be\uff1a\u8e72\u4f0f/\u906e\u6321\u7684\u5b58\u5728\u4f1a\u5bfc\u81f4\u9519\u4f30\u548c\u7f3a\u5931</p> </li> <li> <p>\u672a\u8003\u8651\u201c\u6c34\u82b1\u201d\u7b49 \u201c\u89c6\u89c9\uff08\u73af\u5883\uff09\u201d \u56e0\u7d20</p> </li> </ul> </li> <li> <p>Sample</p> <ul> <li> <p>Pirsiavash</p> <ul> <li> <p>\u4f30\u8ba1 pose -&gt; \u4f7f\u7528 discrete cosine transform \u7f16\u7801 -&gt; SVR \u56de\u5f52\u9884\u6d4b</p> </li> <li> <p>\u7531\u4e8e \u7167\u660e\u3001\u89c6\u89d2\u53d8\u6362\u3001\u906e\u6321 \u7684\u5b58\u5728\uff0c\u4f30\u8ba1\u7684 pose \u5e76\u4e0d\u51c6\u786e</p> </li> </ul> </li> </ul> </li> </ul> </li> <li> <p>Vision-Based <code>State-of-the-Art</code> !!!</p> <p>make FULL use of the visual features</p> <ul> <li> <p>\u5e38\u89c1 pipeline\uff1a<code>\u7279\u5f81\u63d0\u53d6 -- \u7279\u5f81\u805a\u5408 -- \u56de\u5f52\u9884\u6d4b</code></p> </li> <li> <p>\u5e38\u7528\u7279\u5f81\u63d0\u53d6 backbone: I3D / C3D</p> <p>\u4f46\u8fd9\u4e9b 3D-CNN \u5bf9\u4e8e\u5185\u5b58\u548c\u8ba1\u7b97\u8d44\u6e90\u5360\u7528\u6781\u5927\uff0c\u53ea\u80fd\u5e94\u7528\u4e8e\u77ed\u5c0f\u7684 clip</p> </li> <li> <p>\u56e0\u4e3a\u6574\u4e2a\u52a8\u4f5c\u5e8f\u5217\u5bf9\u4e8e\u5206\u6570\u90fd\u6709\u610f\u4e49\uff0c\u6240\u4ee5 \u201c\u52a8\u4f5c\u8bc6\u522b\u201d \u7528\u7684 \u201c\u5173\u952e\u5e27\u62bd\u53d6\u201d \u6280\u672f\u5728 AQA \u95ee\u9898\u4e0a\u4e0d\u592a\u9002\u7528</p> </li> </ul> </li> </ol> <p>\u662f\u5426\u4f7f\u7528 <code>\u5bf9\u6bd4\u5b66\u4e60</code> \u601d\u60f3</p> <p>\u6b64\u5916\uff0c\u4e5f\u53ef\u4ee5\u6309\u7167 Input-Format \u8fdb\u884c\u5212\u5206:</p> <ul> <li> <p>Exampler-Free</p> </li> <li> <p>Exampler-Based (\u7c7b CoRE)</p> <ul> <li> <p>\u4f18\u52bf\uff1a\u901a\u8fc7\u4ec5\u56de\u5f52\u8ba1\u7b97 relative score\uff0c\u63d0\u5347\u4e86\u51c6\u786e\u5ea6</p> </li> <li> <p>\u52a3\u52bf\uff1a</p> <ul> <li> <p>\u4eba\u5de5\u9009\u53d6 sampler \u4f1a\u5f15\u5165 personal bias\uff08\u4f46\u662f\u5e94\u8be5\u53ef\u4ee5\u968f\u673a\u62bd\u6837\uff1f\uff09</p> </li> <li> <p>\u5bf9\u4e8e exampler \u8fdb\u884c feature learning \u4f1a\u5f15\u5165\u989d\u5916\u7684\u8ba1\u7b97\u6210\u672c</p> </li> </ul> </li> </ul> </li> </ul> <p>\u6309\u7167 Output-Fomat \u5212\u5206\uff1a</p> <ul> <li> <p>Quality Score Regression\uff1a\u9884\u6d4b\u51c6\u786e\u7684\u5206\u6570\u503c\uff08\u8f93\u51fa\u662f <code>float</code>\uff09</p> </li> <li> <p>Grading\uff1a\u8f93\u51fa\u8bc4\u7ea7\u6807\u7b7e\uff08\u5982 <code>A-E</code> \u6863\uff0c\u6216\u8005 <code>Good/Bad</code> \u4e8c\u5206\u7c7b\uff09</p> </li> <li> <p>Pairwise-Sorting\uff1a\u8f93\u51fa quality score\uff0c\u5e76\u63d0\u4f9b rank-coefficient \u8fdb\u884c\u8bc4\u4f30</p> </li> </ul>"},{"location":"sum/GCN/#trade-off","title":"Trade off","text":"<p>Sequence Length x Visual Cues</p> <p>\u5927\u90e8\u5206\u5de5\u4f5c\uff1a</p> <ul> <li> <p>\u5c06\u89c6\u9891\u5212\u5206\u4e3a\u7b49\u957f\u7684 clip\uff0816 frames\uff09</p> </li> <li> <p>\u5bf9\u6bcf\u4e2a clip \u72ec\u7acb\u8ba1\u7b97 spatial-temporal features</p> </li> <li> <p>\u751f\u6210 video-level feature\uff1a\u4f7f\u7528\u5e73\u5747\u6c60\u5316</p> </li> </ul> <p>\u4ee5\u4e0a\u65b9\u6848\u7684\u7f3a\u70b9\uff1a</p> <ul> <li> <p>\u7b49\u957f\u5207\u5272\u5bf9\u4e8e\u53ef\u80fd\u5bfc\u81f4\u6bcf\u4e2a clip \u4e2d\u8574\u542b\u7684 motion-related \u8bed\u4e49\u4fe1\u606f\u5b58\u5728 \u7f3a\u5931/\u5197\u4f59</p> </li> <li> <p>\u4e00\u4e2a\u6709\u6548\u7684\u8bc4\u5206\u5468\u671f\u5f80\u5f80\u6a2a\u8de8\u591a\u4e2a\u57fa\u672c\u52a8\u4f5c\u5355\u5143\uff08\u5355\u4e2a unit \u5f80\u5f80\u4e0d\u80fd\u770b\u5230\u5168\u5c40\u4fe1\u606f\uff09</p> </li> </ul>"},{"location":"sum/GCN/#aggregation-methods","title":"Aggregation Methods","text":"<ul> <li> <p>\u5e38\u89c1\u65b9\u6cd5\u5982 Average Pooling\uff0cLSTM \u65e0\u6cd5\u51c6\u786e\u7684\u6355\u6349 local &amp; global dynamics</p> </li> <li> <p>\u4e3a\u4e86\u66f4\u5408\u7406\u7684\u5bf9 clip \u8fdb\u884c\u5212\u5206\uff0cboundary detection (of basic motion units) \u5c06\u4f5c\u4e3a\u9884\u8bad\u7ec3\u4efb\u52a1</p> <ul> <li> <p>\u4f46\u662f \u8fc7\u5ea6\u5207\u5206/\u9519\u8bef\u5207\u5206 \u4ecd\u7136\u4f1a\u5bfc\u81f4\u91cd\u8981\u65f6\u5e8f\u4fe1\u606f\u7684\u7f3a\u5931</p> </li> <li> <p>\u6b64\u5916\uff0c\u8fb9\u754c\u5212\u5206\u9700\u8981\u7684 \u4eba\u5de5\u6807\u7b7e \u6570\u636e\u96c6\u57fa\u672c\u4e0d\u7ed9\uff08\u624b\u6807\u7684\u6210\u672c\u53c8\u5f88\u9ad8\uff09</p> </li> </ul> </li> <li> <p>\u53d7 Structured Video Analysis \u7684\u542f\u53d1\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5c42\u7ea7\u7ed3\u6784</p> <ul> <li><code>shot</code> - basic action unit\uff0c\u5305\u542b\u82e5\u5e72\u5e27</li> <li><code>scene</code> - \u5305\u542b\u82e5\u5e72 \u8fde\u7eed\u7684 <code>shots</code></li> <li><code>action</code> - \u5305\u542b\u82e5\u5e72 <code>scenes</code></li> </ul> </li> </ul> <p></p>"},{"location":"sum/GCN/#22-video-representation-learning","title":"2.2 Video Representation Learning","text":"<p>\u4e3a\u4e0b\u6e38\u4efb\u52a1\u63d0\u4f9b Spatial-Temporal Features\uff0c\u4ece\u800c\u8fdb\u884c \u56de\u5f52/\u5206\u7c7b</p>"},{"location":"sum/GCN/#221-key-points","title":"2.2.1 Key Points\uff08\u65e9\u671f\uff09","text":"<p>\u65e9\u671f\u65b9\u6cd5:</p> <ul> <li> <p>\u9996\u5148\u8bc6\u522b\u5f97\u5230 key points\uff08\u5173\u952e\u70b9\uff09\uff0c\u518d\u4ece\u8fd9\u4e9b\u5173\u952e\u70b9\u4e2d\u63d0\u53d6\u62bd\u8c61\u5ea6\u66f4\u9ad8\u7684 feature\u3002</p> <p>\u5e38\u89c1\u65b9\u6cd5\u6709\uff1aspace-time interest points\uff0cdense trajectories</p> </li> <li> <p>\u968f\u540e\u901a\u8fc7 BoW / Fisher Vector \u7b49\u7f16\u7801\u65b9\u5f0f\uff0c\u5c06\u7279\u5f81\u805a\u5408\u5f97\u5230 video-level representation</p> </li> </ul> <p>\u8fd9\u4e9b\u65b9\u6cd5\u4e0d\u80fd \u5b8c\u6574 \u8bc6\u522b discriminative features</p> <p>\u57fa\u4e8e\u9aa8\u67b6\u7684\u65b9\u6cd5\u4f1a\u5ffd\u7565 subtle diffs\uff0c\u8fd9\u4e5f\u662f AQA \u524d\u671f\u53d1\u5c55\u7f13\u6162\u7684\u91cd\u8981\u539f\u56e0</p>"},{"location":"sum/GCN/#222-deep-learning-based","title":"2.2.2 Deep Learning-Based","text":"<p>\u6027\u80fd\u66f4\u597d\u634f</p> <ul> <li> <p>\u6700\u666e\u904d\u4e14\u6709\u6548\u63d0\u53d6 Spatial-Temporal feature \u7684\u65b9\u6cd5\u662f 3D-CNNs</p> </li> <li> <p>\u4f46\u7531\u4e8e 3D-CNN \u8ba1\u7b97\u6210\u672c\u5de8\u5927\uff0c\u4e0d\u80fd\u5904\u7406 100 frames \u4ee5\u4e0a\u7684\u957f\u89c6\u9891\u7247\u6bb5</p> <p>AQA \u5f80\u5f80\u5148\u8bc6\u522b clip-level \u7279\u5f81\uff08\u518d\u8003\u8651\u805a\u5408\uff09</p> </li> </ul>"},{"location":"sum/GCN/#23-structured-video-analysis","title":"2.3 Structured Video Analysis","text":"<p>WHY structured?</p> <ul> <li>Raw Video \u662f \u975e\u7ed3\u6784\u5316(unstructured) \u6570\u636e =&gt; \u5c31\u662f frame stream</li> <li>\u5982\u679c\u8981\u5b9e\u73b0 content-based video processing\uff0c\u5c31\u9700\u8981\u8003\u8651\u5c06\u5176\u7ed3\u6784\u5316</li> </ul> <ul> <li> <p>Hierarchical Video Stucture \u5c42\u7ea7\u89c6\u9891\u7ed3\u6784</p> <ul> <li> <p><code>shots</code> \u7531\u8fde\u7eed\u7684 frames \u7ec4\u6210</p> <ul> <li> <p>\u901a\u8fc7 shot boundary detection methods \u8fdb\u884c\u5212\u5206</p> </li> <li> <p>\u4e3b\u8981 visual content \u7531 key-frame \u8fdb\u884c\u8868\u793a</p> </li> </ul> </li> <li> <p><code>groups</code> \u7531 similiar shots \u6784\u6210</p> </li> <li> <p><code>scenes</code> \u7531 semantically-related shots \u6784\u6210</p> </li> </ul> </li> <li> <p>\u672c\u6587\u8ba4\u4e3a\uff1a </p> <ul> <li> <p>shots \u662f basci motion unit\uff0c\u5e76\u4e14\u76f8\u4e92\u72ec\u7acb</p> </li> <li> <p>\u533a\u522b\u4e8e Wang \u7b49\u4eba\u4e8e 2013 \u5e74\u63d0\u51fa\u7684 latent hierachical model</p> <p>\u81ea\u52a8\u5c06\u590d\u6742\u52a8\u4f5c\u5212\u5206\u4e3a sub-activitied\uff0c\u4f7f\u7528 SVM \u7ed3\u5c40 Action Classification \u95ee\u9898</p> <p>\u672c\u6587\u4f7f\u7528 GCN \u6784\u5efa\u6df1\u5c42 hierarchical model\uff0c\u4ece\u800c\u89e3\u51b3 AQA \u95ee\u9898</p> </li> </ul> </li> </ul>"},{"location":"sum/GCN/#3-approach","title":"3 Approach","text":""},{"location":"sum/GCN/#31-clip","title":"3.1 Clip \u7279\u5f81\u63d0\u53d6","text":"<ul> <li> <p>Input \u8868\u793a</p> <ul> <li> <p>\u5047\u8bbe\u89c6\u9891\u5171\u6709 \\(T\\) \u5e27\uff0c\u5219\u53ef\u5c06\u8f93\u5165\u8bb0\u4e3a \\(\\mathcal{F} = \\{X_1,X_2, ..., X_T\\}\\)</p> </li> <li> <p>\u5047\u8bbe\u5355\u5e27\u7684\u50cf\u7d20\u4e3a \\(W \\times H\\)\uff0c\u5219\u7b2c \\(t\\) \u5e27\u53ef\u4ee5\u8bb0\u4e3a tensor: \\(X_t \\in \\mathbb{R}^{W\\times H \\times 3}\\)</p> </li> </ul> </li> <li> <p>Clip \u5212\u5206</p> <p>\u5c06 \\(\\mathcal{F}\\) \u5212\u5206\u4e3a 16-frames / each \u7684 \\(N\\) * clips\uff0c\u6211\u4eec\u5c06 clip \u5e8f\u5217\u8bb0\u4e3a \\(\\mathcal{F}_1, \\mathcal{F}_2, ..., \\mathcal{F}_N\\)</p> </li> <li> <p>Featrue Extraction</p> <p>\u4f7f\u7528 weight-sharing \u7684 I3D backbone \u5bf9\u6bcf\u4e00\u4e2a clip \u8fdb\u884c\u7279\u5f81\u63d0\u53d6\uff0c\u5f97\u5230 \\(f_1, f_2, ...,f_N \\in \\mathbb{R}^{C_1}\\)</p> <p>\u5176\u4e2d \\(C_1\\) \u8868\u793a feature demesion</p> <p>=&gt; \u628a\u8fd9\u4e00\u5768 feature \u62fc\u5408\uff0c\u5373\u53ef\u5f97\u5230\u7279\u5f81\u77e9\u9635 \\(\\mathbf{F} \\in \\mathbb{R}^{N \\times C_1}\\)</p> </li> </ul>"},{"location":"sum/GCN/#32-hgcn-modules","title":"3.2 HGCN Modules","text":""},{"location":"sum/GCN/#clip-refinement","title":"Clip Refinement","text":"<p>\u56e0\u4e3a\u7b49\u8ddd\u5212\u5206\u8fc7\u4e8e\u8349\u7387\uff0c\u8fd9\u4e00\u6a21\u5757\u901a\u8fc7\u5de6\u53f3\u8c03\u6574 boundary \u4f7f\u5f97 each clip \u5177\u6709\u5b8c\u5907\u7684\u8bed\u4e49\u4fe1\u606f</p> <p>\u4e0b\u9762\u662f\u4e24\u79cd\u53ef\u80fd\u7684\u8349\u7387\u5212\u5206\u60c5\u5f62\uff1a</p> <ol> <li>\u5f53\u524d clip \u5305\u542b\u7684\u8bed\u4e49\u4fe1\u606f\u4e0d\u5b8c\u6574\uff1a\u53ef\u80fd\u9700\u8981\u4ece \u524d/\u540e \u7684 clip \u632a\u7528\u4e00\u4e9b</li> <li>\u5f53\u524d clip \u5305\u542b\u4e86 \\(\\lt\\) 1 motion\uff1a\u9700\u8981\u5206\u7ed9 \u524d/\u540e \u7684 clip</li> </ol> <p>Feature of Action</p> <p>\u5047\u8bbe action feature \u53ef\u4ee5\u7531\u4e00\u7ec4\u6b63\u4ea4\u5750\u6807 \\(b_1,b_2, ... , b_B \\in \\mathbb{R}^{D_1}\\) \u8868\u793a\uff08\\(D_1\\) \u662f embedding dimension\uff09</p> <p>Clip Refinement Module</p> <p></p> <ul> <li> <p>\u65b9\u6848\uff1a\u5148\u901a\u8fc7 Motion Decomposition \u627e\u51fa\u52a8\u4f5c\u95f4 transition \u53d1\u751f\u7684\u4f4d\u7f6e\uff0c\u518d\u5bf9 clip \u8fdb\u884c\u5747\u8d2b\u5bcc\u5904\u7406</p> </li> <li> <p>\u7ed3\u679c\uff1a\u4ece clip feature \\(\\mathbf{F}\\) \u751f\u6210 shot feature \\(\\mathbf{S}\\)</p> </li> </ul>"},{"location":"sum/GCN/#1-motion-decomposition","title":"1 Motion Decomposition","text":"<p>\u5728\u8fd9\u4e00\u6b65\u4e2d\uff0c\u6211\u4eec\u9700\u8981\u5c06\u62bd\u53d6\u7684\u7279\u5f81 \\(f_i\\) \u6620\u5c04\u5230 \"\u6f5c\u5728\u6d41\u5f62\u7a7a\u95f4 latent manifold space\" \uff0c\u6709\uff1a</p> \\[ m_i = \\lambda_i^1 b_1 + \\lambda_i^2 b_2 + ... + \\lambda_i^B b_B \\] <p>\u5176\u4e2d \\(\\{\\lambda_i^j\\}\\) \u662f\u4e00\u7ec4\u7528\u4e8e\u63cf\u8ff0 action freature \u7684\u6b63\u4ea4\u5750\u6807</p> <p>\u8fd9\u4e00\u64cd\u4f5c\u53ef\u4ee5\u901a\u8fc7 NN \u5b9e\u73b0\uff0c\u8bb0\u4e3a\u4ee5\u4e0b\u5f62\u5f0f\uff1a</p> \\[ m_i = ReLU(Conv1D_{group}(f_i,g)) \\] <ul> <li> <p>\\(Conv1D_{group}(f,g)\\) \u662f <code>groupSize = g</code>\uff0c<code>kernelSize=1</code> \u7684 1D-Conv \u64cd\u4f5c</p> <p>\u5404 clip \u5171\u4eab\u76f8\u540c\u6743\u91cd\u7684 1D-Conv</p> </li> <li> <p>\u9009\u62e9 ReLU \u662f\u56e0\u4e3a\u8ba1\u7b97\u66f4\u9ad8\u6548</p> </li> </ul> <p>Convenience</p> <p>\u901a\u8fc7\u6bd4\u8f83 \\(m_i, m_j\\) \u5bf9\u5e94\u7684\u7cfb\u6570 \\(\\lambda_i^x, \\lambda_j^x\\)\uff0c\u6211\u4eec\u53ef\u4ee5\u65b9\u4fbf\u7684\u8ba1\u7b97\u4efb\u610f\u4e24\u4e2a clip \u4e4b\u95f4\u7684 direction &amp; magnitude</p>"},{"location":"sum/GCN/#2-motion-graph-construction","title":"2 Motion Graph Construction","text":"<p>\u6211\u4eec\u5c06\u6709\u5411\u56fe Motion Graph \u8bb0\u4e3a \\(\\mathcal{G}_{mot} = (\\mathcal{V}_{mot},\\mathcal{E}_{mot})\\)</p> <ul> <li>\\(\\left|\\mathcal{V}_{mot}\\right|\\) \u662f motion nodes \u6784\u6210\u7684\u70b9\u96c6</li> <li>\\(\\left|\\mathcal{E}_{mot}\\right|\\) \u662f\u6709\u5411\u8fb9\uff0c\u4f7f\u7528\u90bb\u63a5\u77e9\u9635 \\(\\mathbf{A}_{mot} \\in \\mathbb{R}^{N \\times N}\\) \u8868\u793a</li> </ul> <p>\u7531\u4e8e information transfer \u53ef\u80fd\u5728\u8fde\u7eed\u7684 \\(r\\) \u4e2a clip \u95f4\u8fdb\u884c\uff0c\u6211\u4eec\u53ef\u4ee5\u5c06 \\(\\mathbf{A}_{mot}\\) \u8bb0\u4e3a\uff1a</p> \\[ A_{mot}^{ij} =  \\left\\{     \\begin{align*}         &amp;1,\\text{ if } \\|i-j\\| \\lt r \\\\         &amp;0, \\text{ otherwise }     \\end{align*} \\right. \\] <p>\u53ef\u4ee5\u770b\u6210\u4e00\u4e2a <code>length=r</code> \u7684\u6ed1\u52a8\u7a97\u53e3\u906e\u7f69</p> <p>\u5982\u4f55\u51b3\u5b9a frames \u79fb\u52a8\u7684 direction (\u524d/\u540e) &amp; magnitude (\u5177\u4f53\u79fb\u52a8\u591a\u5c11\u4e2a clip)\uff1f</p> <ul> <li> <p>\u5b66\u4e60\u8ddd\u79bb\u51fd\u6570 \\(d_{ij}\\) \u6765\u8861\u91cf \\((m_i, m_j)\\) \u4e4b\u95f4\u7684\u8ddd\u79bb\uff08\u4e3b\u8981\u5c31\u662f\u5b66\u53c2\u6570\u77e9\u9635 \\(\\mathbf{W}_{mot} \\in \\mathbb{R}^{D_1 \\times 1}\\)</p> \\[     d_{ij} = \\tanh((m_i - m_j) \\cdot \\mathbf{W}_{mot}) \\in [-1,1] \\] <ul> <li> <p>\\(sign(d_{ij})\\) \u8868\u793a transfer \u53d1\u751f\u7684\u65b9\u5411\uff08\u524d/\u540e/\u4e0d\u53d8\uff09</p> <ul> <li> <p>\\(sign(d_{ij}) \\lt 0\\) \u8868\u793a clip-i \u9700\u8981\u4ece clip-j \u63a5\u6536\u6570\u91cf\u4e3a \\(\\|d_i{j}\\|\\) \u7684 feature\uff08\u6b64\u65f6 transition \u53d1\u751f\u5728 clip-i \u5185\uff09</p> </li> <li> <p>\\(sign(d_{ij}) == 0\\) \u8868\u793a\u8fb9\u754c\u4e0d\u9700\u8981\u79fb\u52a8</p> </li> </ul> </li> <li> <p>\\(\\|d_i{j}\\|\\) \u8868\u793a transfer \u7684\u68af\u5ea6\uff08\u5177\u4f53\u6a2a\u8de8\u51e0\u4e2a clip\uff09</p> </li> </ul> </li> </ul>"},{"location":"sum/GCN/#3-information-transfer","title":"3 Information Transfer","text":"<p>\u8fd9\u91cc\u4f7f\u7528\u4e00\u4e2a GCN Layer \u5b8c\u6210 \\((f_i,f_j)\\) \u4e4b\u95f4\u7684 transfer\uff0c\u5f97\u5230 shot feature \\(s_i \\in \\mathbb{R}^{C_2}\\)\uff1a</p> \\[ s_i = ReLU\\left(     f_i \\mathbf{W}_{tra} + \\sum_{j \\in \\mathcal{N}_i} B_{mot}^{ij}\\ f_j\\ \\mathbf{U}_{tra} \\right) \\] <ul> <li>\u5176\u4e2d\uff1a\\(B_{mot}^{ij} = A_{mot}^{ij} \\cdot d_{ij}\\)\uff0c\\(\\mathbf{W}_{tra}, \\mathbf{U}_{tra} \\in \\mathbb{R}^{C_1 \\times C_2}\\)\uff0c\\(\\mathcal{N}_i\\) \u662f clip-i \u7684\u90bb\u57df\uff08\u524d\u540e \\(r\\) \u4e2a\uff09</li> </ul>"},{"location":"sum/GCN/#4-sequential-property","title":"4 Sequential Property","text":"<ul> <li> <p>\u5148\u524d\u7684\u5de5\u4f5c\u8bc1\u660e\uff1aEncoder \u53ef\u4ee5\u6709\u6548\u7684\u5c06 raw data \u6620\u5c04\u5230 motion manifold</p> </li> <li> <p>\u4f46\u662f seperate clip encoding \u4e0d\u80fd\u4fdd\u7559 sequential property</p> <p>=&gt; \u76f8\u90bb clip \u4e4b\u95f4\u7684\u8ddd\u79bb\u5f88\u5c0f\uff0c\u800c\u4e0d\u76f8\u90bb clip \u4e4b\u95f4\u7684 clip \u8f83\u5927</p> </li> <li> <p>\u5e38\u7528\u7684 Laplace \u6b63\u5219\u5316\u8003\u8651\u4e86 \u65f6\u7a7a \u4e0a\u7684 closeness\uff0c\u4f46\u8fd9\u5728\u672c\u6587\u4e2d\u4e0d\u592a\u9002\u7528</p> <p>=&gt; \u63d0\u51fa\u4e86 clip-level closeness \u7684\u5ea6\u91cf \\(O\\)\uff1a</p> \\[ \\begin{align*} O &amp;= \\mathop{\\min}_h \\sum+{i=1}^N \\sum_{j=1}^N A_{mot}^{ij} \\|h_i - h_j\\|_2^2\\\\ &amp;= \\mathop{\\min}_H \\text{tr}(\\mathbf{HLH}^T) \\end{align*} \\] <ul> <li>\\(h_i \\in \\mathbb{R}^{C_2}\\) \u662f clip feature (\\(f_i\\)) emedding \u540e\u7684\u7ed3\u679c</li> <li>\\(\\mathbf{H} \\in \\mathbb{R}^{N \\times C_2}\\) \u662f \\(\\{h_i\\}\\) \u5f62\u6210\u7684\u77e9\u9635\uff0c\\(\\mathbf{L} \\in \\mathbb{R}^{N \\times N}\\) \u662f \\(A_{mot}\\) \u7684\u62c9\u666e\u62c9\u65af\u77e9\u9635</li> </ul> <p>Theorem</p> <p>\u5f53 \\(H = FW\\) \u65f6\uff0cGCN \u7684\u4f20\u64ad\u8fc7\u7a0b \\(\\Leftrightarrow\\) \u4f18\u5316\u4e0a\u8ff0\u7684 clip-level \u6b63\u5219\u5316\u8868\u8fbe\u5f0f</p> </li> </ul>"},{"location":"sum/GCN/#scene-construction","title":"Scene Construction","text":"<p>\u65e8\u5728\u5c06\u8fde\u7eed\u7684\u51e0\u4e2a shots \u7ec4\u5408\u6210\u4e00\u4e2a meaningful scene</p> <ul> <li> <p>\u672c\u6587\u540c\u6837\u8ba4\u4e3a\u540c\u4e00\u7c7b\u7684 Video \u7531\u56fa\u5b9a\u7684 Action Procedures \u6784\u6210\uff0c\u6bd4\u5982 \u8df3\u6c34 = \u8d77\u8df3 -&gt; \u7ffb\u6eda -&gt; \u5165\u6c34</p> </li> <li> <p>\u800c \u672c\u6a21\u5757 \u53ef\u4ee5\u6355\u6349\u5404\u4e2a procedure \u7684\u6240\u6709 subtle diff\uff0c\u4ece\u800c\u51c6\u786e\u7684\u5bf9\u7ec6\u8282\u8fdb\u884c\u8bc4\u4f30</p> </li> <li> <p>Shot Graph Construction</p> <ul> <li> <p>\u4e0e Motion-Graph \u7c7b\u4f3c\uff0c\u53ef\u4ee5\u5c06 Shot Graph \u89c6\u4f5c \u70b9\u96c6 + \u8fb9\u96c6 \u53ea\u548c\uff0c\u8bb0\u4e3a\uff1a</p> \\[ \\mathcal{G}_{sht} = (\\mathcal{V}_{sht},\\mathcal{E}_{sht}) \\] <p>\u4f46\u4e0d\u540c\u4e8e Motion Graph \u91cf\u5316\u4e0d\u540c shot \u4e4b\u95f4\u7684 diffrerence\uff0cShot Graph \u65e8\u5728\u5bfb\u627e\u540c\u4e00 scene \u4e2d\u5404\u4e2a shot \u4e4b\u95f4\u7684 similarity</p> </li> <li> <p>\u7c7b\u4f3c\u7684\uff0c\u8bbe\u7f6e <code>neighborhoodSize = K</code>\uff0c\u53ef\u4ee5\u5f97\u5230\u63a7\u5236 \\(s_i,s_j\\) \u4e4b\u95f4 connection \u7684\u90bb\u63a5\u77e9\u9635 \\(A_{sht}^{ij}\\)</p> </li> <li> <p>\u6211\u4eec\u5047\u8bbe\uff1a\u5c5e\u4e8e\u540c\u4e00 scene \u7684\u5404 shots \u4e4b\u95f4\u7684\u8fde\u63a5\u66f4\u52a0\u7d27\u5bc6</p> <p>\u663e\u7136\uff0c0-1 \u90bb\u63a5\u77e9\u9635 \\(A_{sht}^{ij}\\) \u65e0\u6cd5\u5bf9\u6b64\u8fdb\u884c\u8861\u91cf</p> <p>\u4e3a\u6b64\uff0c\u4f5c\u8005\u63d0\u51fa\u4e86\u4f7f\u7528 self-attention \u7684\u53ef\u5b66\u4e60\u90bb\u63a5\u77e9\u9635 \\(B_{ij}\\) \u5bf9 \\(s_i,s_j\\) \u4e4b\u95f4\u7684\u90bb\u63a5\u7d27\u5bc6\u6027\u8fdb\u884c\u91cf\u5316\uff1a</p> \\[ B_{sht}^{ij} = \\text{softmax}\\left(     \\frac{(s_iW_1) \\cdot (s_jW_2)^T}{\\sqrt{D_2}} \\right) \\] </li> </ul> </li> <li> <p>Shot Graph Aggregation</p> <p>\u8fd9\u4e5f\u53ef\u4ee5\u7528\u4e00\u4e2a GCN Layer \u5b9e\u73b0\uff1a</p> \\[ s'_i = ReLU(s_iW_{sht} + \\sum_{j\\in \\mathcal{N}_i} A_{adp}^{ij}\\ s_j\\ U_{sht}) \\] <p>\u5176\u4e2d \\(A_{adp}^{ij}\\) \u89c6\u60c5\u51b5\u9009\u62e9 \\(A_{sht} \\odot B_{sht}\\) \u6216 \\(A_{sht} + B_{sht}\\)</p> </li> <li> <p>Shot Reduction</p> <p>\u5c3d\u7ba1\u8df3\u6c34\u6bd4\u8d5b\u90fd\u6709\u76f8\u540c\u7684 procedure \u5e8f\u5217\uff0c\u4f46\u5404 procedure \u53ef\u80fd\u5305\u542b\u4e0d\u540c\u7684 shots</p> <p>=&gt; \u4e3a\u6b64\uff0c\u6587\u7ae0\u901a\u8fc7 GCN Layer \u5b66\u4e60\u77e9\u9635 \\(T\\) \u5b9e\u73b0 \\(N*shots \\rightarrow S*scenes\\) \u7684 \u6620\u5c04</p> \\[ T = \\text{softmax}(\\oplus_{i=1}^N A_{adp}\\ \\sum_{j=1}^N\\ s'_j\\ W_{tft}) \\] </li> </ul> <p>\u6700\u540e\u6211\u4eec\u53ef\u4ee5\u5f97\u5230 scenes \u7684\u8868\u793a\uff1a</p> \\[ E = S'T \\] <p>\u5176\u4e2d \\(E\\) \u662f \\(S\\) \u4e2a scenes \\(\\{e_i\\}\\) \u62fc\u63a5\u5f97\u5230\u7684\uff0c\\(S'\\) \u662f\u66f4\u65b0\u540e\u7684 shot feature \\(\\{s'_i\\}\\) \u62fc\u63a5\u5f97\u5230\u7684</p>"},{"location":"sum/GCN/#action-aggregation","title":"Action Aggregation","text":"<p>\u7528\u4e8e\u6355\u6349\u4e00\u4e9b global dynamics\uff0c\u7528\u4e8e\u8861\u91cf\u603b\u4f53\u8868\u73b0</p> <p>\u7531\u4e8e\u52a8\u4f5c\u53ea\u80fd\u4ece i \u5355\u5411\u8fc7\u6e21\u5230 i+1\uff08\u4e0d\u80fd\u53cd\u8fc7\u6765\uff09</p> <p>=&gt; \u6211\u4eec\u9700\u8981\u5b66\u4e60\u5404\u4e2a procedure \u4e4b\u95f4\u7684\u4f9d\u8d56\u5173\u7cfb\uff0c\u5e76\u6700\u7ec8\u805a\u5408\u5f97\u5230 video-level representation</p> <ol> <li> <p>Scene Graphp Construction</p> \\[ \\mathcal{G}_{sce} = (\\mathcal{V}_{sce},\\mathcal{E}_{sce}) \\] <p>\u4e3a\u4e86\u63a7\u5236\u4fe1\u606f\u4e25\u683c \u524d \\(\\rightarrow\\) \u540e \u6d41\u52a8\uff0c\u6b64\u5904 <code>A_sce[i][j] = (i-j &gt; 0) ? 1 : 0</code></p> <p>\u56e0\u4e3a\u6b64\u5904\u540c\u6837\u9700\u8981\u8fde\u63a5\u7684 \u65b9\u5411 &amp; \u68af\u5ea6\uff0c\u6240\u4ee5\u4e5f\u9700\u8981\u5b66\u4e60 adaptive weight:</p> \\[ B_{sce}^{ij} = \\text{softmax}\\left(\\frac{(e_i\\ U_1)\\cdot(e_j\\ U_2)^T}{\\sqrt{D_3}}\\right) \\] </li> <li> <p>Video-level Aggregation</p> <p>\u8fd8\u662f\u4f7f\u7528 GCN Layer \u5b9e\u73b0\uff1a</p> \\[ e'_i = ReLU(e_i\\ W_{act} + \\sum_{j\\in \\mathcal{{N}_i}}(A_{sce} \\cdot B_{sce})\\ e_j\\ U_{act}) \\] <p>\u66f4\u65b0\u540e\u7684 secene feature \\(\\{e'_i\\}\\) \u5c06\u4f5c\u4e3a\u6700\u540e\u7528\u4e8e\u56de\u5f52\u7684 video-level representation</p> </li> </ol>"},{"location":"sum/GCN/#33-score-regression","title":"3.3 Score Regression","text":"<p>\u5e76\u4e0d\u662f\u76f4\u63a5\u56de\u5f52\uff0c\u4ece\u800c\u964d\u4f4e\u8bc4\u59d4\u4e3b\u89c2\u56e0\u7d20\u7684\u5f71\u54cd</p> <ul> <li> <p>\u964d\u4f4e\u8bc4\u59d4\u4e3b\u89c2\u56e0\u7d20\u5f71\u54cd\u7684\u5e38\u89c1\u65b9\u6cd5\u662f uncertainty-aware score distribution learning</p> <p>\u4e3b\u8981\u601d\u8def\u662f\u5b66\u4e60\u4e00\u4e2a\u5206\u6570\u5206\u5e03\uff0c\u4f7f\u5f97\u4e24\u4e2a\u4e0d\u540c action \u4e4b\u95f4\u7684\u5f97\u5206\u5177\u6709\u8f83\u5927\u5dee\u5f02</p> </li> <li> <p>\u76f8\u4f3c\u7684\uff0c\u672c\u6587\u8ba4\u4e3a\u6700\u7ec8\u7684\u5206\u6570\u662f\u4e00\u4e2a random variable\uff0c\u5e76\u5bf9\u5176\u5206\u6570\u5206\u5e03\u6982\u7387\u51fd\u6570\u8fdb\u884c\u5b66\u4e60</p> </li> </ul> <p>\u5bf9\u4e8e video-level representation \\(v (e')\\):</p> <ol> <li> <p>\u4f7f\u7528 Probabilistic Encoder \\(\\mathbb{R}^{C_4} \\rightarrow \\mathbb{R}\\) \u5c06\u5176\u6620\u5c04\u4e3a random score variable \\(s\\)</p> </li> <li> <p>\u53d8\u91cf \\(s\\) \u5e94\u5f53\u670d\u4ece\u9ad8\u65af\u5206\u5e03\uff0c\u5373\uff1a</p> \\[     p(s;v) = \\frac{1}{\\sqrt{2\\pi\\sigma^2(v)}} \\exp(- \\frac{(s-\\mu(v))^2}{2\\sigma^2(v)}) \\] <p>\u5176\u4e2d \\(\\mu\\) \u5bf9\u5e94\u5747\u503c\uff0c\\(\\sigma\\) \u5bf9\u5e94\u6807\u51c6\u5dee</p> </li> <li> <p>Sample \\(\\varepsilon\\) from \u6807\u51c6\u5206\u5e03 \\(\\mathcal{N}(0,1)\\)\uff0c\u4ece\u800c\u786e\u4fdd encoder \u7684\u8bad\u7ec3\u8fc7\u7a0b\u7684\u53ef\u884c\u6027</p> <p>\u8fd9\u6837\u4e00\u6765 score distribution sampling \u8fc7\u7a0b\u5c31\u662f \u53ef\u5fae\u7684(differentiable)</p> </li> <li> <p>\u57fa\u4e8e\u968f\u673a\u72ec\u7acb\u91c7\u6837\u53d8\u91cf \\(\\varepsilon\\)\u3001\u5747\u503c \\(\\mu(v)\\) \u4e0e \u6807\u51c6\u5dee \\(\\sigma(v)\\) \u8ba1\u7b97\u6700\u7ec8\u7684\u9884\u6d4b\u7ed3\u679c \\(\\hat{s}\\) </p> \\[ \\hat{s} = \\mu(v) + \\varepsilon \\cdot \\sigma(v) \\] </li> </ol>"},{"location":"sum/GCN/#34-loss-function","title":"3.4 Loss Function","text":"<p>\u5bf9\u4e8e score distribution regression \u8fc7\u7a0b\uff0c\u4f7f\u7528 MSE Loss \u8fdb\u884c\u8bc4\u4f30\uff1a</p> \\[ \\mathcal{L}_{MSE} = \\frac{1}{N} \\sum_{i=1}^N (s_i - \\hat{s}_i)^2 \\]"},{"location":"sum/Regression-Based/","title":"Video-Level \u56de\u5f52\u9884\u6d4b","text":""},{"location":"sum/Regression-Based/#2022-tsa-net","title":"2022: TSA-Net","text":""},{"location":"sum/Regression-Based/#1-abstract","title":"1 Abstract","text":"<ul> <li> <p>\u73b0\u6709\u5de5\u4f5c\u7684\u4e0d\u8db3\uff1a</p> <ol> <li> <p>\u76f4\u63a5\u8fc1\u79fb Action Recognition\uff0c\u5ffd\u7565\u4e86\u6700\u672c\u8d28\u7684\u7279\u5f81 \u2014\u2014 foreground &amp; background info</p> <ul> <li>HAR \u7684\u91cd\u70b9\u5728\u4e8e \u2014\u2014 \u533a\u5206 \u4e0d\u540c\u7684\u52a8\u4f5c</li> <li>QAQ \u7684\u91cd\u70b9\u5728\u4e8e \u2014\u2014 \u5bf9\u4e8e \u7279\u5b9a\u52a8\u4f5c \u7684\u4f18\u7f3a\u70b9\u8fdb\u884c\u8bc4\u4f30</li> </ul> <p>=&gt; \u5bf9\u4e24\u8005\u91c7\u7528\u76f8\u540c\u7684 \u7279\u5f81\u63d0\u53d6 \u65b9\u5f0f\u663e\u7136\u662f\u4e0d\u5bf9\u7684</p> </li> <li> <p>Feature Aggregasion \u6548\u7387\u4f4e</p> <p>\u53d7\u5377\u79ef\u64cd\u4f5c\u7684\u611f\u53d7\u91ce\u5927\u5c0f\u9650\u5236\uff0c\u5bfc\u81f4\u635f\u5931\u4e86 long-range dependencies (?)</p> <p>\u770b\u8d77\u6765\u662f\u7f3a\u5c11\u65f6\u95f4\u4fe1\u606f</p> </li> <li> <p>RNN \u53ef\u4ee5\u901a\u8fc7\u9690\u85cf\u6001\u5b58\u50a8\u4e0a\u4e0b\u6587\uff0c\u4f46\u662f \u4e0d\u80fd\u5e76\u884c\u8ba1\u7b97</p> </li> </ol> <p>\u6240\u4ee5\uff0cAQA \u9700\u8981\u4e00\u4e2a \u9ad8\u6548\u7684 Feature Aggregation \u7b97\u6cd5 \uff01</p> </li> <li> <p>\u521b\u65b0\u70b9\uff1a</p> <ol> <li> <p>\u5f15\u5165 single object tracker</p> </li> <li> <p>\u63d0\u51fa Tube Self-Attention Module(TSA)</p> <ul> <li> <p>\u57fa\u4e8e tube \u673a\u5236 \u548c self-attention \u673a\u5236\uff0c\u5b9e\u73b0\u9ad8\u6548\u7684 feature aggregation</p> </li> <li> <p>\u901a\u8fc7 adopting sparse feature interactions \u751f\u6210\u4e30\u5bcc\u7684\u65f6\u7a7a\u4e0a\u4e0b\u6587\u4fe1\u606f</p> </li> </ul> </li> </ol> </li> </ul>"},{"location":"sum/Regression-Based/#attention-based-feature-aggregation","title":"Attention-Based Feature Aggregation","text":"<ol> <li> <p>Non-Local Opertaion</p> <p>\u6bcf\u4e00\u4e2a position \u4f1a\u548c \u6240\u6709 feature \u505a Dense Correlation</p> </li> <li> <p>Tube Self-Attention Operation</p> <p>\u4ec5\u5bf9\u5404\u65f6\u523b\u4e0b\u540c\u4e00 spatio-temporal tube \u5185\u7684\u6bcf\u4e00\u4e2a postion \u505a Sparse Correlation</p> <ul> <li>tube \u4f7f\u6a21\u578b\u4e13\u6ce8\u4e8e feature map \u7684\u4e00\u4e2a\u5b50\u96c6 \u2014\u2014 \u8fd0\u52a8\u5458\uff08\u5ffd\u7565\u80cc\u666f\uff09</li> <li>self-attention \u751f\u6210\u4e86 \u65f6\u95f4\u7ef4\u5ea6 \u4e0a\u7684 \u4e0a\u4e0b\u6587\u4fe1\u606f</li> </ul> </li> </ol>"},{"location":"sum/Regression-Based/#2-approach","title":"2 Approach","text":"<ol> <li> <p>\u5305\u542b\u4e24\u4e2a\u6b65\u9aa4\uff1a</p> <ul> <li> <p>\u4f7f\u7528\u5df2\u6709\u7684 Visual Object Tacking \u6a21\u578b\u751f\u6210 tracking box\uff0c\u7136\u540e\u901a\u8fc7 feature selection \u751f\u6210 tube\uff08ST-Tube\uff09</p> <p>\u5047\u8bbe\u8f93\u5165\u7684\u89c6\u9891\u5171\u6709 \\(L\\) \u5e27\uff0c\\(b_l\\) \u8868\u793a\u7b2c \\(l\\) \u5e27\u4e2d\u7684 Bounding-Box</p> \\[     V:\\text{L-frames Video} \\rightarrow VOT_{SiamMask} \\rightarrow B:\\text{BBoxs}\\{b_l\\}_{l=1}^L \\] </li> <li> <p>\u5c06\u89c6\u9891\u5212\u5206\u4e3a N-Clip \u4ea4\u7ed9 I3D-s1\uff08\u7acb\u4f53\u5377\u79ef\uff09\u751f\u6210 Feature Map</p> <p>\u5047\u8bbe \\(L=MN\\)\uff0c\u6211\u4eec\u5c06\u89c6\u9891\u5212\u5206\u4e3a \\(N\\) \u4e2a\u5305\u542b \\(M\\) \u4e2a\u8fde\u7eed\u5e27\u7684 Clip</p> <p>\u4f7f\u7528 I3D \u7b97\u6cd5\u53ef\u4ee5\u4ece\u4e2d\u751f\u6210 \\(N\\) \u4e2a features</p> \\[     V:\\text{N-fold Video} \\rightarrow I3D \\rightarrow X:\\text{features}\\{x_n\\}_{n=1}^N \\] </li> </ul> </li> <li> <p>\u5bf9 Feature Map \u4e2d tube \u6846\u5b9a\u7684\u90e8\u5206\u5e94\u7528 self-attention \u673a\u5236\uff0c\u4ece\u800c\u5b9e\u73b0 feature aggregation</p> <pre><code>    B: BBoxs    \\\n                 |--&gt; TSA Module --&gt; \u5305\u542b\u4e0a\u4e0b\u6587\u65f6\u7a7a\u4fe1\u606f\u7684 X': features (N\u4e2a)\n    X: features /\n</code></pre> <p>X \u4e0e X' \u7b49\u5927\uff1aTSA Module \u6ca1\u6709\u6539\u53d8 Feature Map \u7684 shape</p> <p>=&gt; \u8fd9\u4e00\u7279\u6027\u5141\u8bb8\u6211\u4eec\u5806\u53e0 TSA\u4ee5\u83b7\u5f97\u66f4\u4e30\u5bcc\u7684\u65f6\u7a7a\u4fe1\u606f</p> </li> <li> <p>\u5c06 Aggregated Feature \u4ea4\u7ed9 I3D-Stage2 \u751f\u6210 \\(H\\)</p> \\[     X' \\rightarrow I3D_{s2} \\rightarrow H: \\text{features}\\{h_n\\}_{n=1}^N \\] </li> <li> <p>Network Head</p> <ol> <li> <p>\u4f7f\u7528 AvgPooling \u5728 Clip \u7eac\u5ea6\u4e0a\u8fdb\u884c fuse</p> \\[     \\overline{h} = \\frac{1}{N} \\sum_{n=1}^N h_n \\] </li> <li> <p>\u8fc1\u79fb\u8bad\u7ec3 MLP_Block \u5bf9 \\(H\\) \u8fdb\u884c\u6253\u5206</p> \\[     \\overline{h} \\rightarrow \\text{MLP-Block} \\rightarrow Score \\] </li> </ol> </li> </ol>"},{"location":"sum/Regression-Based/#tsa-module","title":"TSA Module","text":"<p>\u6839\u636e Bounding-Box \u5bf9 I3D \u751f\u6210\u7684 feature-map \u8fdb\u884c\u8fc7\u6ee4\uff0c\u51cf\u5c11 self-attention \u5904\u7406\u7684\u6570\u636e\u91cf</p> <p>\u4ed6\u8bf4\u8fd9\u80fd\u53bb\u9664\u80cc\u666f\u566a\u58f0\u7684\u5e72\u6270 =&gt; Non-local Module \u4f1a\u4f7f\u7528\u6574\u4e2a Feature-Map\uff0c\u4ece\u800c\u5f15\u5165\u80cc\u666f\u566a\u58f0</p> <ol> <li> <p>Tube \u751f\u6210</p> <ul> <li> <p>\u7531\u4e8e I3D-s1 \u4f7f\u7528\u4e86 2 * temporal Pooling (\\(/2^2\\))\uff0cBounding-Box:feature-map X = 4:1</p> </li> <li> <p>\u95ee\u9898\uff1aSiamMask \u4f1a\u4ea7\u751f \u5e73\u884c\u56db\u8fb9\u5f62 \u8fb9\u6846 -&gt; \u6211\u4eec\u4e0d\u597d\u76f4\u63a5\u5728 feature-map \u91cc\u6846\u65b9\u683c</p> <p>\u89e3\u51b3\uff1a \u4f7f\u7528\u5982\u4e0b\u7684 alignment \u7b56\u7565\u751f\u6210 mask</p> <ol> <li>\u5c06\u8fde\u7eed\u7684 4 \u4e2a BBox \u7f29\u653e\u5230\u548c feature-map \u4e00\u6837\u7684\u5927\u5c0f</li> <li>\u5206\u522b\u751f\u6210 4 \u4e2a Mask\uff1a\u8986\u76d6\u9762\u79ef\u8d85\u8fc7\u9608\u503c \\(\\tau = 0.5\\) \u8bb0 1\uff0c\u5426\u5219\u4e3a 0</li> <li>\u4f7f\u7528\u5e76\u96c6\u4f5c\u4e3a\u6700\u7ec8\u7684 Mask' \\(M_{c,t}^{l\\rightarrow l+3}\\)</li> </ol> </li> <li> <p>\u6700\u7ec8\u53c2\u4e0e self-attention \u662f\u6240\u6709 mask=1 \u7684 feature: \\(\\Omega_{c,t}\\)</p> \\[     \\Omega_{c,t} = \\{(i,j)|M_{c,t}^{l\\rightarrow l+3}(i,j)=1\\} \\] </li> </ul> </li> <li> <p>Self-Attention</p> <p>\u751f\u6210\u4e86\u548c feature-map \\(x\\) \u7b49\u5927\u7684 \\(y\\)\uff0c\u8f93\u51fa\u4e24\u8005\u7684 residual \\(x' = Wy + x\\)</p> <p> \u672c\u6587\u4e2d\u7684 TSA \u7b97\u6cd5</p> </li> <li> <p>Nertwork Head</p> <p>\u901a\u8fc7\u4fee\u6539 MLP_Block \u7684\u8f93\u51fa\u5927\u5c0f &amp; Loss Func\uff0cNetwork Head \u5c31\u53ef\u4ee5\u517c\u5bb9\u4e0d\u540c\u7684\u4efb\u52a1</p> <ul> <li>Classification\uff1a\u8f93\u51fa\u7531 n_class \u51b3\u5b9a + BCE Loss</li> <li>Regression\uff1a\u8f93\u51fa 1 \u7ef4\u5411\u91cf + MSE Loss</li> <li> <p>Score distribution prediction\uff1a</p> <ul> <li> <p>\u5728 USDL \u4e2d\u5d4c\u5165 TSA Module</p> </li> <li> <p>Loss = Kullback-Leibler (KL) divergence of predicted score distribution and ground-truth (GT) score distribution</p> </li> </ul> </li> </ul> </li> </ol>"},{"location":"sum/Segmentation/","title":"\u72e0\u72e0\u5207\u5272","text":""},{"location":"sum/Segmentation/#2018-s3d","title":"2018: S3D","text":"<p>Stacking Segmental P3D for Action Quality Assessment</p> <p>\u9996\u4e2a Stage by Stage \u8bc4\u5206\u7684\u65b9\u6cd5</p>"},{"location":"sum/Segmentation/#1-abstract","title":"1 Abstract","text":"<ul> <li> <p>\u5148\u524d\u7684\u5de5\u4f5c</p> <ul> <li> <p>C3D (Convolutional 3D Network)</p> <ul> <li>\u4f18\u52bf\uff1a\u53ef\u4ee5\u540c\u65f6\u6355\u6349 appearance &amp; subtle motion cues</li> <li>\u7f3a\u70b9\uff1a\u8bad\u7ec3\u6d88\u8017\u4f1a\u6d88\u8017\u8f83\u591a\u5185\u5b58\uff0c\u53ea\u80fd\u5904\u7406\u7b49\u5e27\u5207\u5272\u7684 clip</li> </ul> </li> <li> <p>\u66f4\u591a\u5de5\u4f5c\u5c06 C3D \u5206\u89e3\u4e3a \u201c\u4e24\u6b65\u8d70\u201d\uff1a</p> <ol> <li>\u5728\u56fe\u50cf\u4e0a\u8bad\u7ec3\u7684 2D-CNN\uff08ResNet\uff09</li> <li>\u57fa\u4e8e 2D feature \u751f\u6210\u65f6\u5e8f\u7279\u5f81\u7684 1D-CNN\uff08LSTM\uff09</li> </ol> <p>\u4f46\u9664\u4e86 Two-stream \u5916\u7684\u65b9\u6cd5\u65e0\u6cd5\u6355\u6349 motion cue</p> </li> <li> <p>\u4fe1\u53f7\u5904\u7406\u9886\u57df\u7684\u7814\u7a76\u8868\u660e\uff0c\u4e00\u4e2a filter \u5f80\u5f80\u53ef\u4ee5\u7531\u82e5\u5e72\u66f4\u7b80\u5355\u7684 filter \u76f8\u4e58\u5f97\u5230</p> <p>P3D \u548c I3D \u5de5\u4f5c\u5df2\u7ecf\u8bc1\u660e\u5728\u5927\u89c4\u6a21\u6570\u636e\u96c6(Kinetics)\u4e0a\u8bad\u7ec3\u76843D\u5377\u79ef\u53ef\u88ab\u9690\u5f0f\u5206\u89e3</p> <p>=&gt; \u4f46\u6211\u4eec\u5f88\u96be\u89e3\u91ca\u5177\u4f53\u662f\u54ea\u4e00\u4e2a\u90e8\u5206\u8d77\u5230\u4e86\u5173\u952e\u4f5c\u7528</p> </li> </ul> </li> <li> <p>\u672c\u6587\u5de5\u4f5c</p> <ul> <li> <p>\u57fa\u4e8e ED-TCN\uff0c\u63d0\u51fa\u4e86 Segment-based P3D-fused Network (S3D)</p> <p>\u5bf9\u4e8e\u6bcf\u4e2a segment \u5206\u522b\u5e94\u7528 P3D\uff0c\u518d\u8fdb\u884c aggregation</p> </li> <li> <p>\u8bc1\u660e segment-aware \u7684\u8bad\u7ec3\u65b9\u5f0f\u5f3a\u4e8e full-video \u8bad\u7ec3</p> <p>\u53d1\u73b0 full-video P3D \u548c\u53ea\u89c2\u5bdf \u201c\u6c34\u82b1\u201d \u7684\u7ed3\u679c\u7c7b\u4f3c\uff08\u4f46\u6ca1\u6709\u5173\u6ce8\u5165\u6c34\u524d\u7684\u52a8\u4f5c\u5e8f\u5217\uff09</p> </li> <li> <p>\u8bc1\u660e temporal segmentation \u53ef\u4ee5\u4ee5\u8f83\u5c0f\u7684\u4ee3\u4ef7\u5b8c\u6210</p> </li> <li>benchmark: UNLV-Dive</li> </ul> </li> </ul>"},{"location":"sum/Segmentation/#2-related-works","title":"2 Related Works","text":""},{"location":"sum/Segmentation/#diving-skill-assessment","title":"Diving Skill Assessment","text":"<ul> <li>\u65e9\u671f\u6709\u4f7f\u7528 approximate entropy features \u7684\u5de5\u4f5c</li> <li>Pose + SVR on MIT-Diving</li> <li>C3D + SVR on UNLV-Dive</li> </ul>"},{"location":"sum/Segmentation/#3d-cnn-stn","title":"3D-CNN &amp; STN","text":"<ul> <li> <p>3D-CNN</p> <ul> <li> <p>3D-CNN \u5e76\u4e0d\u662f\u7279\u522b\u4e3a\u4e86\u89c6\u9891\u5206\u6790\u4efb\u52a1\u8bbe\u8ba1\u7684\uff08patch-level 3D-CNN \u662f\u533b\u5b66\u56fe\u50cf\u5206\u6790\u7684\u91cd\u8981\u624b\u6bb5\uff09</p> <p>\u4e8b\u5b9e\u4e0a\uff0c3D-CNN \u662f STN \u7684\u66ff\u4ee3\u54c1\uff08\u6bd5\u7adf\u65f6\u95f4\u548c\u7a7a\u95f4\u5b58\u5728\u672c\u8d28\u4e0a\u7684\u4e0d\u540c\uff09</p> </li> <li> <p>\u5728\u9884\u8bbe\u5e27\u6570\u89c6\u9891\u4e0a\u8bad\u7ec3\u7684 3D-CNN \u8981\u6c42\u8f93\u5165\u4e5f\u6709\u540c\u6837\u7684\u5e27\u6570\uff08\u6bcf\u4e2a clip \u90fd\u5f97\u662f 16-frame\uff09</p> <p>\u5c0f\u7684 clipSize \u4e00\u662f\u51fa\u4e8e\u5185\u5b58\u5360\u7528\u7684\u8003\u8651\uff0c\u540c\u65f6\u4e5f\u4fdd\u8bc1\u4e86\u7279\u5f81\u7684 locality</p> </li> <li> <p>\u548c\u6355\u83b7 spatial local connectivity \u7684 2D-CNN \u76f8\u6bd4\uff0c3D-CNN \u5929\u7136\u652f\u6301\u5bf9 temporally local motion and coherence \u7684\u6355\u6349</p> </li> </ul> </li> <li> <p>TCN: \u5728 segmenting fine-grained actions \u4e0a\u5177\u5907 SOTA \u8868\u73b0</p> <ul> <li> <p>TCN \u5b9e\u9645\u4e0a\u662f 2D-CNN + LSTM \u7684\u7b80\u5316\u7248\u672c \u2014\u2014 \u5728 2D-CNN \u7684\u5012\u6570\u7b2c\u4e8c\u5c42\u4e0a\u6784\u5efa cross-frame \u7684 1D-CNN</p> <p>\u5176\u8f93\u5165\u662f\u7531 2D-CNN \u9010\u5e27\u63d0\u53d6\u7684\u4e00\u7ef4\u7279\u5f81</p> </li> <li> <p>Encoder-Decoder TCN (ED-TCN) \u901a\u8fc7 \u4e0b\u91c7\u6837-\u4e0a\u91c7\u6837 \u5b9e\u73b0 Encode-Decode\uff0c\u968f\u540e\u901a\u8fc7 <code>softmax</code> \u4e3a\u6bcf\u4e00\u5e27\u9884\u6d4b\u52a8\u4f5c\u7c7b\u578b\u6807\u7b7e</p> </li> </ul> </li> </ul>"},{"location":"sum/Segmentation/#3-approach","title":"3 Approach","text":"<p>Diving Video Segmentation</p> <p>\u4f5c\u8005\u8ba4\u4e3a Diving \u89c6\u9891\u53ef\u4ee5\u5212\u5206\u4e3a 5 \u4e2a\u7247\u6bb5\uff1a</p> <p>\u5b9e\u9645\u9700\u8981\u8bc4\u5206\u7684\u53ea\u6709 2-5 (4\u4e2a\u7247\u6bb5)</p> <ol> <li>Preparation\uff08\u4e0d\u8bc4\u5206\uff09</li> <li>Jumping: \u79bb\u5f00\u8df3\u53f0 -&gt; \u624b\u7b2c\u4e00\u6b21\u78b0\u5230\u8eab\u4f53</li> <li>Dropping: \u624b\u7b2c\u4e00\u6b21\u78b0\u5230\u8eab\u4f53 -&gt; \u624b\u6700\u540e\u4e00\u6b21\u79bb\u5f00\u8eab\u4f53</li> <li>Entering into the water</li> <li>Water-spray decaying: \u6c34\u82b1\u9010\u6e10\u6d88\u5931</li> </ol>"},{"location":"sum/Segmentation/#1-full-video-p3d","title":"1 full-video P3D","text":"<ul> <li> <p>\u6539\u88c5 P3D</p> <ul> <li> <p>\u539f\u88c5\u7684 P3D \u9002\u7528\u4e8e action classification \u4efb\u52a1\uff0c\u4f46\u540c\u65f6\u8003\u8651\u4e86 appearance &amp; local motion</p> </li> <li> <p>\u4e3a\u4e86\u5b8c\u6210\u9002\u914d regression \u4efb\u52a1\uff0c\u4f5c\u8005\u628a\u6700\u540e\u4e00\u5c42\u6362\u6210\u4e86 Fully-Connected + Dropout</p> </li> </ul> </li> <li> <p>\u62c9\u8e29 C3D</p> <ul> <li>C3D \u7684 <code>3 * 3 * 3</code> kernel size \u662f\u786c\u6027\u8981\u6c42\uff0c\u8bad\u7ec3\u6210\u672c\u5927</li> <li> <p>P3D \u7684 <code>3 * 3 * 3</code> kernel \u662f \u53ef\u62c6\u5206\u7684\uff0c\u53ef\u4ee5\u770b\u6210 <code>3*3*1 + 1*1*3</code> \u7684\u7b80\u5355 kernel \u7ec4\u5408</p> <p>\u8fd9\u540c\u65f6\u5141\u8bb8\u7f51\u7edc\u72ec\u7acb\u8fdb\u884c 2D-Conv \u548c 1D-Conv</p> </li> <li> <p>\u4e0e ResNet \u7684\u601d\u60f3\u7c7b\u4f3c\uff0cP3D \u5141\u8bb8\u5c06\u8fd9\u4e9b\u5377\u79ef\u5c42\u505a\u6210 residual units\uff0c\u6839\u636e 2D(\\(S(\u00b7)\\))-1D(\\(T(\u00b7)\\)) \u7684\u6df7\u5408\u7b56\u7565\u4e0d\u540c\uff0c\u53ef\u4ee5\u8bb2\u5355\u5143\u5212\u5206\u4e3a\u4e09\u79cd\uff1a</p> <ol> <li>P3D-serial: \\(x_{t+1} = x_t + T(S(x_t))\\)</li> <li>P3D-parallel: \\(x_{t+1} = x_t + T(x_t) + S(x_t)\\)</li> <li>P3D-composition: \\(x_{t+1} = x_t + T(S(x_t)) + S(x_t)\\)</li> </ol> <p>\u5176\u4e2d \\(x_t, x_{t+1}\\) \u5206\u522b\u662f x-th \u5355\u5143\u7684\u8f93\u5165\u8f93\u51fa</p> </li> <li> <p>P3D \u8fd8\u91c7\u7528\u4e86 bottleneck \u7684\u8bbe\u8ba1\uff1a\u5728\u6bcf\u4e2a residual units \u524d\u540e\u52a0\u4e0a <code>1*1*1 Conv</code>\uff0c\u5206\u522b\u7528\u4e8e \u964d\u4f4e\u8f93\u5165\u7eac\u5ea6 &amp; \u589e\u957f\u8f93\u51fa\u7eac\u5ea6</p> </li> </ul> </li> </ul>"},{"location":"sum/Segmentation/#2-segment-level-p3d","title":"2 Segment-level P3D","text":"<p>\u9884\u8bad\u7ec3\u7684 P3D \u6a21\u578b\u53ea\u80fd\u5904\u7406 16-frame \u8f93\u5165\uff0c\u6211\u4eec\u5e94\u8be5\u600e\u4e48\u5bf9 segment \u63d0\u53d6\u7279\u5f81\u5462\uff1f</p> <p>\u53ea\u8981\u5728\u6bcf\u4e2a segment center-frame \u7684\u5de6\u53f3 16-frame \u63d0\u53d6\u7279\u5f81\u5c31\u597d\u5566</p> <ul> <li> <p>\u6211\u4eec\u80fd\u591f independently \u7684\u5bf9\u6bcf\u4e00\u4e2a stage \u8fdb\u884c\u7279\u5f81\u63d0\u53d6/\u5206\u6570\u9884\u6d4b\uff1a</p> <ul> <li> <p>use features: \u53d6 \\(Avg\\) =&gt; single feature</p> </li> <li> <p>use subscores: \u5408\u5e76\u6240\u6709 subscores =&gt; feature vec <code>[s1, s2, ..., sn]</code></p> </li> </ul> </li> <li> <p>\u901a\u8fc7\u4e0a\u9762\u7684\u65b9\u6cd5\uff0c\u6211\u4eec\u53ef\u4ee5\u83b7\u5f97 segmentation+P3D \u5904\u7406\u540e\u7684 feature set \\(x\\)</p> <p>\u52a0\u4e0a\u6574\u4e2a\u89c6\u9891\u7684 ground-truth label \\(y\\)\uff0c\u6211\u4eec\u53ef\u4ee5\u5f97\u5230\u4e00\u4e2a training-sample \\((x,y)\\)</p> </li> <li> <p>\u6700\u7ec8\u8bad\u7ec3\u4e00\u4e2a LR(\u903b\u8f91\u56de\u5f52) / SVR \u6a21\u578b\u8fdb\u884c\u5206\u6570\u7684\u56de\u5f52\u9884\u6d4b</p> </li> </ul>"},{"location":"sum/Segmentation/#3-teomporal-segmentation-using-tcn","title":"3 Teomporal Segmentation using TCN","text":"<ul> <li> <p>\u6b64\u5904\u7684 Temporal Segmentation \u5176\u5b9e\u53ef\u4ee5\u89c6\u4e3a\u4e00\u4e2a \u9010\u5e27\u4e94\u5206\u7c7b \u4efb\u52a1</p> <p>\u540c\u65f6\u9700\u8981\u4fdd\u8bc1 intra-class continuity\uff08\u6bcf\u4e2a\u9636\u6bb5\u662f\u8fde\u7eed\u7684\uff09</p> </li> <li> <p>\u8f93\u5165\uff1aframe-level 2D-CNN feature</p> <ul> <li> <p>\u5047\u8bbe\u8f93\u5165\u89c6\u9891\u5171\u6709 \\(K\\) \u5e27\uff0c2D-CNN \u7684\u8f93\u51fa\u7eac\u5ea6\u4e3a \\(D\\)\uff0c\u6211\u4eec\u53ef\u4ee5\u5c06 2D-CNN \u63d0\u53d6\u7684\u7279\u5f81\u8868\u793a\u4e3a \\(X_0 \\in \\mathbb{R}^{D \\times K}\\)</p> </li> <li> <p>\u6b64\u65f6\u6bcf\u4e00\u5c42 TCN \u7684\u8f93\u5165\u53ef\u4ee5\u88ab\u8bb0\u4e3a \\(X_i(i \\geq 0)\\)</p> </li> </ul> </li> <li> <p>\u8f93\u51fa\uff1a5 segments</p> </li> <li> <p>ED-TCN: \u6211\u4eec\u53ef\u4ee5\u8bb2 l-th Layer \u7684 temporalConv \u8fc7\u7a0b\u8bb0\u4e3a\u4ee5\u4e0b\u5f62\u5f0f</p> \\[ X_l = \\text{Activate}(W_l * X_{l-1} + b)\\\\ \\] \\[ X_l \\in \\mathbb{R}^{N_l \\times T_l},\\ W_l \\in \\mathbb{R}^{d_l \\times N_{l-1}},\\ b \\in \\mathbb{R}^{N_l} \\] <ul> <li>\u521d\u59cb\u6761\u4ef6\u4e3a \\((N_0, T_0) = (D,K)\\)</li> <li>\\(N_l\\) \u662f l-th Layer \u4e2d\u7684 <code>n_Conv_Filters</code>\uff0c\\(T_l\\) \u662f <code>len(feature)</code>\uff0c\\(d_l\\) \u662f l-th Layer \u7684 <code>len(filter)</code></li> </ul> </li> </ul>"},{"location":"sum/Segmentation/#2023-iris","title":"2023 IRIS","text":"<p>\u672c\u6587\u805a\u7126\u7684\u95ee\u9898\uff1a(\u82b1\u6ed1)\u5355\u4eba\u77ed\u8282\u76ee</p> <ul> <li> <p>\u8282\u76ee\u65f6\u957f\u7ea6\u4e3a 3min</p> </li> <li> <p>\u76f8\u6bd4\u4e8e\u81ea\u7531\u6ed1\uff0c\u8282\u76ee\u7f16\u6392\u53d7\u5236\u4e8e\u66f4\u591a\u7684\u89c4\u5219</p> </li> </ul>"},{"location":"sum/Segmentation/#1-abstract_1","title":"1 Abstract","text":"<ul> <li> <p>\u521b\u65b0\u70b9</p> <p>XAI \u5f00\u53d1\u8005\u5e94\u8be5\u5b66\u4e60 \u5177\u4f53\u5e94\u7528\u573a\u666f\uff08\u8fd0\u52a8\uff09\u7684\u8bc4\u5206\u6807\u51c6(rubrics)</p> <p>\u672c\u6587\u63d0\u51fa\u7684 Interpretable Rubric- Informed Segmentation\uff1a</p> <ul> <li> <p>\u5176\u7ed9\u51fa\u8bc4\u5206\u7684\u8fc7\u7a0b\u662f \u53ef\u89e3\u91ca \u7684\uff0c\u4f9d\u7167 rubric \u51b3\u5b9a what to consider</p> <p>\u4f7f\u7528 4 \u4e2a key feature \u8fdb\u884c \u9884\u6d4b&amp;\u89e3\u91ca: Score, Sequence, Segments, and Subscores</p> </li> <li> <p>\u5bf9\u8f93\u5165\u8fdb\u884c\u5206\u5272\uff0c\u4ee5\u5b9a\u4f4d\u5bf9\u5e94\u7279\u5b9a criteria \u7684\u7279\u6b8a section</p> </li> <li> <p>\u672c\u6587\u4ec5\u5bf9 figure skating \u8fdb\u884c\u6d4b\u5b9a\uff0c\u4f46\u65b9\u6cd5\u540c\u6837\u9002\u7528\u4e8e\u5176\u4ed6 Video-based AQA \u95ee\u9898</p> </li> </ul> </li> </ul>"},{"location":"sum/Segmentation/#2-relative-works","title":"2 Relative Works","text":"<ol> <li> <p>Action Quality Assessment</p> <p>\u76ee\u524d\u5df2\u7ecf\u6709\u8bb8\u591a\u9488\u5bf9 Sport \u53ca\u5176\u4ed6\u9886\u57df\u7684 AQA AI \u89e3\u51b3\u65b9\u6848\u88ab\u63d0\u51fa\uff0c\u901a\u8fc7\u8bad\u7ec3\u4e00\u4e2a Regression Model \u5bf9\u6700\u7ec8\u5206\u6570\u8fdb\u884c\u9884\u6d4b\u3002</p> <ul> <li> <p>\u8fd9\u4e9b\u65b9\u6cd5\u5f80\u5f80\u901a\u8fc7 3D CNN (C3D / I3D) \u8fdb\u884c\u7279\u5f81\u63d0\u53d6 \u2014\u2014 \u901a\u8fc7\u5728 2D(spatial) &amp; 1D(temporal) \u8fdb\u884c\u5377\u79ef\uff0c\u5c06\u8f83\u77ed\u7684\u89c6\u9891\u5207\u7247\u8f6c\u6362\u4e3a feature vector</p> </li> <li> <p>\u4e3a\u4e86\u5904\u7406\u66f4\u957f\u7684\u8f93\u5165\u89c6\u9891\uff1a</p> <ul> <li> <p>Parmar \u5c06 3DCNN \u548c LSTM \u7ed3\u5408\uff0c\u63d0\u51fa\u4e86 C3D-LSTM</p> </li> <li> <p>Zheng \u57fa\u4e8e Graph Convolutional Network \u63d0\u51fa\u4e86 Context-aware \u7684\u6a21\u578b</p> <p>\u5bf9 Static Posture &amp; Dynamic Movement \u8fdb\u884c\u5efa\u6a21\u6765\u6355\u6349\u4e0d\u540c\u65f6\u95f4\u8de8\u5ea6\u4e0a\u7684\u8054\u7cfb</p> </li> <li> <p>Nekoui \u63d0\u51fa\u4e86\u4e00\u79cd CNN-based \u7684\u65b9\u6cd5\u540c\u65f6\u5bf9\u7c97\u7ec6\u7c92\u5ea6\u7684 temporal dependencies \u8fdb\u884c\u6355\u6349</p> <p>\u4f7f\u7528 video feature &amp; pose estimation heatmap\uff0c\u5e76\u5806\u53e0\u4e0d\u540c kernel size \u7684 CNN \u6a21\u5757\u6765\u6355\u6349\u4e0d\u540c\u65f6\u95f4\u8de8\u5ea6\u7684 pattern \u8fdb\u884c\u6355\u6349</p> </li> <li> <p>Xu \u9488\u5bf9 figure skating \u573a\u666f\u63d0\u51fa\u4e86 multi-scale and skip-connected CNN-LSTM</p> <p>\u901a\u8fc7\u4e0d\u540c\u5927\u5c0f\u7684 CNN kernel \u5bf9\u77ed\u671f temporal dependencies \u8fdb\u884c\u6355\u6349 &amp; \u901a\u8fc7 LSTM + self-attention \u5bf9\u957f\u671f temporal dependencies \u8fdb\u884c\u6355\u6349</p> </li> </ul> </li> </ul> </li> <li> <p>Explaining Action Quality Assessment</p> <p>\u867d\u7136\u4e0a\u8ff0\u65b9\u6cd5\u5377\u8d62\u4e86\u51c6\u786e\u6027\uff08\u9884\u6d4b\u7ed3\u679c\u548c\u88c1\u5224\u8bc4\u5206\u4e4b\u95f4\u7684 Spearman\u2019s rank correlation\uff09\uff0c\u4f46\u5374\u5ffd\u7565\u4e86\u7528\u6237\u5e94\u8be5\u5982\u4f55\u5e94\u7528\u548c\u89e3\u91ca\u8fd9\u4e9b\u9884\u6d4b\u7ed3\u679c</p> <p>\u8001\u5e08 chua \u7684\u4e00\u4e0b\u7ed9\u4f60\u7684\u5377\u5b50\u6279\u4e86\u4e2a\u603b\u8bc4\u5206\uff0c\u53c8\u4e0d\u544a\u8bc9\u4f60\u6263\u54ea\u4e86</p> <ul> <li> <p>Yu \u901a\u8fc7 Grad-CAM saliency maps \u5bf9 diving \u95ee\u9898\u7684\u8bc4\u5206\u8fc7\u7a0b\u8fdb\u884c\u89e3\u91ca</p> <p>=&gt; \u4f46 saliency maps \u7684\u672c\u610f\u662f\u62ff\u6765 debug \u7684 orz</p> </li> <li> <p>Pirsiavash \u901a\u8fc7\u8ba1\u7b97 \u7531\u59ff\u6001\u4f30\u8ba1\u5f97\u5230\u7684 relative score \u7684\u68af\u5ea6 \u6765\u5bf9 diving \u95ee\u9898\u7684\u8bc4\u5206\u8fc7\u7a0b\u8fdb\u884c\u89e3\u91ca</p> <p>=&gt; \u57fa\u4e8e\u6570\u636e\uff08\u800c\u975e\u57fa\u4e8e\u4eba\u7c7b\u88c1\u5224\u5982\u4f55\u505a\u51fa\u88c1\u51b3\uff09</p> </li> <li> <p>\u4f7f\u7528\u6570\u636e\u4f20\u611f\u5668 + \u6d45\u5c42\u6a21\u578b\u7684\u65b9\u6cd5</p> <ul> <li> <p>Khan \u501f\u9274\u4e86\u7535\u5b50\u677f\u7403\u6e38\u620f\uff0c\u4f7f\u7528\u53ef\u7a7f\u6234\u8bbe\u5907\u6536\u96c6\u6570\u636e\u5e76\u5bf9\u677f\u7403\u51fb\u7403\u6570\u636e\u8fdb\u884c\u5206\u6790\uff0c\u5e76\u4eba\u4e3a\u89c4\u5b9a\u4e86\u82e5\u5e72 low-level sub-actions</p> </li> <li> <p>Thompson \u5728\u91cf\u5316\u8bc4\u5206\u6807\u51c6\u540e\uff0c\u63d0\u51fa\u4e86\u5bf9 \u201c\u76db\u88c5\u821e\u6b65\u201d \u9879\u76ee\u7684\u53ef\u89c6\u5316\u65b9\u6cd5</p> </li> </ul> </li> </ul> </li> <li> <p>Rubrics for Figure Skating</p> <ul> <li> <p>\u201c\u8bc4\u5206\u6807\u51c6\u201d \u7684\u4e24\u8981\u7d20\uff1a</p> <ol> <li> <p>\\(\\geq 1\\) * trait / dimension + \u5bf9\u5e94\u7684\u89e3\u91ca\u6848\u4f8b</p> </li> <li> <p>\u5404 dimension \u7684 \u8bc4\u5206\u8303\u56f4 / \u5206\u6bb5\u6807\u51c6</p> </li> </ol> </li> <li> <p>ISU Judging System: \u5bf9\u6bcf\u4e2a\u7ef4\u5ea6\u7ed9 sub-score\uff0c\u6700\u540e\u6298\u7b97\u603b\u5206</p> <ol> <li> <p>TES\uff08\u6280\u672f\u5f97\u5206\uff09</p> <p>\u8fd0\u52a8\u5458\u7684\u6280\u672f\u52a8\u4f5c\u5e8f\u5217\u5c06\u88ab\u63d0\u524d\u5217\u51fa\uff0c\u6bcf\u4e2a\u52a8\u4f5c\u7684\u5f97\u5206 = Base Value(\u96be\u5ea6\u5206) + GOE(\u504f\u5dee\u503c)</p> <p>\\(GOE \\in [-5,5]\\)</p> <ul> <li> <p>\u52a8\u4f5c\u5e8f\u5217\u5c06\u7531\uff1aJump, Spin, Step Seq \u53ca\u4e4b\u95f4\u7684\u8fc7\u6e21\u52a8\u4f5c\u7ec4\u6210</p> </li> <li> <p>Jump \u53ef\u88ab\u5206\u4e3a\u516d\u79cd\uff1aToe Loop (T), Salchow (S), Loop (Lo), Flip (F), Lutz (Lz), and Axel (A)</p> <p>\u6839\u636e\u8d77\u8df3\u3001\u843d\u51b0\u52a8\u4f5c\u8fdb\u884c\u533a\u5206\uff0c\u4e0d\u540c\u7684\u5708\u6570\u4f1a\u7ed9\u4e0d\u540c\u7684\u5206</p> </li> <li> <p>Rotation \u53ef\u88ab\u5206\u4e3a\u4e09\u79cd\uff1aUpright (USp), Sit (SSp), and Camel (CSp)</p> <p>\u6839\u636e\u96be\u5ea6\u3001\u6d41\u7545\u5ea6\u3001\u7a33\u5b9a\u5ea6\u8fdb\u884c\u8bc4\u5206</p> </li> <li> <p>Step Sequence \u53ef\u88ab\u5206\u4e3a\u4e09\u79cd\uff1aStraight line step sequence (SISt), Circular step sequence (CiSt), and Serpentine step sequence (SeSt)</p> <p>\u6b65\u4f10\u5fc5\u987b\u4e0e\u97f3\u4e50\u76f8\u5339\u914d\uff0c\u5e76\u4e14\u5e72\u51c0\u5229\u843d\u7684\u5b8c\u6210shuo ta</p> </li> </ul> </li> <li> <p>PCS (Program Component Score) \u7531\u4e94\u90e8\u5206\u7ec4\u6210\uff1a</p> <ul> <li> <p>Skating Skills: \u5bf9\u8282\u76ee\u5185\u5bb9\u7684\u4e30\u5bcc\u7a0b\u5ea6\u3001\u6280\u672f\u662f\u5426\u5e72\u51c0\u3001\u6ed1\u901f\u8fdb\u884c\u8bc4\u5206</p> </li> <li> <p>Tran- sition / Linking Footwork: \u5bf9\u6574\u4f53\u6b65\u6cd5\u548c\u59ff\u6001\u8f6c\u53d8\u6d41\u7545\u5ea6\u8fdb\u884c\u8bc4\u5206</p> </li> <li> <p>Performance / Execution: \u662f\u5426\u4ece\u80a2\u4f53\u52a8\u4f5c\u548c\u60c5\u7eea\u4e0a\u4f20\u8fbe\u4e86\u914d\u4e50\u7684\u60c5\u611f</p> </li> <li> <p>Choreography / Composition: \u8282\u76ee\u52a8\u4f5c\u7f16\u6392\u662f\u5426\u4e0e\u97f3\u4e50\u5951\u5408</p> </li> <li> <p>Interpretation: \u7528\u4e8e\u8868\u8fbe\u97f3\u4e50\u7684\u52a8\u4f5c\u662f\u5426\u5177\u6709\u521b\u65b0\u6027</p> </li> </ul> </li> </ol> </li> </ul> </li> </ol>"},{"location":"sum/Segmentation/#3-approach_1","title":"3 Approach","text":"\u4e8b\u524d\u53ef\u89e3\u91ca\u6027\u5efa\u6a21 Ante-hoc <p>IRIS \u5728\u5bf9\u6a21\u578b\u8fdb\u884c\u8bad\u7ec3\u524d\uff0c\u5c06\u53ef\u89e3\u91ca\u6027\u7ed3\u5408\u5230\u6a21\u578b\u7ed3\u6784\u4e2d\uff0c\u4ee5\u5b9e\u73b0\u6a21\u578b\u5185\u7f6e\u7684\u53ef\u89e3\u91ca\u6027</p> <ul> <li> <p>It is informed by a rubric to determine what to consider when calculating its judgement.</p> </li> <li> <p>It performs seg- mentation to specify moments of a performance to judge specific criteria.</p> </li> </ul> <p>IRIS \u4f7f\u7528\u4e86 4 \u4e2a Rubric-Feature \u4ee5\u9884\u6d4b\u548c\u89e3\u91ca\u5176\u4f5c\u51fa\u7684\u5224\u65ad\uff1a</p> <ol> <li> <p>Score: Final Judgement</p> </li> <li> <p>Sequence: \u63cf\u8ff0\u4e86\u7528\u4e8e\u8bc4\u5224\u7684\u52a8\u4f5c (technical elements)</p> </li> <li> <p>Segments: \u6bcf\u4e2a\u52a8\u4f5c\u7531\u54ea\u4e00\u6761\u89c4\u5219\u8bc4\u5224</p> </li> <li> <p>Subscores: \u63cf\u8ff0 TES &amp; PCS \u5404\u9879\u5f97\u5206</p> </li> </ol>"},{"location":"sum/Segmentation/#0-data-preparation","title":"0 Data Preparation","text":"<p>\u4f7f\u7528 MIT-Skate \u6570\u636e\u96c6</p> <ul> <li> <p>\u5c06 3min \u5de6\u53f3\u7684\u89c6\u9891\u8f6c\u5316\u4e3a 4D tensor: <code>[x, y, time, colorChannel]</code></p> </li> <li> <p>\u63d0\u53d6 ISU \u53d1\u5e03\u7684 PDF \u6587\u4ef6\u4e2d\u7684 skater name, TES base, GOE, total subscores, PCS subscore\u3002\u5c06\u5176\u4e0e\u89c6\u9891\u6570\u636e\u76f8\u5bf9\u5e94\u3002</p> </li> <li> <p>\u624b\u52a8 \u6807\u8bb0\u6bcf\u4e2a\u52a8\u4f5c\u7684\u8d77\u6b62\u65f6\u95f4 \uff08train segmentation\uff09</p> </li> <li> <p>\u4e3a\u4e86\u786e\u4fdd\u6bcf\u4e2a\u7c7b\u522b\u4e0b\u7684\u90fd\u6709\u5145\u8db3\u7684\u6570\u636e\uff0c\u8fd9\u91cc\u961f Jump, Spin, Step Sequence, Transition \u7684\u5212\u5206\u8f83\u4e3a\u5bbd\u6cdb</p> <p>\u5efa\u8bae Future Work \u80fd\u591f\u7528\u66f4\u591a\u6570\u636e\u6765\u5212\u5206 more specific labels</p> </li> </ul>"},{"location":"sum/Segmentation/#1-base-embedding","title":"1 Base Embedding","text":"<p>\u751f\u6210 video \u7684 vector presentation</p> <p>\u8bad\u7ec3 3D-CNN\uff08I3D\uff09 \u6a21\u578b \\(M_0\\)\uff1a</p> <ul> <li> <p>\u8f93\u5165: video tensor \\(x\\)</p> </li> <li> <p>\u8f93\u51fa: timeseries embedding \\(\\hat{z}_t\\): 2D tensor</p> <p>\\(M_0\\) \u4f1a\u5bf9\u6bcf 0.534s \u751f\u6210\u4e00\u4e2a vector embedding\uff0c\u6bcf\u4e2a\u89c6\u9891\u5c06\u5bf9\u5e94\u7ea6 356 \u4e2a\uff08zero-padding\uff09</p> </li> </ul>"},{"location":"sum/Segmentation/#2-action-segments","title":"2 Action Segments","text":"<ul> <li> <p>\u8bad\u7ec3 multi-stage temporal convolutional network (MS-TCN) \\(M_t\\)\uff1a</p> <ul> <li> <p>\u8bad\u7ec3\u96c6\uff1a\u4eba\u5de5\u6807\u6ce8\u7684 segments \u4f5c\u4e3a ground truth (\u76d1\u7763\u5b66\u4e60)</p> </li> <li> <p>\u8f93\u5165\uff1a\\(\\hat{z}_t\\)</p> </li> <li> <p>\u8f93\u51fa\uff1aaction sequence embedding \\(\\hat{z}_{\\tau}\\) (Seq-to-Seq)</p> <p>\u6807\u6ce8\u4e86\u6bcf\u4e2a action \u7684\u8d77\u6b62\u65f6\u95f4 (zero-padding)</p> </li> </ul> </li> <li> <p>\u7531\u4e8e TCN \u53ef\u80fd\u9884\u6d4b\u51fa over-segmentation\uff0c\u672c\u6587\u4f7f\u7528\u4e86\u4ee5\u4e0b\u65b9\u6cd5\u8fdb\u884c\u4f18\u5316</p> <ol> <li> <p>\u4f7f\u7528 truncated mean squared err \u5bf9 loss function \u8fdb\u884c\u5e73\u6ed1\u64cd\u4f5c</p> <p>\u5408\u5e76\u8f83\u5c0f\u7684 segmentation</p> \\[ L_{\\mu} = \\frac{1}{T} \\sum_t^T \\max(\\varepsilon_t, \\varepsilon) \\] <ul> <li> <p>\\(\\varepsilon_t = (\\log\\hat{m}(t)- \\log\\hat{m}(t-1))^2\\)</p> </li> <li> <p>\\(\\varepsilon\\): truncation \u8d85\u53c2\u6570</p> </li> </ul> </li> <li> <p>\u4f7f\u7528\u542f\u53d1\u5f0f\u7684\u65b9\u6cd5</p> <ol> <li> <p>\u4ece\u88c1\u5224\u8868\u4e2d\u6570\u5904\u6bcf\u4e2a\u52a8\u4f5c\u7c7b\u522b \\(a\\) \u5305\u542b\u7684\u52a8\u4f5c\u6570\u91cf\uff0c\u603b\u91cf\u4e3a \\(n_a\\) \u4e2a</p> </li> <li> <p>\u4f7f\u7528 \u6700\u957f\u7684 \\(n_a\\) \u4e2a segmentation\uff0c\u5c06\u5176\u5206\u914d\u7ed9\u5404\u7c7b\u522b</p> </li> <li> <p>\u5269\u4e0b\u7684\u8f83\u77ed segmentations \u5168\u90fd\u6253\u4e0a <code>transition</code> \u6807\u7b7e</p> </li> </ol> </li> </ol> </li> </ul>"},{"location":"sum/Segmentation/#3-predict-subscores","title":"3 Predict Subscores","text":"<p>IRIS \u9884\u6d4b\u7684 subscores \u5206\u4e3a\u4ee5\u4e0b\u4e24\u4e2a\u90e8\u5206\uff1a</p> <ol> <li> <p>TES (7): \u5bf9\u6bcf\u4e2a\u6280\u672f\u5f97\u5206\u70b9\u8fdb\u884c\u8bc4\u4f30</p> <ol> <li> <p>\u5bf9 action sequence \u4e2d\u7684\u6bcf\u4e00\u4e2a\u52a8\u4f5c \\(\\tau\\) \u5206\u522b\u9884\u6d4b TES\uff08\u4ec5 GOE \u90e8\u5206\uff09</p> <p>\u4e3a\u6bcf\u4e2a seq step \\(\\tau\\) \u5404\u8bad\u7ec3\u4e00\u4e2a\u4f20\u7edf CNN \u6a21\u578b \\(M_{\\Delta \\tau}\\):</p> <ul> <li> <p>\u8f93\u5165\uff1aseq embedding \\(\\hat{z}_{\\tau}\\) &amp; \u771f\u5b9e\u52a8\u4f5c\u6807\u7b7e \\(a_{\\tau}\\)(\u53ef\u4ee5\u9884\u5148\u5f97\u77e5)</p> </li> <li> <p>\u8f93\u51fa\uff1a\u5355\u4e2a\u52a8\u4f5c\u7684 GOE \u5f97\u5206 \\(\\hat{y}_{\\Delta \\tau}\\)</p> </li> </ul> </li> <li> <p>\u8ba1\u7b97\u5355\u4e2a\u52a8\u4f5c\u7684 TES \u5f97\u5206 \\(\\hat{y}_{\\tau} = \\hat{y}_{\\tau_0} + \\hat{y}_{\\Delta \\tau}\\) (Base + GOE)</p> </li> <li> <p>\u8ba1\u7b97\u6240\u6709\u52a8\u4f5c\u7684\u603b TES \u5f97\u5206 \\(\\hat{y}_{total} = \\sum_r \\hat{y}_r\\)</p> </li> </ol> </li> <li> <p>PCS (5): \u5bf9\u8282\u76ee \u6574\u4f53\u8868\u73b0 \u8fdb\u884c\u8bc4\u4f30</p> <p>\u8bad\u7ec3 multi-task CNN \u6a21\u578b \\(M_{\\pi}\\)\uff1a</p> <p>\u56e0\u4e3a\u5404\u9879\u603b\u8bc4\u4e4b\u95f4\u5b58\u5728 correlation\uff0c\u8bad\u7ec3 multi-task \u6bd4\u8bad\u7ec3 multi-indipendent \u66f4\u51c6\u786e</p> <ul> <li> <p>\u8f93\u5165\uff1a \u6574\u4e2a\u89c6\u9891 \u7684 time series embedding \\(\\hat{z}_t\\)</p> </li> <li> <p>\u8f93\u51fa\uff1a\u9884\u6d4b PCS \u7684\u5404\u5206\u91cf \\(\\hat{y}_p^i\\) (multi-task)\uff0c\u5c06\u5176\u548c \\(\\hat{y}_{\\pi}\\) \u4f5c\u4e3a\u603b PCS</p> </li> </ul> </li> </ol>"},{"location":"sum/Segmentation/#4-predict-final-score","title":"4 Predict Final Score","text":"<p>\u7b80\u5355\u7684\u5c06 TES &amp; PCS \u603b\u5206\u76f8\u52a0\u5373\u53ef\uff1a</p> \\[ \\hat{y} = \\hat{y}_{total} + \\hat{y}_p \\] <p> IRIS \u8bc4\u5206\u8fc7\u7a0b\u53ef\u89c6\u5316</p>"},{"location":"sum/Segmentation/#4-evaluatio","title":"4 Evaluatio","text":"<ol> <li> <p>\u8ba1\u7b97 Final Score \u7684 Spearman Rank Correlation</p> </li> <li> <p>(New) \u5206\u522b\u8ba1\u7b97 TES &amp; PCS \u7684 Spearman Rank Correlation</p> </li> <li> <p>Dice Coefficient: \u8ba1\u7b97 segmentation \u7684\u51c6\u786e\u6027</p> <p>\u5bf9\u9884\u6d4b\u5206\u5272\u5e8f\u5217 \\(\\hat{a}\\) \u548c\u771f\u5b9e\u52a8\u4f5c\u5e8f\u5217 \\(a\\)\uff0c\u8ba1\u7b97\u5176\u4ea4\u53e0\u7a0b\u5ea6 \\(2(a \\cdot \\hat{a})/(|a|^2 + |\\hat{a}|^2)\\)</p> </li> </ol>"},{"location":"sum/Segmentation/#2023-psl","title":"2023 PSL","text":"<p>Label-reconstruction-based Pseudo-subscore Learning</p> <p>\u672c\u6587\u9488\u5bf9 Diving \u95ee\u9898\uff0c\u8ba4\u4e3a\u53ef\u4ee5\u80cc\u5212\u5206\u4e3a\uff1astart - takeoff - drop - entry - end \u4e94\u4e2a substages</p>"},{"location":"sum/Segmentation/#1-introduction","title":"1 Introduction","text":"<ul> <li> <p>\u5927\u90e8\u5206\u73b0\u6709\u5de5\u4f5c</p> <ul> <li> <p>Methods</p> <ul> <li>Provide an Overall Quality Score for input video</li> <li>Lack an evaluation for each substage</li> <li>Cann't provide detailed feedback</li> </ul> </li> <li> <p>Databases: Do not provide labels for substage quality assessment</p> <ul> <li>\u53ea\u6709 <code>UNLV-Diving</code> \u63d0\u4f9b\u4e86\u6bcf\u4e2a substage \u7684 segmentation labels\uff0c\u4f46\u4ecd\u7f3a\u5c11\u5404 substage \u7684 quality labels</li> </ul> </li> </ul> </li> <li> <p>\u672c\u6587\u5de5\u4f5c: PSL</p> <ul> <li> <p>overall score \u4e0d\u4ec5\u88ab\u89c6\u4e3a quality label\uff0c\u540c\u65f6\u4e5f\u88ab\u7528\u4f5c traning set \u7684 feature</p> <p>\u5c06 overall score &amp; \u5404\u9636\u6bb5\u7684 visual feature FUSE \u5728\u4e00\u8d77\uff0c\u4ece\u800c\u751f\u6210 subscores</p> </li> <li> <p>PSL Model \u7528\u4e8e\u751f\u6210 pseudo-subascore label</p> </li> <li>\u63d0\u51fa\u4e86\u7528\u4e8e PSL \u8bad\u7ec3\u7684 <code>label construction loss</code></li> <li> <p>\u57fa\u4e8e <code>pseudo-subascore label</code> &amp; <code>overall score label</code> \u5bf9 PSL \u8fdb\u884c fine-tune\uff0c\u5f97\u5230\u4e00\u4e2a multi-substage Model\uff0c\u7528\u4e8e\u9884\u6d4b\uff1a</p> <ul> <li>quality score label for Each Substage</li> <li>Overall qualirt score label</li> </ul> </li> </ul> </li> </ul>"},{"location":"sum/Segmentation/#2-related-works_1","title":"2 Related Works","text":"<ul> <li> <p>Expanding Scope of Application</p> <ul> <li>Parmar \u8bbe\u8ba1\u4e86\u4e00\u79cd\u7528\u4e8e\u81ea\u52a8\u8bc4\u4f30 piano-playing quality \u7684\u65b9\u6cd5</li> </ul> </li> <li> <p>\u57fa\u4e8e execution score + difficulty-level \u7684\u65b9\u6cd5</p> <ol> <li>\u5c06 DD \u4f5c\u4e3a\u8bad\u7ec3\u7528 feature\uff0c\u8ba4\u4e3a final_score = executionScore * DD</li> <li>Nekoui\uff1a\u7ed3\u5408\u4e86 Pose \u548c Scoring Rule</li> <li>Li\uff1a\u62bd\u53d6 key fragments \u4ee5\u751f\u6210 final feature</li> <li>Xian\uff1a\u5bf9 Diving \u7684 substage \u8fdb\u884c\u4e86\u5212\u5206\uff0c\u5e76\u8ba4\u4e3a\u4e0d\u540c substage \u5bf9\u603b\u5206\u8d21\u732e\u5177\u6709\u4e0d\u540c\u6743\u91cd</li> </ol> </li> <li> <p>\u5176\u4ed6\u7684\u4e00\u4e9b\u65b9\u6cd5</p> <ol> <li>Parmar\uff1a\u8bbe\u8ba1\u4e86 multitask framework\uff0c\u7ed3\u5408 pose\uff0ccategory\uff0cscore \u8fdb\u884c\u8bc4\u4f30</li> <li>Tang\uff1a\u63d0\u51fa\u4e86 distribution learning method \u4ee5\u53bb\u9664\u8bc4\u59d4\u7684\u4e3b\u89c2\u56e0\u7d20\u5e72\u6270</li> </ol> </li> </ul>"},{"location":"sum/Segmentation/#3-approach_2","title":"3 Approach","text":""},{"location":"sum/Segmentation/#1-temporal-semantic-segmentation","title":"1) Temporal Semantic Segmentation","text":"<ol> <li> <p>\u5c06\u8f93\u5165\u89c6\u9891 \\(V\\) \u5212\u5206\u4e3a clips \\(\\{p_i\\}_{i=1}^n \\in \\mathbb{R}^{W\\times H \\times T}\\)</p> <p>\\(T\\) \u4e3a\u6bcf\u4e2a clip \u5305\u542b\u7684\u5e27\u6570</p> </li> <li> <p>\u4f7f\u7528 off-the-shelf Encoder-Decoder Temporal Convolutional Network (ED-TCN) \u5bf9 clips \u8fdb\u884c Temporal Semantic Segmentation</p> <p>\u7ecf\u8fc7 Temporal Segmentation \u540e\uff0c\u6bcf\u4e2a clip \u5c06\u88ab\u5212\u5165\u4e00\u4e2a substage</p> </li> </ol>"},{"location":"sum/Segmentation/#2-psl-module","title":"2) PSL Module","text":"<p> PSL \u7531\u4e09\u90e8\u5206\u6784\u6210\uff0c\u7528\u4e8e\u751f\u6210\u5404 substage \u7684 pseudo-scores </p>"},{"location":"sum/Segmentation/#1-feature-backbone-network-p3d","title":"1 - Feature Backbone Network (P3D)","text":"<ul> <li> <p>P3D \u4f7f\u7528 <code>1*3*3</code> SpatialConv + <code>3*1*1</code> TemporalConv \u66ff\u6362\u5e38\u7528\u7684 <code>3*3*3</code> C3D\uff0c\u4f7f\u5f97\u5728 same depth \u4e0b\uff0cP3D \u53ef\u4ee5\u901a\u8fc7 fewer params \u51c6\u786e\u63d0\u53d6\u7279\u5f81</p> </li> <li> <p>\u5c06 P3D \u63d0\u53d6\u5230\u7684 feature \u8bb0\u4e3a \\(f_i \\in \\mathbb{R}^m\\)\uff08\\(i\\) \u4e3a substage \u7f16\u53f7\uff09</p> <p>\u7ecf\u7279\u5f81\u5904\u7406\u540e\uff0c\u8f93\u5165\u7531 clips \u53d8\u4e3a features \\(V = \\{f_1, f_2, ... , f_n\\}\\)</p> </li> <li> <p>Embed <code>overall-score label</code> in each substage</p> <ul> <li> <p>\\(V' = \\{f_{l1}, f_{l2}, ... , f_{ln}\\} \\in \\mathbb{R}^{m+1}\\)</p> <p>=&gt; \u5176\u5b9e\u5c31\u662f\u7b80\u5355 concat \u64cd\u4f5c\uff0c\u589e\u52a0\u7684\u7eac\u5ea6\u5c31\u662f overall-score\uff0c\u4e5f\u5373\uff1a</p> </li> <li> <p>\\(f_{li} = L \\oplus f_i\\)\uff0c\u5176\u4e2d \\(L\\) \u4e3a overall-score</p> </li> </ul> </li> </ul>"},{"location":"sum/Segmentation/#2-subscore-generator-label-decomposition","title":"2 - Subscore Generator (Label Decomposition)","text":"<p>\u5404 substage \u7684 <code>Subscore Generator</code> \u4eab\u6709 \u72ec\u7acb \u7684\u53c2\u6570</p> <ul> <li> <p>\u901a\u8fc7 Fully-Connected Network \u5b9e\u73b0\uff0c\u5728\u672c\u6587\u4e2d\u5305\u542b 5 \u4e2a FC Layers</p> <ul> <li> <p>\u5404\u5c42\u5305\u542b\u7684\u8282\u70b9\u6570\u4e3a\uff1a<code>[m+1, m/4, m/32, m/128, 1]</code></p> </li> <li> <p>t<sup>th</sup> \u5c42\u8f93\u51fa\u8bb0\u4e3a\uff1a\\(f^t_{li} = \\text{FC}(W^t, f_{lt}^{t-1})\\)</p> </li> </ul> </li> <li> <p>1<sup>st</sup> Layer \u4f7f\u7528 embedded feature \\(\\{f_{li}\\}\\) \u4f5c\u4e3a\u8f93\u5165</p> </li> <li> <p>last Layer \u4f7f\u7528 \\(\\text{Sigmoid}\\) \u4f5c\u4e3a\u6fc0\u6d3b\u51fd\u6570\uff0c\u8f93\u51fa\u9884\u6d4b\u7684 subscores</p> <p>=&gt; \\(\\text{subs}_i = \\text{Sigmoid}(f'_{li})\\) \u4e3a i<sup>th</sup> substage \u7684\u9884\u6d4b\u5f97\u5206</p> </li> </ul>"},{"location":"sum/Segmentation/#3-overall-score-generator-label-construction","title":"3 - Overall Score Generator (Label Construction)","text":"<p>\u4f7f\u7528\u5355\u4e2a\u5168\u8fde\u63a5\u5c42\u5bf9 subscores \u53d6\u52a0\u6743\u5e73\u5747\uff1a</p> \\[ \\hat{S} = \\text{Sigmoid}(\\sum_{i=1}^n (w_i \\times \\text{subs}_i)) \\]"},{"location":"sum/Segmentation/#evaluation-of-psl-module","title":"Evaluation of PSL Module","text":"<ul> <li>\u5047\u8bbe\uff1afinal_score = execution_score * DD = \\(S^e * d\\)</li> <li>\u6bcf\u4e2a\u8f93\u5165\u89c6\u9891\u7684\u6807\u7b7e\u7531\u4e24\u90e8\u5206\u6784\u6210\uff1a(overallScore, executionScore)</li> <li> <p>\\(\\mathcal{L}_{psl}\\)\uff1a\u4f7f\u7528 \u5747\u65b9\u8bef\u5dee(MSE) \u4f5c\u4e3a label-reconstruction loss\uff0c\u6709\uff1a</p> \\[ \\begin{align*}     \\mathcal{L}_{psl} &amp;= \\frac{1}{N} \\sum_{i=1}^N (\\hat{S}_i - S_i)^2 \\\\     &amp;= \\frac{1}{N} \\sum_{i=1}^N ((\\hat{S}_i^e - S_i^e)^2 \\times d_i^2) \\end{align*} \\] </li> </ul>"},{"location":"sum/Segmentation/#3-multi-substage-aqa-module","title":"3) Multi-substage AQA Module","text":"<p>PSL Module \u5b58\u5728\u7684\u95ee\u9898</p> <ul> <li> <p>PSL \u4f7f\u7528\u4e86 overall-score embedded feature\uff0c\u4f46\u5728 AQA \u4efb\u52a1\u4e2d overall-score \u662f\u672a\u77e5\u7684</p> </li> <li> <p>\u56e0\u6b64\uff0c\u6211\u4eec\u9700\u8981\u6784\u5efa\u4e00\u4e2a\u65b0\u7684 SPN\uff08\u5207\u7247\u5206\u7ec4\u7f51\u7edc\uff09\uff0c\u4f7f\u5176\u80fd\u4e0d\u4f9d\u8d56\u4e8e overall-score \u5bf9\u7ed3\u679c\u8fdb\u884c\u9884\u6d4b</p> </li> </ul> <p>Multi-substate AQA Module \u540c\u6837\u7531 5 \u4e2a FC-Layer \u6784\u6210\uff08\u5404\u5c42\u8282\u70b9\u6570\u4e0e PSL \u4e2d\u4e00\u81f4\uff09\uff0c\u5176\u8ba1\u7b97\u8fc7\u7a0b\u5982\u4e0b\uff1a</p> <p>\u4e3b\u8981\u5c31\u662f\u6ca1\u6709\u628a overall-score embed \u8fdb\u53bb</p> \\[     \\begin{align*}         f_i^t &amp;= \\text{FC}(W^t, f_i^{t-1})\\\\         S_i^{sub} &amp;= \\text{Sigmoid}(f_i')\\\\         \\hat{S} &amp;= \\text{Sigmoid}\\left(\\sum_{i=1}^n w_i \\times S_i^{sub}\\right)     \\end{align*} \\]"},{"location":"sum/Segmentation/#evaluation","title":"Evaluation","text":"<p>\u6211\u4eec\u6709\u5982\u4e0b\u6807\u7b7e\uff1a</p> <ul> <li>ground-truth overall label \\(L\\)</li> <li>PSL \u9884\u6d4b\u51fa\u7684 subscores\uff1a \\(\\{\\text{subs}_i\\}\\)</li> <li>Multi-substage AQA \u9884\u6d4b\u7684 subscores: \\(\\{s_i^{sub}\\}\\)</li> </ul> <ul> <li> <p>\u6211\u4eec\u53ef\u4ee5\u8861\u91cf subscores \u4e4b\u95f4\u7684 MSE \u8bef\u5dee\uff1a</p> \\[ \\mathcal{L}_i^{sub} = (\\text{subs}_i - s_i^{sub})^2 \\] </li> <li> <p>\u53e6\u5916\u6709 Overall-score \u7684 MSE \u8bef\u5dee\uff1a</p> \\[ \\mathcal{L}_{o} = (\\hat{S} - L)^2 \\] </li> </ul> <p>\u6700\u540e\u7684 Loss Func \u5373\u4e3a\u4e8c\u8005\u4e4b\u548c \\(\\mathcal{L} = \\mathcal{L}_o + \\sum \\mathcal{L}_i^{sub}\\)</p>"},{"location":"sum/Selekton-Based/","title":"\u57fa\u4e8e\u9aa8\u67b6\u7684\u65b9\u6cd5","text":""},{"location":"sum/Selekton-Based/#2018-st-gcn","title":"2018: ST-GCN","text":"<p>Spatial Temporal Graph Convolutional Networks</p>"},{"location":"sum/Selekton-Based/#1-abstract","title":"1 Abstract","text":"<ul> <li> <p>\u4f20\u7edf \u9aa8\u67b6\u5efa\u6a21\u65b9\u6cd5</p> <ul> <li> <p>\u4f9d\u8d56 hand-crafted parts / traversal rules</p> </li> <li> <p>\u8868\u8fbe\u80fd\u529b\u6709\u9650 &amp; \u6cdb\u5316\u8f83\u4e3a\u56f0\u96be</p> </li> </ul> </li> <li> <p>\u521b\u65b0\u70b9\uff1a\u63d0\u51fa \u201c\u7a7a\u95f4-\u65f6\u95f4 \u56fe\u5377\u79ef\u7f51\u7edc (ST-GCN)\u201d</p> <ul> <li> <p>\u57fa\u4e8e\u52a8\u6001\u9aa8\u67b6\uff0c\u5305\u542b\uff1a</p> <ul> <li> <p>\u7a7a\u95f4\u8fb9\uff1a\u4e0e\u5173\u8282\u4e4b\u95f4\u7684\u7269\u7406\u8fde\u63a5\u5173\u7cfb\u4e00\u81f4</p> </li> <li> <p>\u65f6\u95f4\u8fb9\uff1a\u8fde\u63a5\u5404\u4e2a\u65f6\u95f4\u70b9\u4e2d\u7684\u540c\u4e00\u5173\u8282</p> </li> </ul> </li> <li> <p>\u81ea\u52a8\u5b66\u4e60\u65f6\u7a7a pattern\uff08\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u7684\u4f18\u52bf\uff09\uff0c\u589e\u5f3a\u8868\u8fbe\u80fd\u529b\u4e0e\u6cdb\u5316\u80fd\u529b</p> </li> </ul> </li> </ul>"},{"location":"sum/Selekton-Based/#2-relatvie-works","title":"2 Relatvie Works","text":"<ul> <li> <p>\u52a8\u4f5c\u8bc6\u522b Human Action Recognition</p> <ul> <li> <p>\u53ef\u4ee5\u4ece\u591a\u6a21\u6001\u5165\u624b\uff1a\u5916\u89c2\u3001\u6df1\u5ea6\u3001\u5149\u6d41 ...</p> </li> <li> <p>\u57fa\u4e8e \u201c\u52a8\u6001\u9aa8\u67b6\u5efa\u6a21\u201d \u7684\u65b9\u6cd5\uff08Skeleton Based Action Recognition\uff09\u76f8\u5bf9\u8f83\u5c11</p> <p>\u201c\u52a8\u6001\u9aa8\u67b6\u201c \u53ef\u4ee5\u81ea\u7136\u5730\u8868\u793a\u4e3a\u4eba\u4f53\u5173\u8282\u4f4d\u7f6e\u7684\u65f6\u95f4\u5e8f\u5217\uff0c\u4ee5\u4e8c\u7ef4\u6216\u4e09\u7ef4\u5750\u6807\u7684\u5f62\u5f0f\u5448\u73b0\u3002\u7136\u540e\u53ef\u4ee5\u901a\u8fc7\u5206\u6790\u5176\u8fd0\u52a8\u6a21\u5f0f\u6765\u8bc6\u522b\u4eba\u4f53\u52a8\u4f5c\u3002</p> <ul> <li> <p>\u65e9\u671f\u65b9\u6cd5\u7b80\u5355\u5c06\u5404\u65f6\u95f4\u70b9\u7684\u5173\u8282\u5750\u6807\u4f5c\u4e3a feature\uff0c\u5e76\u5bf9\u5176\u8fdb\u884c\u65f6\u5e8f\u5206\u6790</p> <p>=&gt; \u6ca1\u6709\u5229\u7528\u5173\u8282\u4e4b\u95f4\u7684 \u7a7a\u95f4\u5173\u7cfb</p> </li> <li> <p>\u5927\u591a\u6570\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u4e8e \u624b\u5de5\u8bbe\u8ba1 \u90e8\u4ef6\u548c\u89c4\u5219\uff0c\u4ee5\u5b9e\u73b0\u5bf9 \u7a7a\u95f4\u5173\u7cfb \u7684\u5206\u6790</p> <p>\u624b\u5de5\u8bbe\u8ba1\u7684\u7279\u5f81\u5305\u62ec\uff1a\u5173\u8282\u8f68\u8ff9\u534f\u65b9\u5dee\u77e9\u9635\u3001\u5173\u8282\u76f8\u5bf9\u4f4d\u7f6e\u3001\u8eab\u4f53\u90e8\u4f4d\u95f4\u7684\u65cb\u8f6c\u8bc4\u5e73\u79fb</p> <p>=&gt; \u96be\u4ee5\u6cdb\u5316</p> </li> <li> <p>\u8fd1\u5e74\u6765\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u65b9\u6cd5\u4f7f\u7528 RNN/T-CNN \u5b9e\u73b0\u7aef\u5230\u7aef\u7684\u52a8\u4f5c\u8bc6\u522b</p> </li> </ul> </li> </ul> </li> <li> <p>NN on Graph</p> <p>\u4e00\u822c\u91c7\u7528 CNN\u3001RNN \u548c GCN (\u56fe\u5377\u79ef\u7f51\u7edc)\uff0c\u5176\u4e2d GCN \u7684\u4e3b\u6d41\u89e3\u51b3\u65b9\u6848\u5982\u4e0b\uff1a</p> <ol> <li> <p>Spectral Perspective: \u5bf9\u4e8e GCN \u4e2d\u5b58\u5728\u7684 locality \u91c7\u7528 \u201c\u8c31\u5206\u6790\u201d \u7684\u5f62\u5f0f</p> </li> <li> <p>Spatial Perspective: \u76f4\u63a5\u5bf9\u56fe\u4e0a\u67d0\u4e00\u7279\u5b9a\u8282\u70b9 &amp; \u5176 neighbors \u5e94\u7528\u5377\u79ef\u6838\uff08\u672c\u6587\u8def\u5f84\uff09</p> </li> </ol> </li> </ul>"},{"location":"sum/Selekton-Based/#3-approach","title":"3 Approach","text":"<pre><code>graph LR\n    A[Input Video] --\"Pose Estimation\"--&gt; B[\"ST Graph(joints)\"] --\"ST-GCN\"--&gt; C[feature map] --\"Classification\"--&gt; D[\"Class Scores\"] --SoftMax--&gt; Category\n</code></pre>"},{"location":"sum/Selekton-Based/#1-construct-skeleton-graph","title":"1 Construct Skeleton Graph","text":"<p>\u5bf9\u4e8e\u7ed9\u5b9a\u7684 \\(N\\) \u4e2a\u5173\u8282 + \\(T\\) \u5e27\uff0c\u6784\u5efa undirected spatial temporal graph \\(G = (V,E)\\)</p> <ul> <li> <p>\u8282\u70b9\u96c6 \\(V = \\{v_{ti}| t \\in [1,T], i \\in [1,N]\\}\\) \u5305\u542b\u4e86\u6240\u6709\u5e27\u4e0a\u7684\u6240\u6709\u5173\u8282\u70b9</p> <p>\u8282\u70b9 \\(v_{ti}\\) \u7684 Feature Vector \\(F(v_{ti})\\)\u7531 \u5750\u6807\u5411\u91cf + t \u5e27 i-th \u8282\u70b9\u7684\u7f6e\u4fe1\u5ea6\u6784\u6210</p> <p>\u8282\u70b9\u5750\u6807\u53ef\u4ee5\u662f 2D \u6216 3D \u7684</p> </li> <li> <p>\u8fb9\u96c6 \\(E\\) \u7531\u4e24\u90e8\u5206\u6784\u6210</p> <ol> <li> <p>\u540c\u4e00\u5e27\u7684\u8eab\u4f53\u5185\u90e8\u8fde\u63a5\uff08Spatial\uff09\\(E_S = \\{\\overline{v_{ti}v_{tj}}|(i,j) \\in H\\}\\)</p> </li> <li> <p>\u5e27\u95f4\u8fde\u63a5\uff08Temporal\uff09\\(E_F = \\{ \\overline{v_{ti}v_{(t+1)i}} \\}\\)</p> </li> </ol> </li> <li> <p>\u4f5c\u8005\u901a\u8fc7\u4ee5\u4e0b\u4e24\u4e2a\u6b65\u9aa4\u57fa\u4e8e \u9aa8\u67b6\u5e8f\u5217 \u6784\u5efa \u7a7a\u95f4-\u65f6\u95f4\u56fe \\(G\\)</p> <ol> <li> <p>\u6839\u636e\u4eba\u4f53\u7ed3\u6784\uff0c\u8fde\u63a5\u540c\u4e00\u5e27\u5185\u7684\u5404\u5173\u8282\u70b9\uff08\u4e0d\u662f\u5168\u8fde\u63a5\uff0c\u662f\u706b\u67f4\u4eba\uff09</p> </li> <li> <p>\u8fde\u63a5\u6bcf\u4e00\u5e27\u4e2d\u7684\u7edf\u4e00\u5173\u8282\u70b9</p> </li> </ol> </li> </ul>"},{"location":"sum/Selekton-Based/#2-spatial-graph-convolutional-neural-network","title":"2 Spatial Graph Convolutional Neural Network","text":""},{"location":"sum/Selekton-Based/#spatial-graph-convolutional","title":"Spatial Graph Convolutional","text":"<ul> <li> <p>\u5bf9\u4e8e\u7f16\u53f7\u4e3a \\(\\tau\\) \u7684\u5e27\uff0c\u6211\u4eec\u62e5\u6709\uff1a</p> <ul> <li>\u5305\u542b N \u4e2a\u8282\u70b9\u7684\u8282\u70b9\u96c6\u5408 \\(V_{\\tau}\\)</li> <li>\u5e27\u5185\u9aa8\u67b6\u8fb9\u96c6\u5408 \\(E_{S(\\tau)} = \\{\\overline{v_{\\tau i} v_{\\tau j}}| (i,j) \\in H\\}\\)</li> </ul> </li> <li> <p>\u8003\u8651\u5bf9 Image / Feature Map \uff082D\u6570\u636e\uff09\u8fdb\u884c\u5377\u79ef\uff1a</p> <p>\u5728 stride=1 + \u9002\u5f53 padding \u65f6\uff0c\u53ef\u4ee5\u5b9e\u73b0 \u8f93\u5165\u8f93\u51fa shape \u4e00\u81f4</p> <ul> <li> <p>\u5bf9\u4e8e kernel size = \\(K \\times K\\)\u3001\u8f93\u5165 \\(F_{in}\\) \u5177\u5907 c \u4e2a\u901a\u9053\u7684\u5377\u79ef\u64cd\u4f5c\uff0c\u7a7a\u95f4\u4f4d\u7f6e\uff08\u8282\u70b9\uff09 \\(x\\) \u5904\u7684\u5355\u901a\u9053\u8f93\u51fa\u53ef\u4ee5\u5199\u4e3a\uff1a</p> \\[     f_{out}(x) = \\sum_{h=1}^K \\sum_{w=1}^K f_{in(p(x,h,w)) \\cdot w(h,w)} \\] <ul> <li> <p>\u5176\u4e2d \\(p(\u00b7)= Z^2 \\times Z^2 \\rightarrow Z^2\\) \u4e3a \u91c7\u6837\u51fd\u6570\uff0c\u7528\u4e8e\u679a\u4e3e\u8282\u70b9 \\(x\\) \u7684 neighbors</p> </li> <li> <p>\u6743\u91cd\u51fd\u6570 \\(w(\u00b7) = Z^2 \\rightarrow \\mathbb{R}^c\\) \u63d0\u4f9b c-dimension \u4e0b\u7684\u6743\u91cd\u5411\u91cf</p> <p>=&gt; the filter weights are shared everywhere on the input image (\u4e0e\u4f4d\u7f6e\u65e0\u5173)</p> </li> </ul> </li> </ul> </li> <li> <p>\u6211\u4eec\u53ef\u4ee5\u901a\u8fc7\u4ee5\u4e0b\u65b9\u5f0f\u5c06\u5377\u79ef\u64cd\u4f5c\u6269\u5145\u5230\u7a7a\u95f4\u56fe \\(V_t\\) \u4e0a\uff1a</p> <ul> <li>\u5bf9\u4e8e\u56fe\u4e0a\u7684\u6bcf\u4e00\u4e2a\u8282\u70b9\uff0cfeature map \\(f_{in}^t: V_t \\rightarrow \\mathbb{R}^c\\) \u5747\u6709\u4e00\u4e2a\u5bf9\u5e94\u7684\u7279\u5f81\u5411\u91cf</li> </ul> <p>\u6b64\u5916\uff0c\u6211\u4eec\u8fd8\u9700\u8981\u91cd\u65b0\u5b9a\u4e49\u91c7\u6837\u51fd\u6570 \\(p(\u00b7)\\) \u548c\u6743\u91cd\u51fd\u6570 \\(w(\u00b7)\\)</p> <ul> <li> <p>Sampling function</p> <p>\u5bf9\u4e8e Graph\uff0c\u6211\u4eec\u5c06\u5377\u79ef\u64cd\u4f5c\u5b9a\u4e49\u5728\u8282\u70b9 \\(v_{ti}\\) \u53ca\u5176 neighbors \u96c6\u5408 \\(B(v_{ti}) = \\{v_{tj}|d(v_{ti},v_{tj}) \\leq D\\}\\) \u4e0a</p> <p>\u5176\u4e2d \\(d(v_{ti},v_{tj})\\), \\(D = 1\\) =&gt; \u4ec5\u9009\u62e9\u76f4\u63a5\u76f8\u90bb\u7684\u7684\u8282\u70b9</p> <p>\u6211\u4eec\u53ef\u4ee5\u5c06\u91c7\u6837\u51fd\u6570 \\(p(\u00b7): B(v_{ti}) \\rightarrow V\\) \u6539\u5199\u4e3a:</p> \\[ p(v_{ti}, v_{tj}) = v_{tj} \\] </li> <li> <p>Weight function</p> <p>\u7531\u4e8e Graph \u4e2d\u7684\u5404\u4e2a\u8282\u70b9\u6ca1\u6709\u660e\u786e\u7684\u76f8\u5bf9\u4f4d\u7f6e\uff08\u7f51\u683c\uff09\uff0c\u672c\u6587\u6839\u636e\u4e00\u5b9a\u7b56\u7565\u5c06\u8282\u70b9 \\(v_{ti}\\) \u7684\u6240\u6709 neighbors \\(B(v_{ti})\\) \u5212\u5206\u4e3a \\(K\\) \u4e2a\u5b50\u96c6\uff0c\u7f16\u53f7\u4e3a \\([0, K-1]\\)</p> <p>=&gt; \u7ecf\u8fc7\u5212\u5206\uff0c\\(v_{ti}\\) \u7684\u6bcf\u4e00\u4e2a neighbot \u90fd\u4f1a\u6709\u4e00\u4e2a\u6570\u5b57\u6807\u7b7e\uff08\u5b50\u96c6\u7f16\u53f7\uff09</p> <p>\u6b64\u65f6\u7684\u6743\u91cd\u51fd\u6570\u53ef\u4ee5\u88ab\u5b9e\u73b0\u4e3a shape = \\(c \\times K\\) \u7684\u77e9\u9635\uff0c\u6709\uff1a</p> \\[ w(v_{ti}, v_{tj}) = w'(l_{ti}(v_{tj})) \\] </li> </ul> </li> <li> <p>Spatial Graph Convolution</p> <p>\u5728\u91cd\u65b0\u5b9a\u4e49\u91c7\u6837\u51fd\u6570\u548c\u6743\u91cd\u51fd\u6570\u540e\uff0c\u6211\u4eec\u53ef\u4ee5\u5c06\u7279\u5b9a\u5173\u8282\u70b9 \\(v_{ti}\\) \u4e0a\u7684\u5377\u79ef\u64cd\u4f5c\u8bb0\u4e3a\uff1a</p> \\[ f_{out}(v_{ti}) = \\sum_{v_{tj} \\in B(v_{ti})} \\frac{1}{Z_{ti}(v_{tj})} f_{in}(v_{tj}) \\cdot w(l_{ti}(v_{tj})) \\] <p>\u5176\u4e2d\u6b63\u5219\u5316\u9879 \\(Z_{ti}(v_{tj})\\) \u4e3a\u90bb\u63a5\u70b9 \\(v_{tj}\\) \u6240\u5728\u5b50\u96c6\u7684 cardinality\uff0c\u7528\u4e8e\u5e73\u8861\u5404\u5b50\u96c6\u5bf9 output \u4ea7\u751f\u7684\u5f71\u54cd</p> </li> </ul>"},{"location":"sum/Selekton-Based/#spatial-temporal-modeling","title":"Spatial Temporal Modeling","text":"<ul> <li> <p>\u6211\u4eec\u53ef\u4ee5\u901a\u8fc7\u6269\u5c55 \u201c\u90bb\u57df\u201d \u7684\u6982\u5ff5\uff0c\u4ece\u800c\u5c06 Spatial Conv \u6269\u5c55\u5230 Spatial-Temporal Conv</p> <p>\u5c06\u76f8\u90bb\u5e27\u4e0a\u7684\u540c\u4e00 joint \u8282\u70b9\u4e5f\u7eb3\u5165 neighbor \u7684\u8003\u91cf\u8303\u56f4\uff0c\u7ed9\u5b9a\u5173\u8282\u70b9\u7684\u90bb\u57df\u96c6\u5408 \\(B\\) \u53ef\u8bb0\u4e3a\uff1a</p> \\[ B(v_{ti}) = v_{qj}, \\text{ where } \\left\\{     \\begin{align*}         &amp; d(v_{tj}, v_{ti}) \\leq K \\\\         &amp; \\|q-t\\| \\leq \\lfloor \\Gamma /2 \\rfloor     \\end{align*} \\right. \\] <ul> <li>\\(\\Gamma\\) \u7528\u4e8e\u63a7\u5236\u65f6\u95f4\u4e0a\u8de8\u8d8a\u7684 n_frames\uff0c\u53ef\u4ee5\u88ab\u89c6\u4e3a \"temporal kernel size\"</li> </ul> </li> <li> <p>\u57fa\u4e8e single frame \u4e0b\u7684 labeling func \\(l_{ti}(v_{tj})\\)\uff0c\u6211\u4eec\u4e5f\u4ee5\u6269\u5145\u5230 S-T \u8303\u56f4\uff1a</p> \\[     l_{ST}(v_{qj}) = l_{ti}(v_{tj}) + (q-t + \\lfloor \\Gamma / 2 \\rfloor) \\times K \\] </li> </ul>"},{"location":"sum/Selekton-Based/#3-partition-strategies","title":"3 Partition Strategies","text":"<p>\u4ee5 single frame \u60c5\u51b5\u4e3a\u4f8b\uff0c\u8ba8\u8bba 3 \u79cd\u9886\u57df\u5212\u5206\u65b9\u5f0f</p> <ol> <li> <p>Uni-labeling: \u53ea\u6709 1 \u4e2a == whole neighbor \u7684\u5b50\u96c6</p> <ul> <li> <p>\\(K = 1, l_{ti}(v_{tj}) = 0\\)</p> </li> <li> <p>\u6bcf\u4e2a\u76f8\u90bb\u8282\u70b9\u4e0a\u7684\u7279\u5f81\u5411\u91cf\u5c06\u4e0e\u76f8\u540c\u7684\u6743\u91cd\u5411\u91cf\u8fdb\u884c\u5185\u79ef</p> </li> <li> <p>\u5728\u5355\u5e27\u60c5\u51b5\u4e0b\uff0c\u4f7f\u7528\u8fd9\u79cd\u7b56\u7565\u76f8\u5f53\u4e8e\u8ba1\u7b97\u6743\u91cd\u5411\u91cf\u4e0e\u6240\u6709\u76f8\u90bb\u8282\u70b9\u7684\u5e73\u5747\u7279\u5f81\u5411\u91cf\u4e4b\u95f4\u7684\u5185\u79ef</p> <p>\u53ef\u80fd\u5bfc\u81f4\u4e22\u5931\u5c40\u90e8\u5dee\u5206\u7279\u6027</p> </li> </ul> </li> <li> <p>Distance partitioning: \u6839\u636e\u4e24\u70b9\u95f4\u7684\u8def\u5f84\u8ddd\u79bb \\(d(v_{ti}, v_{tj})\\) \u5212\u5206     &gt; \u672c\u6587\u8003\u8651 \\(D==1\\), \u6545 \\(d \\in \\{0,1\\}\\)</p> <ul> <li> <p>\\(K = 2, l_{ti}(v_{tj}) = d(v_{ti}, v_{tj})\\)</p> </li> <li> <p>\\(d=0\\) \u4e3a\u4e2d\u5fc3\u4f4d\u7f6e \\(v_{ti}\\) \u672c\u8eab</p> </li> <li> <p>\\(d=1\\) \u4e3a\u5176\u4ed6\u4e0e \\(v_{ti}\\) \u76f4\u63a5\u76f8\u90bb \u7684\u8282\u70b9</p> </li> </ul> </li> <li> <p>Spatial configuration partitioning: \u6839\u636e\u4ee5\u4e0b\u7b56\u7565\u5c06 neighbors \u5212\u5206\u4e3a\u4e09\u4e2a\u5b50\u96c6</p> <ol> <li> <p>\\(v_{ti}\\) \u672c\u8eab</p> </li> <li> <p>centripetal group\uff08\u5411\u5fc3\uff09\uff1a \u6bd4 \\(v_{ti}\\) \u66f4\u9760\u8fd1\u9aa8\u67b6 \u91cd\u5fc3 \u7684\u70b9\u96c6</p> </li> <li> <p>centrifugal group\uff08\u79bb\u5fc3\uff09\uff1a\u5269\u4e0b\u7684\u70b9</p> </li> </ol> <p>\u5c06 all frames \u4e2d\u6307\u5b9a\u5173\u8282 i \u5230\u91cd\u5fc3\u7684\u5e73\u5747\u8ddd\u79bb\u8bb0\u4e3a \\(r_i\\)\uff0c\u6709\uff1a</p> \\[ l_{ti}(v_{tj}) =      \\left\\{         \\begin{align*}             0 &amp;\\text{  if } r_j = r_i \\\\             1 &amp;\\text{  if } r_j \\lt r_i \\\\             2 &amp;\\text{  if } r_j \\gt r_i          \\end{align*}     \\right. \\] </li> </ol>"},{"location":"sum/Selekton-Based/#4-learnable-edge-importance-weighting","title":"4 Learnable edge importance weighting","text":"<ul> <li> <p>\u5728\u4eba\u4f53\u8fd0\u52a8\u65f6\uff0c\u9aa8\u67b6\u5173\u8282\u5c06\u4ee5 Group \u7684\u5f62\u5f0f\u79fb\u52a8\uff1b\u4e14\u540c\u4e00\u4e2a\u5173\u8282\u53ef\u80fd\u53c2\u4e0e\u4e86\u591a\u4e2a Group \u7684\u534f\u4f5c\u3002</p> <p>\u4e3a\u6b64\uff0c\u5728\u6a21\u62df\u4e0d\u540c\u7ec4\u522b\u7684\u8fc7\u7a0b\u4e2d\uff0c\u540c\u4e00\u5173\u8282\u53ef\u80fd\u5177\u6709\u4e0d\u540c\u7684 \u201c\u91cd\u8981\u6027\u201d</p> </li> <li> <p>\u4f5c\u8005\u56e0\u6b64\u5728 ST-GCN \u7684\u6bcf\u4e00\u5c42\u6dfb\u52a0\u4e86\u4e00\u4e2a learnable mask \\(M\\)\uff0c\u5c06\u6839\u636e\u6bcf\u4e2a Spatial Graph \u4e2d\u7684\u8fb9\u96c6 \\(E_S\\) \u5b66\u4e60\u6743\u91cd</p> </li> </ul>"},{"location":"sum/Selekton-Based/#4-implementation","title":"4 Implementation","text":"<p>\u7531\u4e8e\u5bf9\u4e8e Graph \u7684\u5377\u79ef\u4e0e 2D / 3D \u5377\u79ef\u5b58\u5728\u4e00\u4e9b\u4e0d\u540c\uff0c\u6b64\u5904\u4ecb\u7ecd\u4e00\u4e9b\u5b9e\u73b0\u7ec6\u8282</p> <ul> <li> <p>single frame </p> <ul> <li> <p>\u5404\u5173\u8282\u8282\u70b9\u7684\u8fde\u63a5\u901a\u8fc7 \u90bb\u63a5\u77e9\u9635 \\(A\\) \u8868\u793a</p> </li> <li> <p>\u6b64\u5916\u6709\u4e00\u4e2a identity matrix \\(I\\) \u7528\u4e8e\u8868\u793a self-connections</p> </li> <li> <p>\u4f7f\u7528 uni-label partition strategy\uff0c\u5219 ST-GCN \u64cd\u4f5c\u53ef\u8bb0\u4e3a\uff1a</p> \\[ f_{out} = \\Lambda^{-\\frac{1}{2}}(A+I) \\Lambda^{-\\frac{1}{2}} f_{in}W \\] <ul> <li> <p>\\(\\Lambda^{ii} \\sum_j (A^{ij} + I^{ij})\\)</p> </li> <li> <p>\u6240\u6709\u901a\u9053\u7684 weight vectors \u76f8\u4f1a\u5806\u53e0\u5f62\u6210\u6743\u91cd\u77e9\u9635 \\(W\\)</p> </li> <li> <p>\u5c06 \\(A+I\\) \u66ff\u6362\u4e3a \\((A+I) \\otimes M\\) \u5373\u53ef\u5b9e\u73b0 \u201c\u53ef\u5b66\u4e60\u7684\u91cd\u8981\u6027\u63a9\u7801\u201d\uff0cM \u88ab\u521d\u59cb\u5316\u5168 1 \u77e9\u9635</p> </li> </ul> </li> </ul> </li> <li> <p>\u795e\u7ecf\u7f51\u7edc\u67b6\u6784</p> <ul> <li> <p>\u7531\u4e8e ST-GCN \u7684\u6240\u6709 nodes \u5171\u4eab\u6743\u91cd\u77e9\u9635\uff0c\u6211\u4eec\u5fc5\u987b\u4fdd\u8bc1\u4e0d\u540c\u5173\u8282\u7684 input \u8303\u56f4\u76f8\u540c</p> <p>=&gt; \u5c06\u539f\u59cb\u7684\u9aa8\u67b6\u6570\u636e\u653e\u5165 Batch Normalization Layer</p> </li> <li> <p>ST-GCN \u6a21\u578b\u5171\u5305\u542b\u4e86 9 \u4e2a units\uff0c\u6709 9 \u79cd temporal kernel size</p> <ul> <li> <p>\u524d\u4e09\u4e2a units \u8f93\u51fa 64 channel\uff0c\u4e2d\u95f4\u4e09\u4e2a units \u8f93\u51fa 128 channels\uff0c\u6700\u540e\u4e09\u4e2a units \u8f93\u51fa 256 channels</p> </li> <li> <p>\u6bcf\u4e00\u4e2a unit \u90fd\u5e94\u7528\u4e86 ResNet \u673a\u5236\uff0c\u5e76\u4e14\u4ee5 \\(p=0.5\\) \u968f\u673a drop feature \u907f\u514d\u8fc7\u62df\u5408</p> </li> <li> <p>4th &amp; 7th \u65f6\u95f4\u5377\u79ef\u5c42\u7684 <code>strides=2</code>\uff0c\u7528\u4f5c\u6c60\u5316\u5c42</p> </li> <li> <p>\u6700\u7ec8\u8f93\u51fa\u5c06\u901a\u8fc7 Global Pooling \u5f97\u5230\u4e00\u4e2a 256D \u7684 feature vector</p> </li> </ul> </li> <li> <p>feature vector \u5c06\u901a\u8fc7 SoftMax Classifier \u8bc6\u522b\u52a8\u4f5c\u7c7b\u578b</p> </li> </ul> </li> </ul>"},{"location":"sum/Selekton-Based/#2019-action-assessment-by-joint-relation-graphs","title":"2019: Action Assessment by Joint Relation Graphs","text":""},{"location":"sum/Selekton-Based/#1-introduction","title":"1 Introduction","text":"<ul> <li> <p>\u5148\u524d\u7684\u5de5\u4f5c</p> <ul> <li> <p>\u5927\u90e8\u5206\u805a\u7126\u4e8e whole secene\uff08\u5305\u62ec performer's body &amp; background\uff09\uff0c\u4f46\u5ffd\u7565\u4e86 detailed joint interactions</p> </li> <li> <p>\u7531\u4e8e\u5404\u5173\u8282\u7684\u52a8\u4f5c\u8d28\u91cf\u4e00\u5b9a\u7a0b\u5ea6\u4f9d\u8d56\u4e8e\u5176\u90bb\u57df\uff0c\u8fd9\u4f7f\u5f97\u7ec6\u7c92\u5ea6\u7684\u52a8\u4f5c\u8d28\u91cf\u8bc4\u4f30\u65e0\u6cd5\u6b63\u786e\u8fdb\u884c</p> </li> <li> <p>\u867d\u7136\u90e8\u5206\u5de5\u4f5c\u5c1d\u8bd5\u901a\u8fc7\u5206\u6790\u5404 joint \u7684\u8fd0\u52a8\uff0c\u4f46\u90fd\u662f individually \u5730\u8fdb\u884c\u5206\u6790</p> </li> </ul> </li> <li> <p>\u672c\u6587\u5de5\u4f5c</p> <ul> <li>\u8003\u8651\u4e86 locally connected joints \u95f4\u7684 interactive motion\uff08\u800c\u975e individually \u7684\u5bf9\u6bcf\u4e2a\u5173\u8282\u70b9\u8fdb\u884c\u5355\u72ec\u5206\u6790\uff09</li> <li>\u6784\u5efa\u4e86 trainable Joint Relation Graphs \u5e76\u4ee5\u6b64\u4e3a\u57fa\u7840\u5bf9 joint motion \u8fdb\u884c\u5206\u6790</li> <li> <p>\u63d0\u51fa\u4e86\u4e24\u4e2a\u65b0\u7684 Modules:</p> <p>Good Performance = Excellent Movement for each body part + Good Coordination among joints</p> <ul> <li> <p>Joint Commonality Module\uff1a\u5bf9 certain body part \u7684 general motion \u8fdb\u884c\u5efa\u6a21</p> <p>locally connected joints \u4e4b\u95f4\u7684\u8fd0\u52a8\u5171\u6027\u53cd\u6620\u4e86\u8eab\u4f53\u90e8\u5206\u7684 general motion</p> <p>\u901a\u8fc7\u5728 spatial graph \u4e2d\u6c47\u603b\u5173\u8282\u8fd0\u52a8\u6765\u63d0\u53d6\u7279\u5b9a time step \u7684\u8eab\u4f53\u90e8\u4f4d\u52a8\u529b\u5b66\u4fe1\u606f</p> </li> <li> <p>Joint Difference Module\uff1a\u5bf9\u4e8e body part \u5185\u7684 motion difference \u8fdb\u884c\u5efa\u6a21</p> <p>locally connected joints \u4e4b\u95f4\u7684\u5dee\u5f02\u6027\u53cd\u6620\u4e86\u8fd0\u52a8\u7684\u534f\u8c03\u6027</p> <p>\u901a\u8fc7\u5c06\u6bcf\u4e2a\u5173\u8282\u4e0e\u5176\u5728\u7a7a\u95f4\u56fe\u548c\u65f6\u95f4\u56fe\u4e2d\u7684\u5c40\u90e8\u8fde\u63a5\u90bb\u5c45\u8fdb\u884c\u6bd4\u8f83\uff0c\u6765\u63d0\u53d6\u534f\u8c03\u4fe1\u606f</p> </li> </ul> </li> <li> <p>\u5206\u6790\u4e86\u672c\u65b9\u6cd5\u5bf9\u4e8e\u63cf\u8ff0\u52a8\u4f5c\u8d28\u91cf\u8bc4\u4f30\u8fc7\u7a0b\u7684\u53ef\u89e3\u91ca\u6027</p> </li> </ul> </li> </ul>"},{"location":"sum/Selekton-Based/#2-related-works","title":"2 Related Works","text":""},{"location":"sum/Selekton-Based/#action-performance-assessment","title":"Action Performance Assessment","text":"<ul> <li> <p>Gordon \u9996\u5148\u63a2\u7d22\u4e86\u5bf9\u89c6\u9891\u8fdb\u884c\u81ea\u52a8\u52a8\u4f5c\u8bc4\u4f30\u7684\u53ef\u884c\u6027\uff0c\u5e76\u5c1d\u8bd5\u901a\u8fc7 skeleton trajectories \u5206\u6790\u8df3\u9a6c</p> </li> <li> <p>\u5bf9\u5916\u79d1\u624b\u672f\u8bc4\u4f30\u7684\u5de5\u4f5c\u4e3a\u4e0d\u540c\u7684\u5916\u79d1\u52a8\u4f5c\u5b9a\u4e49\u4e86 specific features\uff0c\u4f7f\u5f97\u8fd9\u4e9b\u6210\u679c\u96be\u4ee5\u6cdb\u5316</p> </li> </ul>"},{"location":"sum/Selekton-Based/#relation-models","title":"Relation Models","text":"<p>CV \u793e\u533a\u5c1d\u8bd5\u5bf9 semantic / spatial / temporal relation \u8fdb\u884c\u5efa\u6a21</p> <p>\u4e00\u4e9b\u5de5\u4f5c\u5c1d\u8bd5\u5bf9 skeleton structure \u4e0a\u7684 spatial-temporal relations \u8fdb\u884c\u5efa\u6a21</p> <ul> <li>\u90e8\u5206\u5de5\u4f5c\u5c06 human skeleton \u6784\u5efa\u6210 tree\uff08\u79fb\u9664\u4e86\u4e00\u4e9b\u8fb9\uff09</li> <li>\u90e8\u5206\u5de5\u4f5c\u5bf9 neighbouring joints \u8fdb\u884c\u63d0\u53d6\uff0c\u5e76\u91cd\u65b0\u8fdb\u884c\u6392\u5217\uff08\u4f46 image \u4e2d\u76f8\u90bb\u7684\u70b9\u5728\u771f\u5b9e\u9aa8\u9abc\u7ed3\u6784\u4e2d\u53ef\u80fd\u4e0d\u76f8\u90bb\uff09</li> <li>Celiktutan \u5c1d\u8bd5\u5bf9 skeleton dynamic sequences \u8fdb\u884c\u5206\u6790\uff0c\u4f46\u5ffd\u89c6\u4e86\u5355\u5e45 skeleton graph \u4e2d\u5404 joint \u4e4b\u95f4\u7684\u8054\u7cfb</li> </ul>"},{"location":"sum/Selekton-Based/#graph-based-joint-relations","title":"Graph-based Joint Relations","text":"<ul> <li> <p>\u90e8\u5206 Action Recognition \u4e5f\u5c1d\u8bd5\u901a\u8fc7 graph \u5bf9 spatial-temporal joint relation \u8fdb\u884c\u5efa\u6a21</p> <p>simply connect the same joints individually across time</p> <p>\u4f46\u7531\u4e8e short-term, local fluency and proficiency \u5bf9\u4e8e AQA \u5341\u5206\u91cd\u8981\uff0c\u8fd9\u4e00\u65b9\u6cd5\u5e76\u4e0d\u597d\u7528</p> </li> <li> <p>\u672c\u6587\u901a\u8fc7\u5206\u6790 joints' neighbours on Both spatial &amp; temporal relation graphs \u6765\u5bf9\u66f4\u7ec6\u7c92\u5ea6\u7684\u7279\u5f81\u8fdb\u884c\u5efa\u6a21</p> </li> </ul>"},{"location":"sum/Selekton-Based/#3-approach_1","title":"3 Approach","text":""},{"location":"sum/Selekton-Based/#1-joint-commonality-module","title":"1) Joint Commonality Module","text":"<p>Learning motion of joint neighbourhoods</p> <p>learnable Spatial Relation Graph</p> <p>represents how much impact each neighbour has on the motion of a certain joint within each time step</p> <ul> <li>\u4f7f\u7528\u90bb\u63a5\u77e9\u9635 \\(A_s \\in \\mathbb{R}^{J\\times J}\\) \u8868\u793a\u9876\u70b9\u95f4\u7684\u8fde\u63a5\u5173\u7cfb\uff08\\(J\\) \u4e3a\u9876\u70b9\u603b\u6570\uff09\uff0c\u5176\u5143\u7d20\u503c\u975e\u8d1f<ul> <li>relevant pair \u4e0a\u7684\u5143\u7d20 is learnable</li> <li>irrelevant pair \u4e0a\u7684\u5143\u7d20\u88ab\u8bbe\u4e3a 0</li> </ul> </li> <li>\\(A_s(i,j)\\) \u8868\u793a i<sup>th</sup>-node \u5bf9 j<sup>th</sup>-node \u65bd\u52a0\u7684\u5f71\u54cd</li> </ul> <p>Joint Commonality Module \u672c\u8d28\u4e0a\u662f\u5728 Spatial Relation Graph \u8fdb\u884c graphConv \u64cd\u4f5c\uff0c\u6700\u7ec8\u8f93\u51fa Commonality Features</p> <p>\u5728\u8fdb\u884c Conv \u64cd\u4f5c\u524d\uff0c\u8be5\u6a21\u5757\u5b66\u4e60\u4e86 individual joints' motion\uff0c\u800c\u5728\u5377\u79ef\u540e\u5bf9 neighbourhoods \u8fdb\u884c\u4e86\u5b66\u4e60</p> <ul> <li> <p>\u5047\u8bbe\u8fdb\u884c graphConv \u524d\u7684 feature matrix \u4e3a \\(H_c^t\\)\uff0c\u5305\u542b\u4e86 t<sup>th</sup> time-step \u4e2d\u6240\u6709\u8282\u70b9\u7684 hidden state</p> <p>\\(c \\in \\{0,1\\}\\) \u8868\u793a graphConv \u662f\u5426\u5df2\u7ecf\u6267\u884c</p> </li> <li> <p>graphConv \u64cd\u4f5c\u53ef\u4ee5\u88ab\u89c6\u4e3a feature Matrix \u4e0e adjacent Matrix \u4e4b\u95f4\u7684\u4e58\u79ef\uff0c\u5373</p> \\[ H_1^t = A_s \u00b7 H_0^t \\in \\mathbb{R}^{J \\times M} \\] <ul> <li>\\(M\\) \u4e3a feature dimension of the hidden states</li> <li>the hidden states contain the motion features of the joints BEFORE the convolution\uff0c\u5373 \\(H_0^t = F^t \\in \\mathbb{R}^{J \\times M}\\)</li> </ul> </li> <li> <p>\u968f\u540e Module \u5bf9\u6240\u6709 nodes \u7684 hidden state \u8fdb\u884c\u805a\u5408(MeanPooling)\uff0c\u5f97\u5230 Commonality Feature</p> \\[ \\overline{h^t_c} = \\frac{1}{N} ({H^t_c}^T \u00b7 [1,1,1,...,1]^T) \\] </li> </ul> <p>Joint Difference Module \u672c\u8d28\u4e0a\u662f\u5c06\u7279\u5b9a joint \u4e0e\u5176 spatial &amp; temporal neighbours \u8fdb\u884c\u5bf9\u6bd4\uff0c\u4ece\u800c\u5bf9 motion difference \u8fdb\u884c\u5b66\u4e60\uff0c\u6700\u7ec8\u5f97\u5230 Difference Features</p> <p>\u76ee\u524d\u53ea\u8003\u8651\u7279\u5b9a joint \u5728 current &amp; previous time step \u4e2d\u7684 neighbours</p> <p>Temporal Relation Graph</p> <p>model the joint relations across two immediate time steps</p> <ul> <li>\u4f7f\u7528 \\(A_p \\in \\mathbb{R}^{J\\times J}\\) \u8868\u793a\u8282\u70b9\u95f4\u7684\u90bb\u63a5\u5173\u7cfb\uff0c\u88ab\u521d\u59cb\u5316\u4e3a \\([0,1)\\) \u7684\u968f\u673a\u6570</li> <li>\\(A_p(i,j)\\) \u8868\u793a (t-1)<sup>th</sup> \u4e2d\u7684 \\(v_i\\) \u5bf9 t<sup>t</sup> \u4e2d\u7684 \\(v_j\\) \u7684\u5f71\u54cd</li> </ul> <ol> <li> <p>compute the motion differences between joint \\(i\\) and each of its neighbours \\(j\\)</p> </li> <li> <p>aggregates the motion differences with weighted sum (\u6743\u91cd\u4e3a \\(w_j\\))</p> \\[ \\begin{align*} D_s^t(i,m) &amp;= \\sum_j w_j \u00b7 (A_s(i,j) \u00b7 (F^t(i,m) - F^t(j,m)))\\\\ D_p^t(i,m) &amp;= \\sum_j w_j \u00b7 (A_p(i,j) \u00b7 (F^t(i,m) - F^t(j,m)))\\\\ &amp;1 \\leq i,j \\leq J, 1 \\leq m \\leq M \\end{align*} \\] </li> <li> <p>\u901a\u8fc7 MeanPooling \u805a\u5408\u5f97\u5230 Difference Features \\(\\overline{d^t_s}, \\overline{d^t_p}\\)</p> \\[ \\overline{d^t_s} = \\frac{1}{N}({D_s^t}^T \u00b7 [1,1,1,...,1]^T) \\] </li> </ol>"},{"location":"sum/Selekton-Based/#2-joint-difference-module","title":"2) Joint Difference Module","text":"<p>Great motion differences among joints within a neighbourhood indicate a lack of coordination</p>"},{"location":"sum/Selekton-Based/#3-regression-module","title":"3) Regression Module","text":"<p>Input \u5305\u542b\u4e86:</p> <ol> <li>whole-scene video feature \\(q^t \\in \\mathbb{R}^M\\)\uff08\u8fd0\u52a8\u5458\u5728\u80cc\u666f\u4e2d\u7684\u4f4d\u7f6e\u4e5f\u662f\u4e00\u4e2a\u91cd\u8981\u4fe1\u606f\uff09</li> <li>Commonality Features \\(\\overline{h^t_c}\\)</li> <li>Difference Features \\(\\overline{d^t_s}, \\overline{d^t_p}\\)</li> </ol> <ol> <li> <p>\u901a\u8fc7 feature encoder \u5bf9 input feature \u8fdb\u884c encode</p> \\[ \\begin{align*} \\hat{u^t_i} &amp;= \\text{EncodeFunc}_i(u_i^t)\\\\ u_i^t &amp;\\in \\{q^t,\\overline{h^t_0}, \\overline{h^t_1}, \\overline{d^t_s}, \\overline{d^t_p}\\} \\end{align*} \\] </li> <li> <p>\u4f7f\u7528 feature pooling layer \u6574\u5408\u5f97\u5230 overall feature \\(v^t\\)\uff08\\(\\alpha_i ,\\beta_i\\) \u5206\u522b\u4e3a scalar &amp; bias\uff09</p> \\[ v^t = \\sum_i \\alpha_i \u00b7 \\hat{u^t_i} + \\beta_i \\] <p>\u4e3a\u4e86\u964d\u4f4e\u4e0d\u540c feature \u4e4b\u95f4\u5b58\u5728\u7684\u5197\u4f59\uff0c\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u4f1a\u4e3a feature pooling layer \u6dfb\u52a0\u6b63\u5219\u9879\uff1a</p> \\[ R_O = \\sum_{i,j} \\gamma \u00b7 (\\hat{u^t_i}^T \u00b7 \\hat{u^t_j}) \\] </li> <li> <p>\u4f7f\u7528\u4e24\u4e2a FC Layer \u8fdb\u884c\u56de\u5f52\u8ba1\u7b97\uff08\u89c6\u9891\u88ab\u5212\u5206\u4e3a \\(t\\) \u4e2a segments\uff0c\u5e76\u72ec\u7acb\u8fdb\u884c\u56de\u5f52\u9884\u6d4b\uff09</p> \\[ \\hat{s} = \\sum_t \\text{RegFunc}(v^t) \\] </li> </ol>"},{"location":"sum/Selekton-Based/#evaluation","title":"Evaluation","text":"<p>\u4f7f\u7528 MSE Loss\uff0ctogether with:</p> <ul> <li>orthogonal regularization term (with a weight 0.8)</li> <li>L2 regularization terms (with a weight 0.1) on the relation graphs</li> </ul>"},{"location":"sum/Selekton-Based/#2022-skeleton-based-deep-pose-feature-learning","title":"2022: Skeleton-based Deep Pose Feature Learning","text":"<p>\u597d\u50cf\u53ea\u662f\u628a ST-GCN \u53e0\u4e86 10 \u5c42\uff0c\u518d\u52a0\u4e00\u4e2a LSTM</p>"},{"location":"sum/Selekton-Based/#1-abstract_1","title":"1 Abstract","text":"<ul> <li> <p>\u73b0\u6709\u5de5\u4f5c</p> <ul> <li> <p>Evaluate single / sequential-defined action in short-term videos</p> <p>Sample: diving(\u8df3\u6c34), vault(\u8df3\u9a6c)</p> </li> <li> <p>Extract features directly from RGB videos through 3D-ConvNets</p> <p>\u5bfc\u81f4 feature \u548c scene info \u6df7\u6dc6\u5728\u4e00\u8d77</p> <ul> <li>\u4f46\u53e6\u4e00\u7bc7\u6587\u7ae0\u53c8\u6279\u5224\u4e86 skeleton-based \u65b9\u6cd5\u5ffd\u89c6\u4e86\u6c34\u82b1\u7b49\u73af\u5883\u56e0\u7d20\uff0c\u5bfc\u81f4\u51c6\u786e\u5ea6\u4e0b\u964d</li> <li>\u5927\u6982\u5f97\u8003\u8651\u4e00\u4e0b trade off\uff1f</li> </ul> </li> </ul> </li> <li> <p>Long-duration Video \u9762\u4e34\u7684\u6311\u6218</p> <ul> <li> <p>Contain multiple chronologically(\u65f6\u5e8f\u4e0a) inconsistent actions</p> <p>e.g. \u4e0d\u540c\u82b1\u6ed1\u77ed\u8282\u76ee\u5bf9\u4e8e \u6ed1\u884c/\u8df3\u8dc3/\u65cb\u8f6c \u7684\u7f16\u6392\u987a\u5e8f\u5e76\u4e0d\u4e00\u81f4</p> </li> <li> <p>Actions only have slight difference in a few frames</p> <p>e.g. 3Flip &amp; 3Lutz \u5176\u5b9e\u957f\u5f97\u5f88\u50cf</p> </li> </ul> </li> <li> <p>WHY Skeleton-based \uff1f</p> <p>Yes, but: \u6027\u80fd\u6ca1\u6709 RGB-Based \u65b9\u6cd5\u597d</p> <ul> <li> <p>\u4e0d\u5e94\u8be5\u53ea\u63d0\u4f9b\u4e00\u4e2a final score\uff0c\u8fd8\u5e94\u8be5\u63d0\u4f9b meaningful feedbacks \u5e2e\u52a9\u4eba\u4eec\u8fdb\u884c\u6539\u8fdb</p> </li> <li> <p>Robust to changes in: appearance, lighting, surrounding env</p> </li> </ul> </li> <li> <p>\u672c\u6587\u5de5\u4f5c\uff1a<code>deep pose feature learning</code> <code>long-duration videos (\u82b1\u6ed1/\u827a\u672f\u4f53\u64cd)</code></p> <ul> <li> <p>\u7279\u5f81\u63d0\u53d6\uff1a\u4f7f\u7528 Spatial-Temporal Pose Extraction (STPE) Module</p> <ul> <li> <p>Capture subtle changes</p> </li> <li> <p>Obtain skeletal data in space &amp; time dimensions</p> </li> </ul> </li> <li> <p>\u65f6\u5e8f\u7279\u5f81\u8868\u793a\uff1a\u4f7f\u7528 Inter-action Temporal Relation Extraction (ITRE) Module</p> <p>\u901a\u8fc7 RNN \u5bf9\u9aa8\u67b6\u6570\u636e\u7684\u65f6\u5e8f\u7279\u5f81\u8fdb\u884c\u5efa\u6a21</p> </li> <li> <p>Score Regression\uff1a\u4f7f\u7528 FCN\uff08\u5168\u5377\u79ef\u7f51\u7edc\uff09\u5b9e\u73b0</p> </li> <li> <p>Benchmarks: MIT-Skate, FIS-V</p> </li> </ul> </li> </ul>"},{"location":"sum/Selekton-Based/#2-relative-works","title":"2 Relative Works","text":""},{"location":"sum/Selekton-Based/#skeleton-based-aqa","title":"Skeleton-based AQA","text":"<p>\u5728 Sports \u9886\u57df\u7684\u7814\u7a76\uff1a</p> <ol> <li> <p>Pirsiavash \u6700\u65e9\u63d0\u51fa\u5c06 pose feature \u5e94\u7528\u4e8e AQA \u9886\u57df</p> <p>\u901a\u8fc7 Discrete Cosine Transform (DCT) \u5bf9 pose feature \u8fdb\u884c\u7f16\u7801\uff0c\u968f\u540e\u4f7f\u7528 SVR \u8fdb\u884c\u56de\u5f52\u9884\u6d4b</p> </li> <li> <p>Venkataraman \u63d0\u51fa\u8ba1\u7b97 multivariate approximate entropy (\u591a\u53d8\u91cf\u8fd1\u4f3c\u71b5)\uff0c\u5bf9 \u5355\u4e2a\u5173\u8282\u7684\u53d8\u5316 &amp; \u5173\u8282\u95f4\u8054\u7cfb \u8fdb\u884c\u5efa\u6a21</p> </li> <li> <p>Nekoui \u5efa\u7acb\u4e86 two-stream(\u53cc\u6d41) \u7f51\u7edc\uff0c\u5bf9 appearance &amp; pose feature \u5206\u522b\u8fdb\u884c\u5efa\u6a21</p> </li> <li> <p>Pan \u63d0\u51fa\u4e86 Graph-based Model \u5bf9 \u5173\u8282\u95f4\u534f\u65b9\u5dee &amp; \u8eab\u4f53\u5c40\u90e8\u52a8\u4f5c \u8fdb\u884c\u5efa\u6a21</p> </li> </ol> <p>\u4e0e GCN \u76f8\u5173\u7684\u65b9\u6cd5\uff1a\uff08\u53ea\u5728 diving \u8fd9\u6837\u7684\u77ed\u65f6\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u8fc7\uff09</p> <p>GCN \u76f8\u6bd4\u4e8e\u5176\u4ed6\u5377\u79ef\u7f51\u7edc\uff0c\u5728\u9aa8\u67b6\u6570\u636e\u8fd9\u79cd graph-structured \u7684\u7279\u5f81\u4e0a\u5177\u6709\u66f4\u597d\u7684\u6cdb\u5316\u6027</p> <ol> <li> <p>Bruce \u63d0\u51fa\u4e86\u4e00\u4e2a two-task GCN \u5bf9 deep pose feature \u8fdb\u884c\u63d0\u53d6</p> <p>\u5e94\u7528\u4e8e\u8001\u5e74\u75f4\u5446\u75c7\u7684\u5f02\u5e38\u68c0\u6d4b\u4e0e\u8d28\u91cf\u8bc4\u4f30</p> </li> <li> <p>Nekoui \u63d0\u51fa\u4e86\u6570\u636e\u96c6 ExPose\uff0c\u5e76\u4f7f\u7528\u4e86 ST-GCN \u4ece\u63d0\u53d6\u7684\u5173\u8282\u5e8f\u5217\u4e2d\u63d0\u53d6 pose feature</p> </li> </ol>"},{"location":"sum/Selekton-Based/#3-approach_2","title":"3 Approach","text":"Figure Skating Grading Rule <p>\u8fd9\u91cc\u4e5f\u6807\u699c\u4e86\u4e00\u4e0b \"based on the rules of Figure-Skating\"\uff0c\u7136\u540e\u6c34\u4e86\u597d\u957f\u4e00\u6bb5</p> \\[ \\text{Final Score} = TES + PCS - TDS \\] <ul> <li>\u6bcf\u4e2a\u52a8\u4f5c\u7684 \\(TES = \\text{basic score} + GOE\\)\uff0c\\(TDS \\geq 0\\) \u662f\u5931\u8bef\u6263\u5206</li> <li>\u4e00\u822c\u6709 9 \u4f4d\u88c1\u5224\uff0c\u53bb\u6389 \u6700\u9ad8&amp;\u6700\u4f4e \u540e\u53d6\u5e73\u5747\u503c</li> </ul>"},{"location":"sum/Selekton-Based/#1","title":"1 \u9aa8\u67b6\u4fe1\u606f\u83b7\u53d6 &amp; \u9884\u5904\u7406","text":"<p>\u8bb0\u5171\u6709 \\(N\\) \u4e2a\u89c6\u9891\u7684 Labled RGB \u89c6\u9891\u6570\u636e\u96c6 \\(V = \\{v_i, l_i\\}_{i= 1 \\sim N}\\)</p> <ul> <li>\u5176\u4e2d i-th \u5177\u6709 \\(m\\) \u5e27\u7684\u89c6\u9891\u8bb0\u4e3a \\(v_i = \\{I_j\\}_{j=1\\sim m}\\)</li> <li>\\(l_i\\) \u662f i-th \u89c6\u9891\u7684 ground-truth label</li> </ul> <ol> <li> <p>\u5bf9 i-th \u89c6\u9891\u8fdb\u884c Pose Estimation \u540e\uff0c\u5f97\u5230\u9aa8\u67b6\u6570\u636e \\(v_i \\rightarrow \\{S_j\\}_{j=1\\sim m}\\)</p> <p>\u8fd9\u7bc7\u6587\u7ae0\u4f7f\u7528\u4e86 OpenPose \u63d0\u4f9b\u7684 18-joint Model</p> </li> <li> <p>\u5bf9\u6240\u6709\u7684 Skeleton Seq \u91c7\u53d6\u76f8\u540c\u7684\u91c7\u6837\u7b56\u7565\uff1a\u53ea\u5904\u7406\u524d \\(T\\) \u5e27\uff0c\u4f7f \\(s = \\{S_j\\}_{j=1\\sim m} \\rightarrow \\{S_j\\}_{j=1\\sim T}\\)</p> </li> <li> <p>\u5c06 \\(s\\) \u5212\u5206\u4e3a \\(M\\) \u4e2a\u4e0d\u91cd\u53e0\u7684\u5b50\u5e8f\u5217 \\(s \\rightarrow \\{P_k\\}_{k = 1 \\sim M}\\)\uff0c\u6bcf\u4e2a\u5b50\u5e8f\u5217\u5bf9\u5e94\u957f\u5ea6 \\(Z = \\frac{T}{M}\\)</p> </li> <li> <p>\u8003\u8651\u5355\u4e2a\u5b50\u5e8f\u5217 \\(P = \\{p^i = \\{(x_j^i, y_j^i, \\text{ac}_j^i)\\}_{j=1}^{18}\\}_{i=1}^Z\\)</p> <p>\u5176\u4e2d\uff1a \\((x_j, y_j)\\) \u662f j-th joint \u5728\u7b1b\u5361\u5c14\u5750\u6807\u7cfb\u4e0b\u7684\u5750\u6807\uff0c\\(\\text{ac}_j\\) \u662f\u8be5\u5750\u6807\u7684\u7f6e\u4fe1\u5ea6</p> </li> <li> <p>BatchNormalization\uff1a \\(x' = \\frac{x - \\mu}{\\sigma}\\)</p> </li> </ol>"},{"location":"sum/Selekton-Based/#2-stpe","title":"2 \u65f6\u7a7a\u59ff\u6001\u7279\u5f81\u63d0\u53d6 (STPE)","text":"<p>\u719f\u6089\u7684\u7c7b ST-GCN \u601d\u8def\uff1a</p> <p>\u597d\u7684\uff0c\u76f4\u63a5\u62ff ST-GCN \u5f53 backbone \u4e86</p> <ul> <li>Spatial Dimension: \u4f7f\u7528 skeleton-graph \u6765\u8868\u793a\u5173\u8282\u53ca\u5176\u8fde\u63a5\u5173\u7cfb</li> <li>Temporal Dimension:  \u628a\u76f8\u90bb frame \u91cc\u7684\u540c\u4e00\u4e2a joint \u8fde\u8d77\u6765\u5c31\u7b97\u5b8c\u4e8b</li> </ul> <p>\u5c0f\u5c0f\u6539\u8fdb\uff1a</p> <ul> <li> <p>Basic Block = SpatialConv layer \\(\\rightarrow\\) TemporalConv Layer \\(\\rightarrow\\) Dropout Layer</p> <p>SConv &amp; TConv \u7684\u8f93\u51fa\u90fd\u6709 BatchNorm + ReLU \u7684\u5904\u7406</p> </li> <li> <p>\u5806\u53e0 10 \u4e2a Basic Block\uff1a</p> <ul> <li>\u4f7f\u7528 \\(C\\) \u8868\u793a feature Channel\uff0c\\(Z\\) \u4e3a\u5b50\u5e8f\u5217\u65f6\u957f\uff0c\\(J = 18\\) \u4e3a\u5173\u8282\u603b\u6570</li> <li>\u4ee4 temporal kernel size = 9 &amp;&amp; \\(L_4, L_7\\) strides = 2\uff0c\u5219 <code>4-3-3</code> \u5c42\u7684\u8f93\u51fa\u901a\u9053\u6570\u5206\u522b\u4e3a <code>64-128-256</code></li> </ul> \\[ \\text{input}^{C \\times Z \\times J} \\rightarrow \\text{STPE} \\rightarrow \\text{output}^{256 \\times Z' \\times J} = f_p \\] </li> </ul>"},{"location":"sum/Selekton-Based/#3-atre","title":"3 \u52a8\u4f5c\u95f4\u65f6\u5e8f\u8054\u7cfb\u63d0\u53d6 (ATRE)","text":"<ul> <li>\u5bf9\u4e8e\u82b1\u6ed1\u6765\u8bf4 action \u4e4b\u95f4\u7684\u8854\u63a5\u4f1a\u5f71\u54cd PCS \u5f97\u5206</li> <li>\u8fd9\u91cc\u901a\u8fc7\u4f7f\u7528\u4e86 LSTM \u7684 RNN \u5b9e\u73b0</li> </ul> <p>\u7ecf\u8fc7 STPE \u6a21\u5757\u7684\u5904\u7406\uff0c\u6211\u4eec\u5f97\u5230\u4e86 pose feature \\(F_p = \\{f_p^k\\}_{k=1\\sim m}\\)</p> <ol> <li> <p>\u4f7f\u7528 \u5168\u8fde\u63a5(FC) \u5c42\u6765</p> <ul> <li>Remove redundant information</li> <li>Reduce dimension of Pose Feature</li> </ul> </li> <li> <p>\u4f7f\u7528 BatchNorm \u5c42\u63d0\u5347\u6cdb\u5316\u80fd\u529b</p> </li> </ol> <p>\u5173\u4e8e LSTM \u7684\u5b9e\u73b0</p> <p>\u4f7f\u7528\u4e00\u4e2a \\(M\\) time steps \u7684 LSTM \u7f51\u7edc\uff08\u56e0\u4e3a segmentation \u7684\u6570\u91cf\u662f\u56fa\u5b9a\u7684\uff09</p> <ul> <li> <p>\u5171\u6709 \\(M\\) \u4e2a Memory-cells \u7528\u4e8e\u5b58\u50a8 info * output feature</p> </li> <li> <p>\u6bcf\u4e2a cell\uff1a</p> <ul> <li>\u8f93\u5165 = i-th skeleton subSeq + \u4e0a\u4e00\u4e2a cell \u7684 output</li> <li>layer \u6570\u91cf\u4e3a 1\uff0chidden size of layer = 256</li> </ul> </li> <li> <p>\u6700\u7ec8\u8f93\u51fa \\(f_t\\)\uff08\u4e24\u79cd\u65b9\u6848\uff09\uff1a</p> <ol> <li>\u2705 \u6700\u540e\u4e00\u4e2a LSTM cell \u7684\u8f93\u51fa</li> <li>\u6240\u6709 LSTM cells \u8f93\u51fa\u7684\u5e73\u5747\u503c</li> </ol> </li> </ul>"},{"location":"sum/Selekton-Based/#4","title":"4 \u56de\u5f52\u9884\u6d4b","text":"<ul> <li> <p>\u4f7f\u7528 3 Layers FCN(\u5168\u8fde\u63a5\u795e\u7ecf\u7f51\u7edc) \u8fdb\u884c\u7279\u5f81\u964d\u7eac</p> <p>\u7531\u4e8e STPE \u7684\u8f93\u51fa\u4e3a \\(C' \\times Z' \\times J\\)\uff0c3 Layers \u7684\u8282\u70b9\u6570\u5206\u522b\u4e3a <code>[C'*Z'*J, 2048, 1]</code></p> </li> <li> <p>\u4f7f\u7528 2 Layers FCN \u8fdb\u884c\u56de\u5f52\u9884\u6d4b\uff0c\u8282\u70b9\u6570\u5206\u522b\u4e3a <code>[256, 1]</code></p> </li> </ul> <p>\u6574\u4e2a\u8fc7\u7a0b\u53ef\u88ab\u63cf\u8ff0\u4e3a\uff1a</p> \\[ \\hat{l} = \\text{Activation}(\\text{FC}(f_s)), f_s \\in \\{f_p, f_t\\} \\] <p>Finding</p> <p>\u7531\u4e8e\u4e24\u4e2a Benchmark \u4e4b\u95f4\u7684\u5206\u6570\u5206\u5e03\u4e0d\u592a\u4e00\u6837\uff0c\u4f5c\u8005\u53d1\u73b0\u4f7f\u7528 <code>original data + ReLU()</code> \u7684\u6548\u679c &gt; <code>norm(data) + sigmoid()</code></p> <ul> <li> <p>Loss Function</p> <p>\u4e0d\u540c\u4e8e\u5176\u4ed6\u65b9\u6cd5\u590d\u6742\u7684\u635f\u5931\u51fd\u6570\uff0c\u8fd9\u91cc\u53ea\u7528\u9884\u6d4b\u503c\u4e0e\u5b9e\u9645\u503c\u4e4b\u95f4\u7684 MSE \u8bef\u5dee</p> \\[ L_{MSE} = \\frac{1}{N}\\sum_{i=1}^N(l_i - \\hat{l}_i)^2 \\] </li> </ul>"},{"location":"sum/Selekton-Based/#2023-ms-gcn","title":"2023: MS-GCN","text":"<p>Multi-skeleton Structure Graph Convolution Network</p>"},{"location":"sum/Selekton-Based/#1-introduction_1","title":"1 Introduction","text":"<ul> <li> <p>\u5df2\u6709\u5de5\u4f5c</p> <ul> <li> <p>\u5e7f\u6cdb\u7684\u5e94\u7528\u573a\u666f\uff1aphysical therapy &amp; rehabilitation, sporting event scoring, special skill training</p> </li> <li> <p>\u5f53\u524d\u7814\u7a76\u7684\u4e00\u4e9b\u4e0d\u8db3\uff1a</p> <ol> <li> <p>\u5927\u90e8\u5206\u805a\u7126\u4e8e short video, \u5ffd\u7565\u4e86 long-duratoin videos\u3002\u800c\u957f\u4f53\u80b2\u89c6\u9891\u7684 challenges \u5728\u4e8e\uff1a</p> <ul> <li>large action speed variance &amp; duration</li> <li>high similarity categories</li> </ul> </li> <li> <p>\u901a\u8fc7 Deeplearning NN \u4ece RGB video \u4e2d\u63d0\u53d6\u4fe1\u606f\u7684\u65b9\u6cd5\uff1a</p> <ul> <li>\u5ffd\u89c6\u4e86 specific postures defnined by dynamic changes in human body joints </li> <li>\u65e0\u6cd5\u533a\u5206 3Lz-3Lp \u4e0e 3Lz-3Tp \uff08\u4f46\u662f skeletion-based \u65b9\u6cd5\u53ef\u4ee5\uff09</li> </ul> </li> </ol> </li> <li> <p>Pose Estimation \u6280\u672f\u5bf9 AQA \u7684\u5f71\u54cd</p> <ul> <li>\u53ef\u4ee5\u76f4\u63a5\u4ece RGB video \u4e2d\u63d0\u53d6 Skeleton data</li> <li>\u73b0\u6709\u65b9\u6cd5\u53ea\u662f\u6839\u636e\u65e0\u529b\u7ed3\u6784\u5bf9 joints \u8fdb\u884c\u8fde\u63a5\uff0c\u5ffd\u7565\u4e86\u8bed\u4e49\u4fe1\u606f\u4ee5\u53ca\u8eab\u4f53\u5404\u90e8\u5206\u95f4\u7684\u8fde\u63a5\u5173\u7cfb</li> </ul> </li> </ul> </li> <li> <p>\u672c\u6587\u5de5\u4f5c</p> <ul> <li> <p>\u63d0\u51fa\u4e86\u4e09\u79cd skeleton structures \u4ee5\u5b9e\u73b0\u5bf9 \u5173\u8282 &amp; \u8eab\u4f53\u90e8\u4f4d \u8fd0\u52a8\u6a21\u5f0f\u7684\u5efa\u6a21</p> <p>Joints' self-connection, intra-part connection, inter-part connection</p> </li> <li> <p>\u8bbe\u8ba1\u4e86\u4e00\u4e2a Temporal Attention Learning Module\uff0c\u7528\u4e8e\u63d0\u53d6 skeleton subSeq \u4e2d\u7684\u65f6\u5e8f\u8054\u7cfb\uff0c\u5e76\u5bf9\u4e0d\u540c action \u8d4b\u4e88\u6b63\u786e\u7684\u6743\u91cd</p> </li> <li> <p>benchmarks: MIT-Skate, Rhythmic Gymnastics</p> </li> </ul> </li> </ul>"},{"location":"sum/Selekton-Based/#2-related-works_1","title":"2 Related Works","text":""},{"location":"sum/Selekton-Based/#rgb-video-baesd-aqa","title":"RGB video-baesd AQA","text":"<ul> <li>RGB video-based \u65b9\u6cd5\u901a\u8fc7\u63d0\u53d6\u4e30\u5bcc\u7684\u65f6\u7a7a\u4fe1\u606f\uff0c\u5df2\u7ecf\u5b9e\u73b0\u4e86\u5bf9 short-video \u4e2d\u7684 simple action \u7684\u6709\u6548\u8bc4\u4f30</li> <li>\u4f46\u53d7\u590d\u6742\u573a\u666f\u56e0\u7d20\u5f71\u54cd\uff08\u4eba\u4f53\u5916\u89c2 / \u80cc\u666f\u53d8\u5316\uff09\uff0cRGB-based \u65b9\u6cd5\u5305\u542b\u4e86\u8bb8\u591a\u65e0\u5173\u7684\u573a\u666f\u4fe1\u606f\uff0c\u4f7f\u5176\u4e0d\u80fd\u51c6\u786e\u63cf\u8ff0\u4eba\u4f53\u8fd0\u52a8</li> </ul> <ol> <li>\u5c11\u6570\u4eba\u5c06 AQA \u89c6\u4e3a\u4e00\u4e2a level classifying task</li> <li> <p>\u5927\u591a\u5c5e\u4eba\u5c06 AQA \u5f53\u505a\u4e00\u4e2a regression task:</p> <ul> <li>Dong \u63d0\u51fa\u4e86 Multistage Regression Model (MSRM) \u7528\u4e8e\u4ece\u4e0d\u540c\u7684 hidden substages \u4e2d\u63d0\u53d6\u5e76 fuse feature</li> <li>Li \u63d0\u51fa\u4e86\u4e00\u4e2a 2 * C3D + 2 * FC \u7684\u7aef\u5230\u7aef\u6a21\u578b\uff0c\u5e76\u4f7f\u7528\u4e86\u4e24\u4e2a loss\uff08\u5206\u522b\u7ea6\u675f quality score &amp; rank\uff09</li> <li>Gao \u5c06 agents \u5212\u5206\u4e3a primary/secondary \u4e24\u7c7b\uff0c\u4ee5\u5bf9\u4ea4\u4e92\u5f0f\u52a8\u4f5c\u4e2d\u7684\u975e\u5bf9\u79f0\u5173\u7cfb\u8fdb\u884c\u5206\u6790</li> <li>Wang \u63d0\u51fa\u4e86 TSA \u6a21\u578b\uff0c\u4f7f\u7528 single object tracker \u6765\u533a\u5206\u524d\u540e\u666f</li> </ul> </li> <li> <p>\u6700\u8fd1\uff0c\u4e5f\u6709\u4e00\u4e9b\u4eba\u805a\u7126\u4e8e pair-ranking task:</p> <ul> <li>Doughty \u6700\u521d\u8003\u8651\u4e86\u5c06\u4efb\u610f\u4e24\u4e2a\u89c6\u9891\u7ec4\u6210\u8f93\u5165\uff0c\u5e76\u63d0\u51fa\u4e86 similarity loss</li> <li>Jain \u4f7f\u7528 C3D \u8fdb\u884c\u7279\u5f81\u63d0\u53d6\uff0c\u5e76\u901a\u8fc7 Siamese Network \u5b9e\u73b0\u4e86 reference-guided evaluation</li> <li>Yu \u4f7f\u7528 I3D \u63d0\u53d6\u7279\u5f81\uff0c\u5e76\u901a\u8fc7 GART \u6765\u5b66\u4e60\u4e00\u5bf9\u89c6\u9891\u95f4\u7684 relative score</li> </ul> </li> </ol>"},{"location":"sum/Selekton-Based/#skeleton-based-aqa_1","title":"Skeleton-based AQA","text":"<p>\u73b0\u6709\u7684 Skeleton-based Methods \u805a\u7126\u4e8e local \u4fe1\u606f</p> <p>\u4ed6\u4eec\u901a\u8fc7\u6784\u5efa single joint adjacnet matrix \u6765\u8868\u793a\u4eba\u4f53\u9aa8\u9abc\u7684\u81ea\u7136\u62d3\u6251\u7ed3\u6784</p> <p>\u4f46\u8fd9\u79cd\u65b9\u6cd5\u5ffd\u7565\u4e86\u65e0\u9aa8\u9abc\u8fde\u63a5\u5173\u8282\u95f4\u7684\u9690\u542b\u4fe1\u606f\uff0c\u4ee5\u53ca global \u89c6\u89d2\u4e0b\u8eab\u4f53\u5404\u90e8\u5206\u95f4\u5173\u7cfb\u4e2d\u6697\u542b\u7684\u4fe1\u606f</p> <p>=&gt; \u8fd9\u4f7f\u5f97\u63d0\u53d6\u4fe1\u606f\u5bf9\u4e8e\u4eba\u4f53\u8fd0\u52a8\u7279\u5f81\u7684\u8868\u5f81\u80a4\u6d45\u3001\u4e0d\u5145\u5206\uff0c\u8fdb\u800c\u964d\u4f4e\u9884\u6d4b\u6027\u80fd</p> <ul> <li>Prisiavash \u9996\u5148\u5c06 pose estimation \u5e94\u7528\u4e8e AQA\uff08DCT + SVR\uff09</li> <li>Bruce \u63d0\u51fa\u4e86\u57fa\u4e8e GCN \u7684\u65b9\u6cd5\uff0c\u5c06 feature embed into 2D-Vecs\uff0c\u5e76\u901a\u8fc7 SoftMax Classifier \u63a2\u6d4b\u5f02\u5e38\u52a8\u4f5c</li> <li> <p>Pan \u63d0\u51fa\u5728\u77ed\u89c6\u9891\u4e2d\u5e94\u7528 joints relation graph\uff0c\u4ed6\u4eec\u63d0\u51fa\u4e86\u4e24\u4e2a\u6a21\u5757\uff1a\u4e00\u4e2a\u6a21\u5757\u4ece\u8eab\u4f53\u90e8\u4f4d\u52a8\u529b\u5b66\u4e2d\u63d0\u53d6\u7279\u5f81\uff0c\u53e6\u4e00\u4e2a\u6a21\u5757\u4ece\u5173\u8282\u534f\u8c03\u4e2d\u5b66\u4e60\u3002</p> <p>\u5728\u968f\u540e\u7684\u5de5\u4f5c\u4e2d\uff0c\u4ed6\u4eec\u5f00\u53d1\u4e86\u4e00\u79cd\u81ea\u9002\u5e94\u52a8\u4f5c\u8bc4\u4f30\u7cfb\u7edf\uff0c\u8be5\u7cfb\u7edf\u53ef\u4ee5\u6839\u636e\u5173\u8282\u76f8\u4e92\u4f5c\u7528\u4e3a\u4e0d\u540c\u7c7b\u578b\u7684\u52a8\u4f5c\u81ea\u52a8\u6784\u5efa\u4e0d\u540c\u7684\u8bc4\u4f30\u67b6\u6784</p> </li> <li> <p>[Action Recognition] Plizzari \u63d0\u51fa\u4e86 two-stream Spatial-Temporal Transformer Network (ST-TR)\uff0c\u4f7f\u7528 Spatial/Temporal Self-attention Module \u5206\u522b\u5bf9\u5e27\u5185/\u5e27\u95f4\u5173\u7cfb\u8fdb\u884c\u5efa\u6a21</p> </li> <li> <p>\u4e00\u4e9b\u65b9\u6cd5\u5c1d\u8bd5\u5c06 appearance &amp; pose \u4fe1\u606f\u7ed3\u5408\uff0c\u4ed6\u4eec\u901a\u5e38\u91c7\u7528\u53cc\u6d41\u7f51\u7edc\u72ec\u7acb\u5904\u7406\u4e24\u79cd\u7279\u5f81\uff0c\u5e76\u5728 fuse \u540e\u8fdb\u884c\u56de\u5f52\u9884\u6d4b</p> </li> </ul>"},{"location":"sum/Selekton-Based/#3-approach_3","title":"3 Approach","text":""},{"location":"sum/Selekton-Based/#1-skeleton-sequence-extraction-sampling","title":"1) Skeleton Sequence Extraction &amp; Sampling","text":"<ul> <li> <p>\u672c\u6587\u4f7f\u7528 18-joints OpenPose + avanced 2D Pose Estimation \u8fdb\u884c\u7279\u5f81\u63d0\u53d6</p> </li> <li> <p>\u8003\u8651\u5230\u957f\u89c6\u9891\u4e2d skeletonSeq \u5b58\u5728\u4fe1\u606f\u5197\u4f59\uff0c\u672c\u6587\u9996\u5148\u5728 temporal demension \u4e0a\u8fdb\u884c\u91c7\u6837\uff1a</p> <p>\u5bf9\u4e8e skeletonSeq \\(S = \\{s_i^t\\}_{i=1\\sim 18}^{t=1\\sim T} = \\{(x_i^t, y_i^t ,acc_i^t)\\}\\)</p> <ol> <li>\u4f7f\u7528 uniform sampling strategy with fixed interval \\(\\Delta l\\)\uff0c\u6709\uff1a\\(S' = \\{s'_l\\}_{l=1\\sim L}, L = T/\\Delta l\\)</li> <li>\u4f7f\u7528 uniform partition strategy \u5c06\u91c7\u6837\u540e\u7684 skeletonSeq \u5212\u5206\u6210 \\(G\\) \u4e2a\u4e0d\u91cd\u53e0\u7684 subSeqs (\u6bcf\u4e2a subSeq \u7684\u957f\u5ea6\u4e3a \\(L/G\\))</li> </ol> </li> </ul>"},{"location":"sum/Selekton-Based/#2-deep-pose-feature-learning","title":"2) Deep Pose Feature Learning","text":"3 types of skeleton structures <ol> <li> <p>Joints' Self-connection \\(A_{self}\\)</p> <p>\u4ee5 local \u89c6\u89d2\u805a\u7126\u4e8e\u5173\u8282\u672c\u8eab\uff0c\u786e\u4fdd\u6bcf\u4e2a joint \u5728\u5377\u79ef\u8fc7\u7a0b\u4e2d\u80fd\u88ab\u5e73\u7b49\u5bf9\u5f85</p> </li> <li> <p>Intra-part Connection \\(A_{intra}\\)</p> <p>\u6839\u636e\u9aa8\u9abc\u81ea\u7136\u8fde\u63a5\u5173\u7cfb\u5b9a\u4e49\uff0c\u5bf9\u8eab\u4f53\u7279\u5b9a\u90e8\u4f4d\u7684\u8fd0\u52a8\u6a21\u5f0f\u8fdb\u884c\u5efa\u6a21</p> </li> <li> <p>Inter-part Connection \\(A_{inter}\\)</p> <p>\u4f7f\u5f97\u4ece\u5c5e\u4e8e\u4e0d\u540c\u8eab\u4f53\u90e8\u4f4d\u7684 joint \u95f4\u4ea7\u751f\u4f9d\u8d56\u5173\u7cfb\uff0c\u786e\u4fdd\u80fd\u6355\u6349\u5230 global \u4fe1\u606f</p> </li> </ol> <p>\u5bf9\u4e8e normalized skeleton subSeq \\(P = \\{(x,y,acc)_v^t\\}_{v=1\\sim 18}^{t=1\\sim T} \\in \\mathbb{R}^{3\\times V \\times T'}\\)\uff0c\u4f7f\u7528\u4ee5\u4e0b\u7684\u65b9\u5f0f\u5bf9\u4e8e skeleton graph \u8fdb\u884c encode\uff1a</p> <ul> <li> <p>\u6784\u5efa Spatial Relation Graph \\(A = A_{self},A_{intra},A_{inter}\\)</p> <ul> <li> <p>\\(A_{self}\\) \u662f\u4e00\u4e2a \\(V \\times V\\) \u7684 identity matrix\uff0c\\(A_{intra},A_{inter}\\) \u662f \\(V\\times V\\) \u7684\u90bb\u63a5\u77e9\u9635</p> </li> <li> <p>\\(A_{ij} \\in \\{0,1\\}\\) \u8868\u793a\u9876\u70b9 \\((v_i, v_j)\\) \u95f4\u662f\u5426\u5b58\u5728\u8fb9</p> </li> </ul> </li> <li> <p>\u6784\u5efa MS-GCN: a hierachy of 10 stacked blocks</p> <ul> <li>10\u4e2a blocks \u7684\u8f93\u51fa\u901a\u9053\u6570\u4e3a 64*4 + 128*3 + 256*3</li> <li>4<sup>th</sup> &amp; 7<sup>th</sup> \u7684 <code>strideSize=2</code>\uff0c\u5176\u4ed6\u5c42\u5747\u4e3a 1</li> <li>\u8f93\u5165 \\(P_{in} \\in \\mathbb{R}^{C_{in} \\times 18 \\times T'}\\)</li> <li>\u6700\u540e\u4e00\u5c42\u7684\u8f93\u51fa\u5c06\u88ab flattern \u4e3a\u4e00\u4e2a vector\uff0c\u5e76\u8f93\u5165\u4e00\u4e2a\u5177\u6709 \\(d\\) \u4e2a\u795e\u7ecf\u5143\u7684 FC Layer \u8fdb\u884c\u964d\u7eac\uff0c\u5f97\u5230 \\(f_p \\in \\mathbb{R}^d\\)</li> </ul> </li> <li> <p>\u5355\u4e2a block \u7684\u7ed3\u6784\u5982\u4e0b\uff1a     </p> <ol> <li> <p>GCN Layer \u62e5\u6709\u4e09\u4e2a\u72ec\u7acb\u7684 Conv Operator \u4ee5\u5206\u522b\u5904\u7406\u4e09\u4e2a Graph \\(A\\)\uff0c\u4ece\u800c\u62bd\u53d6 motion pattern of joints &amp; body parts\u3002</p> <ul> <li> <p>\u6574\u4e2a block \u5728 Spatial \u7eac\u5ea6\u4e0a\u7684\u64cd\u4f5c\u53ef\u4ee5\u8868\u8ff0\u4e3a\uff1a\\(P_{spatial}(v_i) = \\sum_{j=1}^{18} \\sum_{k=1}^3 (M_{ij}^k \\odot A_{ij}^k)P_{in}(v_j) W^k\\)</p> </li> <li> <p>\\(M^k\\) \u7528\u4e8e scale the contribution of a nodes' feature\uff0c\\(W^k\\) \u662f Conv Kernel</p> </li> </ul> </li> <li> <p>TCN Layer \u901a\u8fc7 2D Conv (<code>kernelSize = 9*1</code>) \u5b9e\u73b0</p> <ul> <li> <p>GCN &amp; TCN \u5c42\u540e\u90fd\u8ddf\u4e86\u4e00\u4e2a BatchNorm\uff1b\u6b64\u5916\uff0c\u4e3a\u4e86\u8bad\u7ec3\u7684\u7a33\u5b9a\u6027\u589e\u52a0\u4e86 residual connection</p> </li> <li> <p>\u6574\u4e2a block \u7684\u5904\u7406\u8fc7\u7a0b\u53ef\u4ee5\u63cf\u8ff0\u4e3a\uff1a\\(P_{out} = \\sigma(\\gamma_{temporal}(\\sigma(\\gamma_{spatial}(P_{in}))))\\)\uff0c\\(\\sigma = ReLU(BatchNorm(\u00b7))\\)</p> </li> </ul> </li> </ol> </li> </ul>"},{"location":"sum/Selekton-Based/#3-temporal-attention-learning","title":"3) Temporal Attention Learning","text":"<p>extract features from the layout &amp; arrangement of actions</p> <p>\u7ecf\u8fc7 MS-GCN \u6a21\u5757\u5904\u7406\uff0c\u6211\u4eec\u53ef\u4ee5\u5f97\u5230 Pose Features \\(F_p = \\{f_p^m\\}_{m=1\\sim G} \\in \\mathbb{R}^d\\)\uff08\\(m\\) \u4e3a skeleten subSeq \u7f16\u53f7\uff09</p> <ol> <li> <p>\u6b64\u5904\u4f7f\u7528\u62e5\u6709 \\(G\\) \u4e2a cells \u7684 LSTM \u6765\u63d0\u53d6 subSeqs \u95f4\u7684 temporal structures\uff0c\u5bf9\u4e8e\u6bcf\u4e2a cell \u6765\u8bf4</p> <ul> <li><code>n_layer = 1</code>, <code>n_hidden_size = d'</code></li> <li>input = m<sup>th</sup>-pose features + \u4e0a\u4e00\u4e2a cell \u7684\u8f93\u51fa</li> <li>output = temporal relation feature \\(f_t^m \\in \\mathbb{R}^{d'}\\)\uff0c\u6240\u6709 cell \u7684\u8f93\u51fa\u96c6\u5408\u8bb0\u4e3a \\(F_t = \\{f_t^m\\}_{m=1\\sim G}\\)</li> </ul> </li> <li> <p>\u6839\u636e self-attention learning strategy \u5c06\u65b0\u7684 \\(F_t\\) embed \u6210 vector</p> </li> <li> <p>\u6839\u636e embedded vector \u751f\u6210 correlation matrix &amp; attention weight</p> </li> <li> <p>\u4f7f\u7528 \\(SoftMax(\u00b7)\\) \u5bf9 attention weight \u8fdb\u884c normalize</p> </li> <li> <p>\u5c06 weighted sum \u4f5c\u4e3a\u65b0\u7684 feature</p> </li> </ol> <p>\u4e0a\u8ff0\u8fc7\u7a0b\u53ef\u4ee5\u63cf\u8ff0\u4e3a\uff1a</p> \\[ \\begin{align*} e_m &amp;= V_p^T tanh(W_pf_t^m) \\\\ \\hat{F}_t &amp;= \\sum_{m=1}^G \\alpha(e_m) f_t^m \\end{align*} \\] <ul> <li> <p>\\(V_p \\in \\mathbb{R}^{d'}, W_p \\in \\mathbb{R}^{d' \\times d'}\\) \u5747\u4e3a\u53ef\u8bad\u7ec3\u53c2\u6570\uff0c\u7528\u4e8e\u8ba1\u7b97 correlation</p> </li> <li> <p>\\(e_m\\) \u4ee3\u8868 self-attention learning \u5bf9 m<sup>th</sup>-cell output \u4f30\u8ba1\u7684 attention weight</p> </li> <li> <p>\\(\\alpha(\u00b7)\\) \u8868\u793a\u5bf9 weight vec \\(\\{e_m\\}_{,=1\\sim G}\\) \u8fdb\u884c\u7684 SoftMax \u64cd\u4f5c</p> </li> <li> <p>\\(\\hat{F}_t \\in \\mathbb{R}^{d'}\\) \u662f Temporal Attention Learning \u5f97\u5230\u7684\u6700\u7ec8\u7279\u5f81</p> </li> </ul>"},{"location":"sum/Selekton-Based/#4-score-prediction-evaluation","title":"4) Score Prediction &amp; Evaluation","text":"<ul> <li> <p>\u5206\u6570\u9884\u6d4b\u901a\u8fc7 \u4e00\u4e2a\u5168\u8fde\u63a5\u5c42 + \u6fc0\u6d3b\u51fd\u6570 \u5b9e\u73b0\uff1a</p> \\[ \\hat{S} = \\text{activation}(FC(\\hat{F}_t)) \\] </li> <li> <p>\u4f7f\u7528 MSE Loss \u4ee5\u6700\u5c0f\u5316\u9884\u6d4b\u603b\u5206\u4e0e ground-truth \u4e4b\u95f4\u7684\u8bef\u5dee (\\(N\\) \u4e3a batchSize)\uff1a</p> \\[ \\mathcal{L} = \\frac{1}{n} \\sum_{n=1}^N (\\hat{S}_n - L_n)^2 \\] </li> </ul>"},{"location":"sum/Self-Supervised/","title":"\u81ea\u76d1\u7763\u5b66\u4e60 SSL","text":""},{"location":"sum/Self-Supervised/#2023-pecop","title":"2023: PECoP","text":""},{"location":"sum/Self-Supervised/#1-abstract","title":"1 Abstract","text":"<ul> <li> <p>\u5148\u524d\u7684\u5de5\u4f5c</p> <ul> <li> <p>\u56e0\u4e3a\u6709\u6807\u7b7e\u7684 AQA \u6570\u636e\u5f88\u5c11\uff0c\u5148\u524d\u7684\u5de5\u4f5c\u4e00\u822c\u57fa\u4e8e\u5728 large-scale domain-general dataset \u4e0a\u9884\u8bad\u7ec3\u7684\u6a21\u578b\u8fdb\u884c\u4f18\u5316</p> <p>=&gt; \u5728\u5b58\u5728\u8f83\u5927 domain shift \u65f6\uff0c\u6a21\u578b\u7684 generalisation \u8f83\u5dee</p> </li> <li> <p>\u4fa7\u91cd\u70b9\u4e0d\u540c\uff1a</p> <ul> <li> <p>\u5728 Parkinson\u2019s Disease \u4e25\u91cd\u6027\u8bc4\u4f30\u4e2d\uff0c\u8fd0\u52a8\u8282\u594f\u4e2d\u7684\u4e00\u4e24\u6b21\u95f4\u65ad\u90fd\u4f1a\u5bf9 quality score \u4ea7\u751f\u6781\u5927\u5f71\u54cd</p> </li> <li> <p>\u800c\u5728 pretraining task\uff08\u52a8\u4f5c\u5206\u7c7b\uff09\u4e2d\uff0c\u8f7b\u5fae\uff08\u751a\u81f3\u66f4\u52a0\u4e25\u91cd\u7684\uff09\u533a\u522b\u5e76\u4e0d\u4f1a\u5f71\u54cd action classification</p> </li> </ul> </li> <li> <p>Continual Pretraining</p> <p>\u4e3b\u8981\u7528\u4e8e NLP\uff0c\u4ee5\u901a\u8fc7 domain-specific unlabeled data \u8bad\u7ec3\u4efb\u52a1\u4e13\u7cbe\u7684\u6a21\u578b</p> <p>\u8fd9\u79cd\u65b9\u6cd5\u8981\u6c42\u66f4\u65b0\u6240\u6709\u53c2\u6570\uff0c\u540c\u65f6\u5b58\u50a8\u9488\u5bf9\u6240\u6709 separate task \u7684 params</p> </li> <li> <p>BatchNorm Tuning</p> <p>\u652f\u6301\u901a\u8fc7\u4ec5\u8c03\u8282 BatchNorm Layer \u53c2\u6570\u83b7\u53d6 domain-specific model</p> <p>\u4f46\u662f\u5728 \u8f83\u5c0f\u7684 / \u66f4\u52a0 domain-specific \u7684 AQA \u6570\u636e\u96c6\u4e2d\u4e0d\u8d77\u4f5c\u7528</p> </li> </ul> </li> <li> <p>\u521b\u65b0\u70b9</p> <p>PECoP \u901a\u8fc7\u589e\u52a0\u4e00\u4e2a\u989d\u5916\u7684\u9884\u8bad\u7ec3\u8fc7\u7a0b\u6765 reduce domain shift\uff08\u4e13\u6ce8\u4e8e\u7279\u5b9a AQA \u4efb\u52a1\uff09\uff1a</p> <ul> <li> <p>\u5f80\u9884\u8bad\u7ec3\u7684 3D CNN \u5916\u9762\u5305\u4e00\u5c42 3D-Adapters \u81ea\u76d1\u7763\u5b66\u4e60 spatiotemporal &amp; in-domain info</p> </li> <li> <p>\u53ea\u66f4\u65b0 Adapter \u4e2d\u7684\u53c2\u6570\uff0c\u9884\u8bad\u7ec3\u6a21\u578b\u53c2\u6570\u4fdd\u6301\u4e0d\u53d8</p> <p>\u6b64\u5904\u62c9\u8e29\u9700\u8981\u66f4\u65b0 \u6240\u6709\u53c2\u6570 \u7684 HPT</p> </li> </ul> </li> </ul>"},{"location":"sum/Self-Supervised/#2-relative-works","title":"2 Relative Works","text":"<ul> <li> <p>AQA\uff1a\u5982 Tang(USDL)\uff0cYu\uff08CoRe\uff09</p> <p>\u5927\u591a\u6570\u65b9\u6cd5\u5c06 AQA \u89c6\u4e3a \u76d1\u7763\u5b66\u4e60\uff08score\uff09\u7684 \u56de\u5f52\u4efb\u52a1\uff0c\u5e76\u5c1d\u8bd5\u964d\u4f4e\u88c1\u5224\u7684\u4e3b\u89c2\u5f71\u54cd</p> <p>\u8fd9\u4e9b\u65b9\u6cd5\u90fd\u5ffd\u89c6\u4e86 base dataset(K400) \u548c target dataset \u4e4b\u95f4\u7684 domain gap</p> </li> <li> <p>SSL for AQA</p> <p>\u8fd1\u671f\u7684\u4e00\u4e9b\u65b9\u6cd5\u5f00\u59cb\u5c1d\u8bd5 \u81ea\u76d1\u7763\u5b66\u4e60\uff08SSL\uff09\u3002\u5728\u4f20\u7edf\u7684 Regression Loss \u5916\uff0c\u5728\u8fc1\u79fb\u5b66\u4e60\u4e2d\u589e\u52a0\u4e86 SSL Loss\uff08\u4e0d\u9700\u8981\u6dfb\u52a0\u989d\u5916\u7684\u6570\u636e\u6807\u6ce8\uff09</p> <p>\u5982 Liu \u5e94\u7528\u4e86 Self-Supervised Contrasive Loss</p> </li> <li> <p>Continual Pretraining</p> <p>\u76f8\u6bd4\u4e8e\u4f20\u7edf\u7684\u8fc1\u79fb\u5b66\u4e60\uff0cContinual Pretraining \u901a\u8fc7 in-domain SSL \u589e\u5f3a\u4e86 domain shift \u95ee\u9898</p> <ul> <li> <p>Gururangan\uff1a\u8bc1\u660e\u4e86\u589e\u52a0 in-domain data pretraining \u5bf9\u4e8e\u6587\u672c\u5206\u7c7b\u6027\u80fd\u7684\u5f71\u54cd</p> </li> <li> <p>Reed\uff1a\u8bc1\u660e\u4e86\u5728\u66f4\u63a5\u8fd1 target dataset \u7684\u6570\u636e\u96c6\u4e0a\u8fdb\u884c Continual Pretraining \u53ef\u4ee5\u52a0\u5feb\u6536\u655b\u3001\u63d0\u9ad8\u9c81\u68d2\u6027</p> </li> <li> <p>Azizi\uff1a\u7ed3\u5408\u4e86 supervised pretraining\uff08on ImageNet\uff09 &amp; intermediate contrastive SSL\uff08on MedicalImages\uff09\uff0c\u5f97\u5230\u4e86\u5177\u6709\u66f4\u4f73\u6cdb\u5316\u6027\u7684\u533b\u5b66\u56fe\u50cf\u8bca\u65ad\u5668</p> </li> </ul> </li> <li> <p>Adapters</p> <p>Transformer \u67b6\u6784\u7684 lightweight bottleneck module\uff0c\u7528\u4e8e\u5b9e\u73b0 parameter-efficient \u8fc1\u79fb\u5b66\u4e60</p> <ul> <li> <p>Chen\uff1a</p> <ul> <li> <p>\u63d0\u51fa AdaptFormer \u7528\u4e8e\u53ef\u91cf\u5316\u7684 img/video \u8bc6\u522b\u4efb\u52a1</p> </li> <li> <p>\u63d0\u51fa Conv-Adapter\uff0c\u4f7f\u5176\u53ef\u4ee5\u5e94\u7528\u4e8e 2D CNN</p> </li> </ul> </li> </ul> </li> </ul>"},{"location":"sum/Self-Supervised/#3-approach","title":"3 Approach","text":"<ul> <li> <p>Training Set\uff1a</p> <ol> <li>\\(D_g\\): large-scale, labelled, domain-general</li> <li>\\(D_t\\): target video dataset in the AQA domain</li> </ol> <p>\u4e24\u4e2a\u6570\u636e\u96c6\u5206\u522b\u5bf9\u5e94\u5177\u6709 significant domain discrepancy \u7684\u5b66\u4e60\u4efb\u52a1 \\(T_g, T_t\\)</p> </li> <li> <p>Test Set: \\(D_q \\subseteq D_t\\), unlabelled</p> </li> <li> <p>Target\uff1a\u5728 \\(D_g\uff0c D_t\\) \u4e0a\u8c03\u53c2\uff0c\u4ee5\u627e\u5230\u4e00\u4e2a\u53ef\u7528\u4e8e \\(D_q\\) \u7684 transferable spatiotemporal feature extractor </p> </li> </ul>"},{"location":"sum/Self-Supervised/#domain-general-pretraining","title":"Domain-general pretraining","text":"<p>\u8fd9\u4e00\u90e8\u5206\u76f4\u63a5\u4f7f\u7528\u4e86 pretrained backbone model \u2014\u2014 \u5728 K400 \u4e0a\u7ecf\u8fc7\u76d1\u7763\u5b66\u4e60\u5f97\u5230\u7684 I3D \u6a21\u578b</p>"},{"location":"sum/Self-Supervised/#in-domain-ssl-continual-pretraining","title":"In-domain SSL continual pretraining","text":"<ul> <li> <p>\u672c\u6587\u63d0\u51fa\u7684 3D-Adapter \u548c Transformer &amp; 2D CNN \u4e2d\u7684\u76f8\u5e94\u6a21\u5757\u5177\u6709\u7c7b\u4f3c\u7684\u7ed3\u6784\uff0c\u5728\u6b64\u57fa\u7840\u4e0a\u6dfb\u52a0\u4e86 3D Layers \u4ee5\u5b9e\u73b0\u89c6\u9891\u6570\u636e\u7684 3D CNN \u64cd\u4f5c</p> </li> <li> <p>\u5728\u6bcf\u4e00\u4e2a Inception Module \u7684 concatenation Layer \u540e\u63d2\u5165\u4e00\u4e2a 3D-Adapter \u53ef\u4ee5\u5f97\u5230\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347</p> <p>\u4e8b\u5b9e\u4e0a R3D + 3D-Adapter \u4e5f\u80fd\u63d0\u5347\u6027\u80fd</p> </li> </ul> <p> \u6dfb\u52a0 3D-Adapter \u540e\u7684 Inception Module</p> <ul> <li> <p>3D-Adapter modules \u5c06\u88ab \u968f\u673a\u521d\u59cb\u5316 </p> </li> <li> <p>\u6bcf\u4e00\u4e2a 3D-Apater \u4f7f\u7528\u4e86\uff1a</p> <ul> <li>\u53ef\u5b66\u4e60\u53c2\u6570 \\(\\theta_{down}\\) \u2014\u2014 downsampling \u00b7 depth-wise \u00b7 3D convolution</li> <li>\u975e\u7ebf\u6027\u6fc0\u6d3b\u51fd\u6570 \\(f(.)\\)\uff0c\u5982 ReLU</li> <li>\u53ef\u5b66\u4e60\u53c2\u6570 \\(\\theta_{up}\\) \u2014\u2014 upsampling \u00b7 point-wise \u00b7 3D convolustion</li> </ul> </li> <li> <p>\u76f8\u5173\u53c2\u6570\u5982\u4e0b\uff1a</p> <ul> <li>\\(C_{in}, C_{out}\\)\uff1a \u8f93\u5165/\u8f93\u51fa \u7684 channel dimensions</li> <li>compression factor \\(\\lambda\\)\uff1a bottleneck dimension</li> </ul> </li> </ul> <p>\u56e0\u6b64\uff0c\u5bf9\u4e8e\u7ed9\u5b9a\u7684 input feture vecor \\(h_{in} \\in \\mathbb{R}^{C_{in} \\times D \\times H \\times W}\\)\uff0c3D-Adapter \u5c06\u8f93\u51fa \\(h_{out} \\in \\mathbb{R}^{C_{out} \\times D \\times H \\times W}\\)</p> <p>\u5177\u4f53\u53d8\u6362\u8fc7\u7a0b\u53ef\u63cf\u8ff0\u4e3a\uff1a</p> \\[ h_{out} = \\alpha \\odot (\\theta_{up} \\otimes f(\\theta_{down} \\overline{\\otimes} h_{in})) + h_{in} \\] <p>\u5176\u4e2d\uff1a</p> <ul> <li>\\(\\otimes, \\overline{\\otimes}\\) \u5206\u522b\u8868\u793a depth-wise / point-wise \u5377\u79ef\u64cd\u4f5c</li> <li>\\(\\alpha \\in \\mathbb{R}^{C_{out}}\\) \u662f\u53ef\u8c03\u8d85\u53c2\u6570\uff0c\u521d\u59cb\u5316\u4e3a ones</li> <li>\\(\\odot\\) \u8868\u793a element-wise \u4e58\u6cd5</li> </ul> <p>Continual Pretraining \u9636\u6bb5\uff1a</p> <p>\u53ea\u6709 3D-Adapter \u4e2d\u7684\u53c2\u6570\u4f1a\u88ab\u66f4\u65b0</p> <ul> <li> <p>\u5728\u96c6\u5408 \\(D_q\\) \u4e2d\u4ee5 SSL \u5f62\u5f0f\u8fdb\u884c \u2014\u2014 \u89c6\u9891\u6807\u7b7e\u7531\u6a21\u578b\u81ea\u52a8\u751f\u6210</p> </li> <li> <p>\u4f7f\u7528\u4e86 Video Segment Pace Prediction (VSPP) \u8fdb\u884c\u4e86\u9884\u5904\u7406\uff0c\u4ece\u800c\u5bf9 \u201c\u4ee5\u4e0d\u540c\u901f\u5ea6\u5b8c\u6210\u7684\u52a8\u4f5c\u201d \u8fdb\u884c\u5bf9\u6bd4\uff0c\u751f\u6210\u89c6 SSL \u6807\u7b7e\u4fe1\u606f\uff1a</p> <ul> <li> <p>speed rate \\(\\lambda_i\\)</p> </li> <li> <p>Segment No. \\(\\zeta_i\\)</p> </li> </ul> </li> </ul> <p>\u4e00\u4e9b\u62c9\u8e29\u4fe1\u606f</p> <ol> <li> <p>\u5173\u4e8e SSL Pretext \u7684\u9009\u53d6</p> <p>\u4f5c\u8005\u4e5f\u5c1d\u8bd5\u4f7f\u7528\u4e86 Constrastive-Based \u7684 RSPNet &amp; \u540c\u6837\u662f Transformatoin-Based \u7684 VideoPace\uff0c\u4f46\u8868\u73b0\u90fd\u4e0d\u53ca VSPP</p> </li> <li> <p>\u5173\u4e8e BatchNorm Tuning\uff08\u5bf9\u6bd4 Adapter\uff09</p> <p>\u5728\u5c0f\u578b\u6570\u636e\u5e93\u4e0a\u5bb9\u6613\u8fc7\u62df\u5408</p> </li> </ol>"},{"location":"sum/Self-Supervised/#supervised-fine-tuning","title":"Supervised fine-tuning","text":"<p>\u4e3a\u5565\u8fd8\u8981 Fine-Tuning (?)</p> <p>\u672c\u6587 3D-Adapter \u53ea\u5bf9 I3D \u6a21\u5757\u8fdb\u884c Continual Pretraining\uff0c\u6574\u4e2a\u6a21\u578b\u7684\u53c2\u6570\u8fd8\u662f\u9700\u8981\u5fae\u8c03</p> <p>\u5728\u8fd9\u4e2a\u9636\u6bb5\uff0c\u6240\u6709\u53c2\u6570\uff08\u9884\u8bad\u7ec3\u53c2\u6570 &amp; 3D-Adapter\uff09 \u5c06\u5728 \\(D_t\\) \u4e0a\u88ab\u5fae\u8c03</p> <ul> <li> <p>USDL / MUSDL</p> <ul> <li> <p>\u7279\u5f81 -&gt; I3D backbone -&gt; temporal Pooling -&gt; SoftMax -&gt; \u9884\u6d4b\u5206\u5e03</p> </li> <li> <p>\u4f7f\u7528 \u9884\u6d4b\u5206\u5e03 \u4e0e \u7531 ground-truth \u4ea7\u751f\u7684\u9ad8\u65af\u5206\u5e03 \u4e4b\u95f4\u7684 KL loss \u8fdb\u884c\u4f18\u5316</p> </li> </ul> <p>MUSDL \u662f USDL \u7684 multi-path \u7248\u672c\uff0c\u9002\u7528\u4e8e MTL-AQA / JIGSAWS \u8fd9\u79cd\u7531\u591a\u4e2a\u88c1\u5224\u540c\u65f6\u6253\u5206\u7684\u6570\u636e\u96c6</p> </li> <li> <p>CoRe</p> <ul> <li> <p>\u4f7f\u7528\u6dfb\u52a0 3D-Adapter \u7684 I3D \u6a21\u578b\u8fdb\u884c\u7279\u5f81\u63d0\u53d6\uff0c\u968f\u540e\u4e22\u56de GART \u8fdb\u884c\u56de\u5f52</p> </li> <li> <p>\u6700\u7ec8\u7ed3\u679c\u53d6\u591a\u4e2a exampler \u7684\u5e73\u5747</p> </li> </ul> </li> <li> <p>TSA</p> <p>\u4f7f\u7528\u6dfb\u52a0 3D-Adapter \u7684 I3D \u6a21\u578b\u8fdb\u884c\u7279\u5f81\u63d0\u53d6\uff0c\u968f\u540e\u4e22\u56de Attention Module \u8fdb\u884c\u540e\u7eed\u64cd\u4f5c</p> </li> </ul>"},{"location":"sum/Transformer/","title":"Transformer","text":""},{"location":"sum/Transformer/#2021-sgn","title":"2021: SGN","text":"<p>Semantics-Guided Representations (for Figure Skating)</p>"},{"location":"sum/Transformer/#1-introduction","title":"1 Introduction","text":"<ul> <li> <p>\u5148\u524d\u7684\u5de5\u4f5c</p> <ul> <li> <p>\u5927\u591a\u6570\u5de5\u4f5c\u5c40\u9650\u4e8e\u6839\u636e visual input \u9884\u6d4b\u5f97\u5206\uff0c\u9650\u5236\u4e86 depict highlevel semantic representation \u7684\u80fd\u529b</p> <p>\u4ed6\u4eec\u4fa7\u91cd\u4e8e\u5229\u7528 \u89c6\u89c9\u4e0a\u4e0b\u6587\u4fe1\u606f\uff0c\u4f46\u672a\u63a2\u7d22\u89c6\u9891\u4e2d\u7684\u8bed\u4e49\u4fe1\u606f</p> </li> <li> <p>\u5982\u4f55\u901a\u8fc7\u8ba1\u7b97\u673a\u89c6\u89c9\u5bf9 semantic reprsentation \u8fdb\u884c depicts and interacts \u4ecd\u672a\u660e\u786e</p> </li> </ul> </li> <li> <p>\u672c\u6587\u5de5\u4f5c</p> <ul> <li>\u805a\u7126\u4e8e\u63d0\u53d6 Figure-Skating \u9879\u76ee\u4e2d\u7684 semantic-aware representations</li> <li>\u63d0\u51fa\u4e86 Semantic-Guided Network (SGN) \u4ee5\u5f25\u5408 \u8bed\u4e49-\u89c6\u89c9\u4fe1\u606f \u4e4b\u95f4\u7684\u5dee\u8ddd<ul> <li>\u662f\u4e00\u4e2a teacher-student-based network (with attention mechanism)</li> <li>\u4f7f\u5f97\u77e5\u8bc6\u4ece semantic domain \u8fc1\u79fb\u5230 visual domain</li> </ul> </li> <li> <p>\u5728 teacher branch \u901a\u8fc7 cross-attention \u673a\u5236\u805a\u5408 semantic descriptions &amp; visual features\uff0c\u4e3a visual domain \u63d0\u4f9b\u76d1\u7763</p> <p>\u9664 OlympicFS \u63d0\u4f9b\u7684 comments \u5916\uff0cteacher branch \u4e2d\u7684\u8bed\u4e49\u8868\u793a\u4e5f\u53ef\u4ee5\u4ece\u97f3\u4e50\u7b49\u6a21\u6001\u4e2d\u63d0\u53d6</p> </li> <li> <p>\u5728 student branch \u4f7f\u7528\u4e86\u4e00\u4e32 learnable atomic queries \u6765\u6a21\u62df teacher branch \u7684 semantic-aware distribution</p> <p>\u65cb\u8f6c\u3001\u8df3\u8dc3\u7b49\u7ec6\u7c92\u5ea6 query \u4f7f\u5f97\u6a21\u578b\u80fd\u591f\u8bc6\u522b\u89c6\u9891\u4e2d\u7684\u5173\u952e\u539f\u5b50\u52a8\u4f5c\uff0c\u5176\u8bad\u7ec3\u53d7 semantic domain \u4e2d\u7684 teacher \u76d1\u7763</p> </li> <li> <p>\u63d0\u51fa\u4e86\u4e09\u79cd Loss \u6765 align \u6765\u81ea\u4e0d\u540c domain \u7684 features</p> </li> <li>\u63d0\u51fa\u4e86\u591a\u6a21\u6001\u6570\u636e\u96c6 OlympicFS\uff0c\u63d0\u4f9b\u4e86 score + professional comments\u3002\u6570\u636e\u6765\u81ea 2018\u5e74\u5e73\u660c\u51ac\u5965\u4f1a \u548c 2022\u5e74\u5317\u4eac\u51ac\u5965\u4f1a\u3002</li> <li>benchmarks: OlympicFS, FS1000, Fis-v, MTL-AQA(diving)</li> </ul> </li> </ul>"},{"location":"sum/Transformer/#2-related-works","title":"2 Related Works","text":""},{"location":"sum/Transformer/#figure-skating-analysis","title":"Figure Skating Analysis","text":"<ul> <li> <p>CV \u9886\u57df\u5173\u4e8e FS \u7684\u6700\u65e9\u7814\u7a76\u662f Pirsiavash \u5728 14 \u5e74\u57fa\u4e8e ST-Pose feature \u8bad\u7ec3\u7684\u56de\u5f52\u6a21\u578b</p> </li> <li> <p>Xu \u4ece\u5973\u5b50\u5355\u4eba\u9879\u76ee\u4e2d\u6536\u96c6\u4e86 500 \u4e2a\u89c6\u9891\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u5305\u542b self-attentive &amp; multi-scale LSTM \u7684\u673a\u5236\uff0c\u7528\u4e8e\u5b66\u4e60 local &amp; global SeqInfo</p> </li> <li> <p>\u6570\u636e\u96c6 FSD-10 \u5305\u542b\u4e86 \u7537\u5b50/\u5973\u5b50 \u9879\u76ee\u4e2d\u7684 10 \u79cd\u4e0d\u540c\u52a8\u4f5c\uff0c\u5e76\u63d0\u51fa\u4e86\u7528\u4e8e \u5206\u7c7b \u7684 key-frame-based temporal segment network</p> </li> <li> <p>ACTION-Net \u5b66\u4e60\u4e86\u5728\u7279\u5b9a\u5e27\u4e2d\u68c0\u6d4b\u5230\u7684\u8fd0\u52a8\u5458\u7684\u89c6\u9891\u52a8\u6001\u4fe1\u606f\u548c\u9759\u6001\u59ff\u52bf\uff0c\u4ee5\u52a0\u5f3a\u89c6\u9891\u4e2d\u7684\u7279\u5b9a\u59ff\u52bf\u3002</p> </li> <li> <p>EAGLE-Eye \u6784\u5efa\u4e86\u4e00\u4e2a\u53cc\u6d41\u7f51\u7edc\uff0c\u7528\u4e8e\u63a8\u7406\u8868\u6f14\u8fc7\u7a0b\u4e2d\u5173\u8282\u534f\u8c03\u548c\u5916\u89c2\u52a8\u6001\u4e4b\u95f4\u7684\u5173\u7cfb\u3002</p> </li> <li> <p>Xia \u63d0\u51fa\u591a\u6a21\u6001\u6a21\u578b MLP-Mixer\uff08\u97f3\u9891 + \u89c6\u89c9\u4fe1\u606f\uff09\uff0c\u5e76\u901a\u8fc7 memory recurrent unit \u6709\u6548\u5730\u5b66\u4e60\u957f\u671f\u8868\u793a\uff1b\u540c\u65f6\u6536\u96c6\u4e86 FS1000 \u6570\u636e\u96c6\u3002</p> </li> </ul>"},{"location":"sum/Transformer/#multimodal-learning","title":"Multimodal Learning","text":"<p>\u6700\u8fd1\uff0cTransformer \u6a21\u578b\u4e0d\u4ec5\u5728 NLP \u4e2d\u5f97\u5230\u4e86\u5e7f\u6cdb\u5e94\u7528\uff0c\u540c\u65f6\u5728 CV \u4efb\u52a1\u91cd\u8868\u73b0\u51fa\u4e86\u826f\u597d\u7684\u6027\u80fd\u3002</p> <p>\u867d\u7136\u8fd9\u4e9b\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u5927\u89c4\u6a21\u6570\u636e\u96c6\uff0c\u5e76\u91c7\u7528\u591a\u6a21\u6001\u81ea\u76d1\u7763\u4efb\u52a1\u8fdb\u884c\u9884\u8bad\u7ec3\u3002</p> <p>\u4f46 AQA \u9886\u57df\u5c1a\u672a\u63d0\u51fa\u591a\u6a21\u6001\u5927\u89c4\u6a21\u6a21\u578b</p>"},{"location":"sum/Transformer/#3-approach","title":"3 Approach","text":""},{"location":"sum/Transformer/#1-feature-extraction","title":"1) Feature Extraction","text":"<ul> <li> <p>\u5bf9\u4e8e\u89c6\u9891\u8f93\u5165\uff1a</p> <ol> <li>\u4f7f\u7528 Temporal segment networks \u5c06\u8f93\u5165\u89c6\u9891\u5212\u5206\u4e3a \\(T_v\\) \u4e2a segments</li> <li>\u4f7f\u7528 Video swin transformer \u4ece segment \u4e2d\u63d0\u53d6 visual feature</li> <li>\u4f7f\u7528 MLP \u8fdb\u884c\u7279\u5f81\u964d\u7ef4\uff0c\u5f97\u5230 \\(X_v \\in \\mathbb{R}^{T_v \\times D}\\)</li> </ol> </li> <li> <p>\u5bf9\u4e8e\u6587\u5b57\u8f93\u5165\uff1a\u4f7f\u7528 token embedder + BERT \u63d0\u53d6\u7279\u5f81 \\(X_t \\in \\mathbb{R}^{T_t \\times D}\\)</p> </li> </ul>"},{"location":"sum/Transformer/#2-extract-semantic-aware-representations","title":"2) Extract Semantic-Aware Representations","text":"<p>\u6bcf\u4e2a clip \u4ec5\u5305\u542b\u5f53\u524d segment \u7684\u4fe1\u606f\uff0c\u7f3a\u5c11 global context info</p> <ol> <li> <p>\u4f7f\u7528 self-attention Encoder \u6765\u5145\u5b9e segment-wise representation</p> <ul> <li> <p>\u901a\u8fc7\u8ba1\u7b97 weighted aggregation of segment features \u6765\u5f97\u5230 context info</p> <p>\u6b64\u5904\u7684 weight \u7531\u5f53\u524d segment \u4e0e\u5176\u4ed6 segments \u4e4b\u95f4\u7684\u534f\u65b9\u5dee\u51b3\u5b9a</p> </li> </ul> \\[ H_0 = \\text{SoftMax}\\left(     \\frac{W_{qs} X_v (W_{ks} X_v)^T}{\\sqrt{D}} \\right) W_{vs} X_v + X_v \\] <ul> <li>\\(W_{qs}, W_{ks}, W_{vs}\\) \u90fd\u662f\u53ef\u8bad\u7ec3\u53c2\u6570</li> </ul> </li> <li> <p>\u4f7f\u7528 feed-forward network (FFN) \u5bf9 \\(H_0\\) \u8fdb\u884c\u8fdb\u4e00\u6b65 fusion\uff0c\u5f97\u5230 \\(\\hat{X}_v\\)</p> </li> </ol>"},{"location":"sum/Transformer/#teacher-branch","title":"Teacher Branch","text":"<p>\u4ece\u8bc4\u8bba\u6587\u672c\u4e2d\u63d0\u53d6 semantic-aware representations</p> <ul> <li> <p>\u901a\u8fc7\u6784\u5efa visual-text feature \u4e4b\u95f4\u7684 cross-attention \u6765\u5b66\u4e60 semantic corr.</p> </li> <li> <p>(\u53d7 DETR \u542f\u53d1) \u672c\u6587\u7684 Transformer Decoder \u5171\u5305\u542b\u4e09\u90e8\u5206: self-attention, croess-atention, FFN</p> <ul> <li>self-attention \u7528\u4e8e\u6316\u6398 text feature \u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u5f97\u5230 \\(\\hat{X}_t\\)</li> <li> <p>cross-attention \u7528\u4e8e\u4ece \\(\\hat{X}_t, \\hat{X}_v\\) \u4e2d\u5b66\u4e60 context-aware representations</p> <ul> <li>\\(query\\) \u901a\u8fc7 \\(\\hat{X}_t\\) \u751f\u6210\uff0c\\(key, value\\) \u4ece \\(\\hat{X}_v\\) \u53d8\u6362\u5f97\u5230</li> </ul> \\[ Q_t=W_q\\hat{X}_t,\\ K_v = W_k\\hat{X}_v,\\ V_v = W_v\\hat{X}_v \\] <ul> <li>semantic corr \\(A^T\\) \u7531\u5bf9\u5e94 \\(query-key\\) \u4e4b\u95f4\u7684 dot-product similarity \u8861\u91cf</li> </ul> \\[ A^T = \\text{SoftMax}\\left(\\frac{Q_tK_v^T}{\\sqrt{D}}\\right) \\] </li> <li> <p>FFN \u7528\u4e8e aggregate \\(A^T, V_v\\)\uff0c\u5f97\u5230\u6700\u7ec8\u8f93\u51fa \\(H^T\\)</p> \\[ H^T = FFN(A^T V_v) \\in \\mathbb{R}^{T_t \\times D} \\] </li> </ul> </li> </ul>"},{"location":"sum/Transformer/#3-semantics-guided-network","title":"3) Semantics-Guided Network","text":"<ul> <li>\u5148\u524d\u7684\u591a\u6a21\u6001\u6a21\u578b\u4f1a\u628a visual-semantic info \u6df7\u5408\u5728\u4e00\u8d77\u8fdb\u884c\u9884\u6d4b</li> <li>\u672c\u6587\u5219\u662f\u901a\u8fc7 semanticRep \u6307\u5bfc\u5bf9 visual feature \u7684\u5b66\u4e60</li> </ul>"},{"location":"sum/Transformer/#student-branch","title":"Student Branch","text":"<ul> <li> <p>\u5b9a\u4e49\u957f\u5ea6\u4e3a \\(K\\) \u7684 atomic queries \\(X_q \\in \\mathbb{R}^{K \\times D}\\)</p> <p>\u8fd9\u4e9b query \u7528\u4e8e\u8868\u793a\u8bc4\u5206\u4e2d\u7684\u5173\u952e\u8bed\u4e49\u4fe1\u606f\uff0c\u4f8b\u5982\u51fa\u8272\u7684\u8df3\u8dc3\u6216\u7cdf\u7cd5\u7684\u6454\u5012\u3002</p> <p>\u5728\u5e94\u7528 self-attention \u540e\u66f4\u65b0\u4e3a \\(\\hat{X}_q\\)</p> </li> <li> <p>\u7c7b\u4f3c\u4e8e teacher branch\uff0cstudent branch \u4e5f\u7531\u4e09\u90e8\u5206\u6784\u6210\uff08\u4e0d\u8fc7\u7528 queries \u66ff\u4ee3\u4e86 \\(\\hat{X}_t\\)\uff09</p> \\[ \\begin{align*} Q'_q &amp;= W'_q \\hat{X}_q,\\ K'_v = W'_k\\hat{X}_v,\\ V'_v = W'_v \\hat{X}_v \\\\ A^S &amp;= \\text{SoftMax}\\left(\\frac{Q'_q K'^T_v}{\\sqrt{D}}\\right) \\in \\mathbb{R}^{K \\times T_v}\\\\ H^S &amp;= FFN(A^S V'_v) \\in \\mathbb{R}^{K \\times D} \\end{align*} \\] <p>\\(A^S\\) \u662f attention map\uff0c\\(H^S\\) \u662f student branch \u7684\u8f93\u51fa</p> </li> </ul>"},{"location":"sum/Transformer/#evaluation","title":"Evaluation","text":"<ul> <li> <p>distillation loss</p> <ul> <li>\u76ee\u6807\uff1a\u6700\u5c0f\u5316 \\(A^T, A^S\\) \u4e24\u4e2a self-attention matrices \u4e4b\u95f4\u7684\u5dee\u5f02</li> <li> <p>\u4f46\u662f \\(A^T \\in T_t \\times T_v,\\ A^S \\in K \\times T_v\\) \u5177\u6709\u4e0d\u540c\u7684 shape (\\(T_t \\gg K\\))\uff0c\u56e0\u6b64\u9700\u8981\u6cbf \\(T_t/K\\) \u7eac\u5ea6\u8fdb\u884c MaxPooling\uff0c\u751f\u6210 \\(\\hat{A}^T / \\hat{A}^S\\)</p> <p>\u8fd9\u6837\u53ef\u4ee5\u63d0\u53d6\u6700\u663e\u8457\u7684\u7279\u5f81\u5e76\u589e\u5f3a\u7279\u5f81\u8868\u793a\u80fd\u529b\uff0c\u800c\u4e0d\u4f1a\u4e22\u5931\u6709\u7528\u7684\u4fe1\u606f</p> </li> </ul> \\[ \\mathcal{L}_{attn} = \\frac{1}{T_v h} \\sum_{i=1}^{T_v} \\sum_{j=1}^h \\text{MSE}(\\hat{A}_{ij}^T, \\hat{A}_{ij}^S) \\] <p>\\(h\\) \u4e3a Transformer \u4e2d\u7684 attention heads\uff0c\\(\\hat{A}_{ij}\\) \u662f i<sup>th</sup>-clip j<sup>th</sup>-head \u7684 normalized attention</p> </li> <li> <p>objective loss</p> <p>\u9664\u4e86\u9650\u5236 teacher-student branch \u7684 attention distribution \u4e4b\u5916\uff0c\u4f5c\u8005\u8fd8\u4f7f\u7528\u4e86 Noise Contrastive Estimation(NCE) loss \u6765 align teacher-student branch \u4e4b\u95f4\u7684 output feature</p> <ul> <li> <p>\u901a\u8fc7\u5c06 target instance \\(H^S\\) \u4e0e\u66f4\u591a\u7684 negative sample \u8fdb\u884c\u5bf9\u6bd4\uff0c\u5e76\u4e0e\u5bf9\u5e94\u7684 positive sample\\(H^T\\) \u8fdb\u884c align</p> </li> <li> <p>\u5728 \\(T_t/K\\) \u7eac\u5ea6\u4e0a\u4f7f\u7528 AveragePooling \u6765\u5c06 teacher-student feature \u6620\u5c04\u5230\u76f8\u540c\u7684\u7eac\u5ea6\u4e0a</p> </li> </ul> \\[ \\mathcal{L}_{cons} = -\\log{\\frac{\\exp{(\\text{sim}(\\hat{H^S_i},\\hat{H^T_i})/\\tau)}} {\\sum_{j=1}^N\\mathbb{1}_{[j \\neq i]}\\exp{(\\text{sim}(\\hat{H^S_i},\\hat{H^T_i})/\\tau)}}} \\] <ul> <li>\\(\\mathbb{1}_{[j \\neq i]}\\) \u662f\u4e00\u4e2a indicatorFunc\uff0c\u5f53 \\(j\\neq i\\) \u65f6\u53d6 1</li> <li>\\(\\text{sim}(u,v) = u^T v / |u||v|\\) \u8868\u793a \\(l_2\\) normalized \u7684 \\(u\u00b7v\\) \u70b9\u79ef</li> <li>\\(\\tau\\) \u662f temperature \u8d85\u53c2\u6570</li> <li>\u8fd9\u4e00 loss \u5728 mini-batch \\(N\\) \u4e2d\u7684\u6240\u6709 positive-pairs \u95f4\u88ab\u8ba1\u7b97</li> </ul> </li> <li> <p>scoring loss</p> <ul> <li>\u672c\u6587\u5c06 AQA \u4efb\u52a1\u5b9a\u4e49\u4e3a\uff1a\u9884\u6d4b\u548c\u5177\u6709\u76f8\u540c category \u89c6\u9891\u95f4\u7684 \\(\\Delta s\\)</li> <li> <p>\u5bf9\u4e8e\u8f93\u5165\u5bf9 \\(&lt;X_{v.p}, X_{v,q}&gt;\\)\uff0c\u5176 ground-truth \u4e3a \\(&lt;S_p, S_q&gt;\\)\uff0c\u6709\uff1a</p> \\[ L_{score} = (\\Delta S - |S_p - S_q|)^2 = (\\mathcal{R}_{\\Theta}(\\hat{H_p},\\hat{H_q}) - |S_p - S_q|)^2 \\] </li> <li> <p>\u7531\u4e8e\u6a21\u578b\u5177\u6709 teacher-student \u4e24\u4e2a branch\uff0c\u56e0\u6b64\u4f1a\u6709\u4e24\u4e2a relative score\uff0c\u5206\u522b\u8bb0\u4e3a \\(\\mathcal{L}_{score}^T,\\mathcal{L}_{score}^S\\)</p> </li> </ul> </li> <li> <p>consistency loss</p> <ul> <li>to align the learned feature representations: \u9650\u5236 teacher branch \u9884\u6d4b\u7684 \\(\\Delta S^T\\) \u4e0e student brach \u9884\u6d4b\u7684 \\(\\Delta S^S\\) \u76f8\u7b49</li> </ul> \\[ \\mathcal{L}_{c-score} = (\\Delta S^T - \\Delta S^S)^2 \\] </li> </ul> <p>\u5c06\u4ee5\u4e0a\u5404\u9879\u76f8\u52a0\uff0c\u53ef\u5f97 total loss \\(\\mathcal{L}\\)</p> \\[ \\mathcal{L} = \\mathcal{L}_{score}^T + \\mathcal{L}_{score}^S + \\mathcal{L}_{attn} + \\mathcal{L}_{cons} + \\mathcal{L}_{c-score} \\]"},{"location":"sum/Transformer/#2023-fspn","title":"2023: FSPN","text":""},{"location":"sum/Transformer/#1-abstract","title":"1 Abstract","text":"<ul> <li> <p>\u5148\u524d\u7684\u5de5\u4f5c</p> <ul> <li> <p>\u5927\u591a\u6570\u57fa\u4e8e \u7c97\u7c92\u5ea6(coarse-grained)\u7279\u5f81 \u8fdb\u884c\u8bad\u7ec3\u3001\u91c7\u7528 holistic video representations\uff0c\u7f3a\u4e4f\u5bf9 fine-grained intra-class variations \u7684\u6355\u6349</p> </li> <li> <p>Parmar and Morris \u8ba4\u4e3a\u6240\u6709\u7684 sub-action sequences \u5bf9\u7ed3\u679c\u5177\u6709 \u76f8\u7b49\u7684\u8d21\u732e</p> </li> <li> <p>segmenting action sequences along with their temporal dependence remains a challenging task\uff1a</p> <ol> <li> <p>\u7f3a\u5c11\u9884\u5b9a\u4e49\u7684 \u6807\u7b7e &amp; action sequence \u4e4b\u95f4\u7684\u5173\u8054\u6027</p> </li> <li> <p>sub-action sequences \u5177\u6709\u975e\u5e38\u7ec6\u7684\u7c92\u5ea6\uff0c\u52a8\u4f5c\u95f4\u7684\u53d8\u5316\u5341\u5206\u5e73\u6ed1 =&gt; \u96be\u4ee5\u786e\u5b9a\u5176\u8fb9\u754c</p> </li> <li> <p>\u7531\u4e8e\u52a8\u4f5c\u5341\u5206\u7ec6\u5bc6\u3001\u5728\u76f8\u4f3c\u7684\u80cc\u666f\u4e2d\u8fdb\u884c\uff0c\u5404 sub-action \u4e4b\u95f4\u6709\u8f83\u591a\u7684\u5171\u540c attributes</p> </li> </ol> </li> </ul> </li> <li> <p>\u521b\u65b0\u70b9</p> <p>\u63d0\u53d6 fine-grained sub-action sequence \u548c\u5b83\u4eec\u7684 temporal dependencies \u6709\u52a9\u4e8e\u505a\u51fa\u66f4\u51c6\u786e\u7684\u4f30\u8ba1</p> <p>\u4e3a\u4e86\u964d\u4f4e\u80cc\u666f\u7684\u5e72\u6270\uff0c\u672c\u6587\u4f7f\u7528\u9884\u8bad\u7ec3\u6a21\u578b\u4ece input video \u4e2d\u63d0\u53d6\u4e86 actor-centric regions </p> <ol> <li> <p>\u63d0\u51fa\u4e86\u7531\u4e24\u90e8\u5206\u7ec4\u6210\u7684 FSPN\uff1a</p> <ul> <li> <p>intra-sequence action parsing module \u65e0\u76d1\u7763</p> <p>\u5bf9\u66f4\u7ec6\u7c92\u5ea6\u4e0b\u7684 sub-actions \u8fdb\u884c\u6316\u6398</p> <p>\u5b9e\u73b0 semantical sub-action parsing\uff0c\u4ece\u800c\u66f4\u51c6\u786e\u7684\u63cf\u8ff0\u52a8\u4f5c\u5e8f\u5217\u95f4\u7684\u7ec6\u5fae\u5dee\u522b</p> </li> <li> <p>spatiotemporal multiscale transformer module</p> <p>\u4f4e\u9636\u7279\u5f81\u7f3a\u4e4f\u8bed\u4e49\u4fe1\u606f\uff0c\u9ad8\u9636\u7279\u5f81\u96be\u4ee5\u5bf9 sub-action \u8fdb\u884c\u7ec6\u7c92\u5ea6\u63cf\u8ff0</p> <p>\u5b66\u4e60 motion-oriented action features\u3001\u6316\u6398\u5176\u5728\u4e0d\u540c\u65f6\u95f4\u8303\u56f4\u5185\u7684 long-range \u4f9d\u8d56\u5173\u7cfb</p> </li> </ul> </li> <li> <p>\u63d0\u51fa\u4e86\u4e00\u4e2a group contrastive loss</p> </li> </ol> <p>\u6b64\u5916\uff0c\u7531\u4e8e\u6574\u4e2a\u52a8\u4f5c\u5e8f\u5217\u53ef\u80fd\u5b58\u5728\u7ec4\u4ef6\u91cd\u590d ABBBBCC\uff0c\u6a21\u578b\u4f7f\u7528\u4e86 1D Temporal Convolution + Transformer Network \u6765\u63d0\u53d6 single-scale feature</p> <p>\u6700\u7ec8\uff0c\u5404\u9636\u6bb5\u7279\u5f81\u4f1a\u901a\u8fc7 multiscale temporal fusion \u805a\u5408\u751f\u6210 unified feature represen- tation\uff0c\u5e76\u7528\u4e8e\u6700\u7ec8\u7684\u9884\u6d4b</p> </li> </ul>"},{"location":"sum/Transformer/#2-relative-works","title":"2 Relative Works","text":"<ol> <li> <p>AQA</p> <ul> <li> <p>Regression Formulation</p> </li> <li> <p>Pairwise Ranking Formulation</p> </li> </ul> </li> <li> <p>(Fine-grained) Action Parsing </p> <ul> <li> <p>Zhang: Temporal Query Networks =&gt; \u901a\u8fc7 query \u627e\u51fa\u76f8\u5173\u7684 segments</p> </li> <li> <p>Dian: TransParser =&gt;  \u5bf9 sub-action \u8fdb\u884c\u6316\u6398\uff08\u65e0\u76d1\u7763\uff09</p> </li> </ul> </li> <li> <p>Vision Transformer</p> <p>\u4ece \u4f4e\u5206\u8fa8\u7387\u56fe\u7247 &amp; \u8f83\u5c0f\u7684\u901a\u9053\u6570\u91cf \u5f00\u59cb\uff0c\u9010\u6e10\u589e\u52a0\u901a\u9053\u5e76\u51cf\u5c11 spatial resolution</p> </li> </ol>"},{"location":"sum/Transformer/#3-approach_1","title":"3 Approach","text":""},{"location":"sum/Transformer/#_1","title":"\u95ee\u9898\u5b9a\u4e49","text":"<p>\u5bf9\u4e8e\u7ed9\u5b9a input video \\(x_i \\in \\mathbb{R}^{T \\times H \\times W \\times C}\\) \u53ca\u5176\u5bf9\u5e94\u7684\u5206\u6570\u6807\u7b7e \\(y_i\\)\uff0cAQA \u95ee\u9898\u53ef\u4ee5\u8ba4\u4e3a\u662f\u4e00\u4e2a\u56de\u5f52\u95ee\u9898\uff1a</p> <p>T, H, W, C \u5206\u522b\u4e3a clip \u957f\u5ea6\u3001\u89c6\u9891\u5bbd\u9ad8\u3001\u901a\u9053\u6570</p> <ol> <li> <p>\u4f7f\u7528\u9884\u8bad\u7ec3\u6a21\u578b \\(D(.)\\) \u5f97\u5230\u8fd0\u52a8\u5458\u6240\u5728\u7684 BBox \\(x_a\\)</p> \\[     x_a = D(x_i) \\in \\mathbb{R}^{T \\times H \\times W \\times C} \\] </li> <li> <p>\u5c06 \u539f\u59cb\u8f93\u5165 \u548c BBox \u90fd\u8f93\u5165 (\u76f8\u540c\u7684)I3D \u6765\u63d0\u53d6 spatiotemporal \u7279\u5f81 \\((f_i, f_a)\\)</p> \\[ f_i = E_v(x_i),\\ f_a = E_v(x_a) \\] </li> <li> <p>\u4f7f\u7528 FSPN \\(\\mathbb{F}_\\Theta(.)\\) \u63d0\u53d6\u7279\u5f81\uff0c\u5e76\u6700\u7ec8\u8fdb\u884c\u56de\u5f52\u8fd0\u7b97</p> \\[ \\overline{y}_i = R_{\\theta}(\\mathbb{F}_\\Theta(E_v(x_i)),\\mathbb{F}_\\Theta(E_v(x_a))) \\] </li> </ol>"},{"location":"sum/Transformer/#intra-sequence-action-parsing","title":"Intra-Sequence Action Parsing","text":""},{"location":"sum/Transformer/#1-intra-sequence-action-parsing-iap","title":"1 Intra-Sequence Action Parsing (IAP)","text":"<p>\u786e\u5b9a\u6bcf\u4e2a sub-action \u7684 \u8d77\u59cb\u5e27 &amp; \u7ed3\u675f\u5e27</p> <ul> <li> <p>\u7ed9\u51fa\u7684 Parser \u53ef\u4ee5\u5bf9 \\(S\\) \u4e2a sub-action \u7684\u5206\u5e03\u6982\u7387\u8fdb\u884c\u9884\u6d4b\uff0c\u540c\u65f6\u6307\u51fa \u201c\u8f6c\u53d8\u201d \u53d1\u751f\u7684\u5177\u4f53\u5e27\u7f16\u53f7 \\(f^{th}\\)\uff1a</p> <p>features -&gt; probability vec \\(A_s\\) (\u5bf9 \\(s^{th}\\) sub-action \u7684 middle-level \u8868\u793a)</p> \\[ [A_1, ..., A_s] = IAP(f_i,f_a) \\] </li> <li> <p>\u4f7f\u7528 up-sampling decoder + MLP layers projection head \u6784\u5efa \u201c\u5206\u5e03\u6982\u7387\u9884\u6d4b\u5668\u201d</p> <ul> <li> <p>\u4e0a\u91c7\u6837\u5305\u542b\u56db\u4e2a spatial-temporal dimensions \u5206\u522b\u4e3a\uff1a(1024, 12), (512, 24), (256, 48), and (128, 96) \u7684\u5b50\u5757</p> <ul> <li> <p>temporal axis \u4f1a\u88ab\u5377\u79ef\u64cd\u4f5c\u6269\u5145</p> </li> <li> <p>spatial dimensions \u4f1a\u88ab Max Pooling \u524a\u51cf</p> </li> </ul> </li> <li> <p>\u4f7f\u7528\u4e86 3 Layer MLP</p> </li> </ul> </li> <li> <p>\\(A_s(\\vec{t})\\) \u8868\u793a \\(t^{th}\\) \u5e27\u53ef\u80fd\u5bf9\u5e94\u7684 sub-action \u6982\u7387\u5206\u5e03\uff1b\\(\\vec{t}_s\\) \u662f\u5bf9 \\(s^{th}\\) \u8df3 action sequence \u7684\u9884\u6d4b\u7ed3\u679c</p> <p>\\(\\text{argmax } A_s(\\vec{t})\\) \u5373\u4e3a\u8be5\u5e27\u6700\u53ef\u80fd\u5bf9\u5e94\u7684 sub-action \u7c7b\u578b</p> <p>\u6b64\u65f6 t \u4e0e t+1 \u5fc5\u7136\u5bf9\u5e94\u4e0d\u540c\u7684 sub-aciton =&gt; \u65b0\u7684 sub-action instance \u4ece \\((t+1)^{th}\\) \u5e27\u5f00\u59cb</p> \\[ \\vec{t}_s = \\text{argmax } A_s(\\vec{t}),\\ \\frac{T}{S}(s-1) \\leq \\vec{t} \\leq \\frac{T}{S}s \\] <p>\u4e0a\u5f0f\u4fdd\u8bc1\u4e86 \\(\\vec{t}_1 \\leq ... \\leq \\vec{t}_s\\)</p> </li> </ul>"},{"location":"sum/Transformer/#2-group-contrastive-learning","title":"2 Group Contrastive Learning","text":"<ul> <li> <p>\u4e0a\u4e00\u6b65\u5f97\u5230\u7684 \\((f_i,f_a)\\) \u5171\u4eab\u4e86\u8f83\u591a\u7684\u8bed\u4e49\u4fe1\u606f\uff0c\u76f4\u63a5\u5bf9\u5176\u8fdb\u884c\u6bd4\u8f83\u5b66\u4e60\u4f1a\u5bfc\u81f4 \u5bf9\u5177\u6709\u76f8\u540c\u8bed\u4e49\u7684\u52a8\u4f5c\u5e8f\u5217\u5b66\u4e60\u5f97\u5230\u4e0d\u540c\u7684\u8868\u793a</p> <p>=&gt; \u4f7f\u7528 Group Contrastive Learning\uff0c\u5bf9\u5177\u6709\u76f8\u4f3c sub-action seq \u7684\u89c6\u9891\u8fdb\u884c\u5bf9\u6bd4</p> </li> <li> <p>\u8f93\u5165\u7684 \\((f_i,f_a)\\) \u4f1a\u88ab</p> <ul> <li>\u8d4b\u4e88\u4e0e\u5176\u52a8\u4f5c\u3001\u8bed\u4e49\u5177\u6709\u6700\u5927\u76f8\u4f3c\u5ea6\u7ec4\u522b\u7684 Pseudo Label \\(p\\)</li> <li>\u6700\u7ec8\u751f\u6210 sub-action sequence \\((\\overline{f}_i,\\overline{f}_a)\\)</li> </ul> </li> <li> <p>\u5177\u6709\u76f8\u540c Pseudo Label \u7684 feature \u5c06\u7ec4\u6210\u5982\u4e0b\u7684 group\uff1a</p> \\[ G^p_k = \\frac{\\sum_{i=1}^B g(A_s^k)}{T_B},\\ \\text{where } p=A_s^k \\] <ul> <li> <p>\\(g(.)\\) \u662f\u5e8f\u5217 \\(A_s^k\\) \u7684 logits</p> </li> <li> <p>\\(T_B\\) \u662f Group \\(B\\) \u7684\u5e8f\u5217\u603b\u91cf</p> </li> </ul> </li> <li> <p>\u6211\u4eec\u4f7f\u7528 Group \u7684 average representation \\(G_f^g\\)\uff0c\u5b9a\u4e49\uff1a</p> <ul> <li> <p>positive-pair\uff1a\\(G_a^p, G_i^p\\) \uff08\u4e0a\u6807\u7ec4\u522b\u76f8\u540c\uff09</p> </li> <li> <p>negative-pair\uff1a\\(G_a^p, G_k^q\\) \uff08\u4e0a\u6807\u7ec4\u522b\u4e0d\u540c\uff09</p> <p>Action Seq \u76f8\u540c\uff0c\u4f46 Sub-action Seq \u4e0d\u540c</p> </li> </ul> </li> </ul>"},{"location":"sum/Transformer/#spatio-temporal-multiscale-transformer","title":"Spatio-Temporal Multiscale Transformer","text":"<p>\u8f93\u5165 sub-action sequence \\((\\overline{f}_i,\\overline{f}_a)\\)\uff0c\u5e76\u5728\u4e0d\u540c scale \u4e0a\u6316\u6398 long-range dependencies</p>"},{"location":"sum/Transformer/#1-actor-centric-multiscale-transformer","title":"1 Actor-Centric Multiscale Transformer","text":"<ul> <li> <p>\u672c\u6587\u63d0\u51fa\u7684 Transformer \u5404\u9636\u6bb5\u5177\u6709\u5404\u5f02\u7684 channel resolution =&gt; channel &amp; scale \u9010\u6e10\u589e\u52a0</p> </li> <li> <p>\u8be5 Multiscale Transformer\uff0c\u7531 3 * stage\uff08\u7ed3\u6784\u76f8\u540c\uff09\u6784\u6210</p> <ul> <li> <p>\u6bcf\u4e2a stage </p> <ul> <li> <p>\u5728 early Layer \u5904\u7406\u7c97\u7c92\u5ea6\u7279\u5f81\uff0c\u5e76\u5728\u66f4\u6df1\u5c42\u5904\u7406\u7ec6\u7c92\u5ea6\u7279\u5f81</p> </li> <li> <p>\u5404\u5305\u542b\u4e86 3 \u4e2a transformer block + 8 attention head\uff0c\u7528\u4e8e\u5904\u7406\u76f8\u540c scale \u7684\u4fe1\u606f\u5e76\u751f\u6210 Attention \u503c</p> \\[ \\hat{f}_i += MLP(LN(attention)), attention = Multihead(LN(\\frac{Q_a(K_i)^T}{\\sqrt{d_k}})V_i) + \\overline{f}_i \\] <p>\\(Ln(.)\\) \u4e3a Layer Norm \u64cd\u4f5c\uff0c\\(MLP\\) \u4e3a\u4e24\u5c42\u4f7f\u7528 GELU \u6fc0\u6d3b\u51fd\u6570</p> <p></p> <p>block \u7ed3\u6784\uff1a\\(\\overline{f}_a = query, \\overline{f}_i=memory\\)</p> </li> </ul> </li> <li> <p>\u5bf9\u4e8e\u8f93\u5165\u7279\u5f81 \\(f \\in \\mathbb{R}^{T' \\times C'}\\):</p> <ol> <li> <p>\u8fdb\u884c\u4e00\u6b21 1D \u5377\u79ef (kernel=3, stride=1)</p> </li> <li> <p>\u4f7f\u7528\u5305\u542b \\(L\\) \u4e2a block \u7684 Multiscale Transformer\uff0c\u5176\u4e2d stage n \u7684 output_shape = \\(T' \\times 2^nC'\\)\uff08\u653e\u5927\uff09</p> <p>channel dimesion \u4f1a\u5728 stage \u5207\u6362\u65f6\u901a\u8fc7 MLP \u6269\u5927 2 \u500d</p> </li> </ol> </li> </ul> </li> </ul>"},{"location":"sum/Transformer/#2-multiscale-temporal-fusion","title":"2 Multiscale Temporal Fusion","text":"<p>\u5bf9\u4e8e\u7b2c \\(n\\) \u5c42\u7684 output shape \u4e3a \\(F_n = T' \\times 2^nC'\\)\uff0c\u800c\u7b2c \\(n+1\\) \u5c42\u5219\u4e3a \\(F_{n+1}= T' \\times 2^{n+1}C'\\)</p> <ul> <li> <p>\u4e3a\u4e86\u6210\u529f\u8fdb\u884c aggregate\uff0c\u6211\u4eec\u9700\u8981\u8fdb\u884c upsampling: \\(U_\\varphi(F_n) = \\text{Upsampling}(R_n\\mathcal{W}^n)\\)</p> </li> <li> <p>\u968f\u540e\u4f7f\u7528 element-wise addition \u66f4\u65b0 \\(F_{n+1} = U_\\varphi(F_n) + F_{n+1}\\mathcal{W}^{n+1}\\)</p> </li> </ul> <p>\u6700\u7ec8\u901a\u8fc7 fuse \u7684\u5230\u7684 intergrated feature \\(\\mathcal{F} = \\text{Concat}(F_1, ..., F_{N})\\)</p>"},{"location":"sum/Transformer/#4-optimization","title":"4 Optimization","text":""},{"location":"sum/Transformer/#overall-training-loss","title":"Overall Training Loss","text":"<p>\u6700\u7ec8\u5c06\u7531 2-Layer MLP \u5bf9 \\(\\text{MaxPool}(\\mathcal{F})\\) \u9884\u6d4b\u5f97\u5230\u5206\u7ec4\u6807\u7b7e \\(\\gamma_i\\) \u548c \u56de\u5f52\u5206\u6570 \\(y_i\\)</p> <ul> <li> <p>\u5bf9\u4e8e\u56de\u5f52\u9884\u6d4b\uff0c\u6709\uff1a</p> \\[ \\begin{align*} L_{bce} &amp;= - \\sum_{i=1}^I(\\gamma_i log(\\overline{\\gamma_i}) + (1-\\gamma_i)log(1-\\overline{\\gamma}_i)) \\\\ L_{reg} &amp;= \\sum_{i=1}^I \\| \\overline{y}_i - y_i\\|^2,\\ \\text{where } \\gamma_i = 1 \\end{align*} \\] </li> <li> <p>\u5bf9 Group Contrastive\uff0c\u6709\uff1a</p> \\[ L_{gc} = - \\log{\\frac{     h(G_a^p,G_i^p) / \\tau }{     h(G_a^p,G_i^p) / \\tau + \\sum_{q=1,k}^S  h(G_a^p,G_k^q)  }}, \\text{ where }p\\neq q \\] <ul> <li> <p>\\(h(.) = \\exp{(\\text{cosine similarity})}\\) </p> </li> <li> <p>\\(\\tau\\) \u662f teperature \u8d85\u53c2\u6570</p> </li> </ul> </li> </ul>"},{"location":"sum/survey/","title":"\u51e0\u7bc7\u7efc\u8ff0","text":""},{"location":"sum/survey/#2021-a-survey-of-video-based-aqa","title":"2021: A Survey of Video-based AQA","text":""},{"location":"sum/survey/#1-definition-challenges","title":"1 Definition &amp; Challenges","text":"<ul> <li> <p>\u76f8\u5173\u9886\u57df\uff1aHuman Action Recognition &amp; Analysis</p> <ul> <li> <p>\u73b0\u6709\u7684\u6280\u672f\u652f\u6301 action classification of short/long-term videos, temporal action segmentation and spatial-temporal action location</p> </li> <li> <p>\u5728 video sruveillance, vedio retrieval &amp; human-computer interaction \u9886\u57df\u6709\u7740\u5e7f\u6cdb\u5e94\u7528</p> </li> </ul> <p>\u53ea\u80fd\u5bf9\u52a8\u4f5c\u8fdb\u884c \u7c97\u7c92\u5ea6\u7684\u5206\u7c7b/\u5b9a\u4f4d\uff0c\u5e76\u4e0d\u80fd\u5bf9\u7279\u5b9a\u52a8\u4f5c\u7684\u8d28\u91cf\u8fdb\u884c\u5ba2\u89c2\u8bc4\u4ef7</p> <ul> <li>\u5f3a\u8c03\u8bc6\u522b\u3001\u6355\u6349\u4e0d\u540c\u79cd\u7c7b\u52a8\u4f5c\u4e4b\u95f4\u7684 external differences</li> </ul> <p>AQA \u5219\u805a\u7126\u4e8e\u540c\u4e00\u79cd\u7c7b\u52a8\u4f5c\u95f4\u7684 internal differences</p> </li> <li> <p>AQA \u95ee\u9898\u76ee\u6807</p> <p>\u5f97\u5230\u4e00\u4e2a\u80fd \u81ea\u52a8\u505a\u51fa\u5ba2\u89c2\u8bc4\u4ef7\u7684\u667a\u80fd\u7cfb\u7edf\uff0c\u4ece\u800c\u51cf\u5c11\u5728\u52a8\u4f5c\u8bc4\u4f30\u4e2d\u6295\u5165\u7684\u4eba\u529b\u7269\u529b\u3001\u5e76\u964d\u4f4e\u4e3b\u89c2\u5f71\u54cd\u3002</p> </li> </ul>"},{"location":"sum/survey/#definition-form","title":"Definition &amp; Form","text":"<ul> <li> <p>Video-based AQA \u662f\u4e00\u4e2a\u57fa\u4e8e\u89c6\u9891\u6570\u636e\u751f\u6210\u5bf9\u7279\u5b9a\u52a8\u4f5c\u8d28\u91cf\u5ba2\u89c2\u8bc4\u4ef7\u7684 internal differences \u4efb\u52a1</p> </li> <li> <p>\u7528\u4e8e AQA \u548c HAR \u4efb\u52a1\u7684 \u6a21\u578b \u5177\u6709\u4e00\u5b9a\u7a0b\u5ea6\u7684\u76f8\u4f3c\u6027\uff1a</p> <p>\u9996\u5148\u8fdb\u884c feature extraction\uff0c\u968f\u540e\u901a\u8fc7 network head \u5b9e\u73b0\u590d\u6742\u4efb\u52a1</p> <ul> <li> <p>\u4f20\u7edf\u65b9\u6cd5\u4f1a\u91c7\u53d6 DFT/DCT/linear combination \u5b9e\u73b0 feature aggregation</p> </li> <li> <p>\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u7684\u53d1\u5c55\u5219\u4f7f\u5f97\u901a\u8fc7 \u6df1\u5c42\u5377\u79ef\u7f51\u7edc(DCN)/RNN \u8fdb\u884c video embedding \u6210\u4e3a\u53ef\u80fd</p> </li> </ul> </li> <li> <p>AQA \u4efb\u52a1\u5927\u81f4\u53ef\u88ab\u5212\u5206\u4e3a\u4ee5\u4e0b\u4e09\u79cd\uff1a</p> <ol> <li> <p>Regression Scoring\uff1a\u5e38\u89c1\u4e8e\u8fd0\u52a8\u9886\u57df</p> <ul> <li> <p>\u4e00\u822c\u76f4\u63a5\u4f7f\u7528 SVR/FCN \u76f4\u63a5\u8fdb\u884c\u9884\u6d4b</p> </li> <li> <p>\u4ee5 MSE \u4f5c\u4e3a\u4f18\u5316\u76ee\u6807</p> </li> </ul> </li> <li> <p>Grading\uff1a\u5e38\u89c1\u4e8e\u5bf9\u624b\u672f\u64cd\u4f5c\u7684\u8bc4\u5206</p> <ul> <li> <p>\u5b9e\u9645\u4e0a\u662f\u4e2a\u5206\u7c7b\u4efb\u52a1\uff0c\u8f93\u51fa\u7684\u662f\u8bf8\u5982 <code>novice</code>, <code>medium</code>, <code>expert</code> \u7684\u6807\u7b7e</p> </li> <li> <p>\u4e00\u822c\u4f7f\u7528 classification accuracy \u8fdb\u884c\u8bc4\u4f30</p> </li> </ul> </li> <li> <p>Pairwise Sorting</p> <ul> <li> <p>\u4ece\u6d4b\u8bd5\u96c6\u91cc\u9762\u968f\u624b\u6293\u4e24\u4e2a\uff08\u4e00\u5bf9\uff09\u89c6\u9891\u8fdb\u884c\u62c9\u8e29</p> </li> <li> <p>\u4f7f\u7528 pairwise sorting accuracy \u8fdb\u884c\u8bc4\u4f30</p> </li> </ul> </li> </ol> </li> </ul>"},{"location":"sum/survey/#challenges","title":"Challenges","text":"<ul> <li> <p>Area Specifed</p> <ul> <li> <p>Medical Care\uff1a\u7531\u4e8e\u533b\u7597\u52a8\u4f5c\u5177\u6709\u8f83\u9ad8\u7684\u65f6\u95f4\u590d\u6742\u5ea6\u3001\u8bed\u8a00\u4fe1\u606f\u91cf\u548c\u4f4e\u5bb9\u5fcd\u5ea6\uff0c\u533b\u7597\u76f8\u5173\u7684 AQA \u89e3\u51b3\u65b9\u6848\u9700\u8981\u6709\u8f83\u5f3a\u7684\u8bed\u4e49\u7406\u89e3\u80fd\u529b</p> </li> <li> <p>Sports\uff1abody distortion &amp; motion blur</p> </li> </ul> </li> <li> <p>Common Challenges</p> <p>\u8ba1\u7b97\u6548\u7387\u3001\u89c6\u89d2\u906e\u6321\u3001\u6a21\u578b\u53ef\u89e3\u91ca\u6027  etc.</p> </li> </ul>"},{"location":"sum/survey/#2-datasets-evaluation","title":"2 Datasets &amp; Evaluation","text":"<p>TES \u548c PCS\u76f8\u5173\u6027\u4e0d\u5927 (by MS-LSTM)</p> <p>Datasets for AQA</p> \u7c7b\u578b \u540d\u79f0 Desc. Sport MIT-Diving 60fps, \u5e73\u5747\u6bcf\u4e2a\u89c6\u9891\u5305\u542b 150 \u5e27\uff0c\u5206\u6570\u8303\u56f4\u4e3a [20,100] MIT-Skiing 24fps, \u5e73\u5747\u6bcf\u4e2a\u89c6\u9891\u5305\u542b 4200 \u5e27\uff0c\u5206\u6570\u8303\u56f4\u4e3a [0,100] UNLV Dive &amp; UNLV Vault \u5e73\u5747\u6bcf\u4e2a\u89c6\u9891\u5305\u542b 75 \u5e27\uff0c\u5206\u6570\u8303\u56f4\u4e3a [0,20] Basketball Performance Assessment Dataset 24 Train + 24 Test\uff0c\u6709 250 + 250 \u4e2a pair label AQA-7 \u5305\u542b7\u79cd\u8fd0\u52a8\u7684\u89c6\u9891\uff0c803 train + 303 test MTL-AQA 16\u79cd\u4e0d\u540c\u7c7b\u522b\u7684\u8df3\u6c34\u89c6\u9891\uff0c\u6bcf\u4e2a\u75317\u4f4d\u88c1\u5224\u6253\u5206 FisV-5 \u5e73\u5747\u65f6\u957f\u4e3a 2min50s\uff0c\u75319\u4f4d\u88c1\u5224\u8bc4\u5224 TES &amp; PCS Fall Recognition in Figure Skating 276 \u987a\u5229\u843d\u51b0 + 141 \u6454\u5012 Medical Care \u5305\u542b\u7f1d\u5408\u3001\u7a7f\u9488\u3001\u6253\u7ed3\u4e09\u90e8\u5206\uff0c\u6709\u90e8\u5206\u5206\u548c\u603b\u8bc4\u5206 Others Epic skills 2018 \u5305\u542b\u63c9\u9762\u56e2\u3001\u7ed8\u753b\u3001\u4f7f\u7528\u7b77\u5b50\u4e09\u4e2a\u5b50\u96c6 BEST \u5e73\u5747\u65f6\u957f\u4e3a 188s\uff0c\u5305\u542b5\u79cd\u4e0d\u540c\u65e5\u5e38\u6d3b\u52a8\u7684\u89c6\u9891 Infinite grasp dataset \u5305\u542b\u4e8694\u4e2a\u5a74\u513f\u6293\u4e1c\u897f\u7684\u89c6\u9891\uff0c\u65f6\u957f\u5728 80-500s\uff0c\u6709 pair label <p>Performance Metrics</p> <p> Task Metric Regression Scoring \u5747\u65b9\u8bef\u5dee MSE Grading Classification Accuracy Pairwise Sorting Spearman Correlation Coefficient <p></p> <p>Spearman Correlation Coefficient \\(\\rho\\)</p> \\[ \\rho = \\frac{\\sum_i(p_i - \\overline{p})(q_i - \\overline{q})}{\\sqrt{\\sum_i(p_i - \\overline{p})^2 \\sum_i(q_i - \\overline{q})^2}} \\]"},{"location":"sum/survey/#3-models-2021","title":"3 Models (\u622a\u81f3 2021)","text":"<ul> <li> <p>Medical Skill Evaluation</p> <p>\u7531\u4e8e\u5bf9\u533b\u7597\u52a8\u4f5c\u8bc4\u4f30\u95ee\u9898\u7684\u7814\u7a76\u65e9\u4e8e\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u7684\u53d1\u5c55\uff0c\u5927\u591a\u6570\u533b\u7597\u76f8\u5173\u7684\u6a21\u578b\u90fd\u4f7f\u7528\u4e86 traditional feature</p> </li> <li> <p>Sport AQA</p> <p>\u7531\u4e8e\u8d77\u6b65\u76f8\u5bf9\u8f83\u665a\uff0c\u4f53\u80b2\u76f8\u5173\u7684\u7814\u7a76\u4f7f\u7528 CNN \u548c RNN \u5b9e\u73b0\u4e86\u8f83\u597d\u7684\u6210\u679c</p> <ol> <li> <p>based on Deep Learning</p> <ul> <li> <p>\u901a\u5e38\u4f7f\u7528 2D-CNN/3D-CNN/LSTM \u6765\u8fdb\u884c feature extract &amp; aggregate</p> </li> <li> <p>\u901a\u8fc7 Network Head \u6765\u9002\u914d\u4e0d\u540c\u7c7b\u578b\u7684\u4efb\u52a1</p> </li> </ul> <p>\u6839\u636e\u5173\u6ce8\u70b9\u4e0d\u540c\u53ef\u5212\u5206\u4e3a: Structure Design / Loss Desgin \u4e24\u7c7b</p> </li> <li> <p>based on Handcrafted Features (before 2014)</p> </li> </ol> </li> <li> <p>Medical Care</p> <p>\u7531\u4e8e\u8f83\u5f3a\u7684\u4e13\u4e1a\u6027\uff0c\u533b\u7597\u9886\u57df\u6ca1\u6709\u4e00\u4e2a\u53ef\u4ee5\u4f5c\u4e3a\u7edf\u4e00 benchmark \u7684\u6570\u636e\u96c6</p> <ul> <li> <p>GIT \u805a\u7126\u4e8e OSATS \u7cfb\u7edf\u4e0b\u7684\u5916\u58f3\u624b\u672f\u6280\u80fd\u8bc4\u4f30</p> </li> <li> <p>JHU \u805a\u7126\u4e8e \u673a\u5668\u4eba\u5fae\u521b\u624b\u672f(RMIS)</p> </li> <li> <p>ASU \u805a\u7126\u4e8e \u8179\u8154\u955c\u624b\u672f(laparoscopic surgery)</p> </li> </ul> </li> </ul>"},{"location":"sum/survey/#4","title":"4 \u53d1\u5c55\u524d\u666f","text":"<ul> <li> <p>Dataset</p> <p>\u76ee\u524d\u5b58\u5728\u7684 AQA \u6570\u636e\u96c6\u89c4\u6a21\u8f83\u5c0f\uff0c\u4e14\u5305\u542b\u7684\u8bed\u4e49\u4fe1\u606f\u8f83\u5c11</p> <p>=&gt; \u5e0c\u671b\u5728\u672a\u6765\u63a8\u51fa\u66f4\u5927\u89c4\u6a21\u3001\u5305\u542b\u66f4\u591a\u8bed\u4e49\u4fe1\u606f\u7684\u6570\u636e\u96c6</p> </li> <li> <p>Model: more Efficient &amp; Accurate</p> <ul> <li> <p>\u66f4\u597d\u7684\u5229\u7528 temporal info \u5bf9\u52a8\u4f5c\u8fdb\u884c\u5efa\u6a21</p> </li> <li> <p>\u4f7f\u7528 Unsupervised \u65b9\u6cd5\u51cf\u5c11\u6570\u636e\u6807\u6ce8\u3001\u964d\u4f4e\u4e3b\u89c2\u5f71\u54cd</p> </li> <li> <p>\u5bf9 \u590d\u6742\u3001\u957f\u671f \u52a8\u4f5c\u7684\u8d28\u91cf\u8fdb\u884c\u8bc4\u4f30</p> </li> </ul> </li> </ul>"},{"location":"sum/survey/#2022","title":"2022: \u89c6\u9891\u7406\u89e3\u4e2d\u7684\u52a8\u4f5c\u8d28\u91cf\u8bc4\u4f30\u65b9\u6cd5\u7efc\u8ff0","text":""},{"location":"sum/survey/#1-aqa","title":"1 AQA \u65b9\u6cd5\u5206\u7c7b","text":"<ol> <li>\u4ee5\u8d28\u91cf\u5206\u6570\u4e3a\u8bc4\u4f30\u7ed3\u679c</li> <li>\u4ee5\u7b49\u7ea7\u7c7b\u522b\u4e3a\u8bc4\u4f30\u7ed3\u679c</li> <li>\u4ee5\u8d28\u91cf\u7b49\u7ea7\u6392\u5e8f\u4e3a\u8bc4\u4f30\u7ed3\u679c</li> </ol>"},{"location":"sum/survey/#2","title":"2 \u6570\u636e\u96c6 &amp; \u8bc4\u4ef7\u6307\u6807","text":"<p>21 \u90a3\u7bc7\u6309\u7167\u6570\u636e\u96c6\u7684 \u201c\u5185\u5bb9\u201d \u805a\u7c7b\uff0c\u8fd9\u7bc7\u6309 \u201c\u6307\u6807\u201d \u805a\u7c7b\u4e86</p> <ol> <li> <p>\u4ee5\u8d28\u91cf\u5206\u6570\u4e3a\u8bc4\u4f30\u7ed3\u679c</p> <ul> <li>\u8bc4\u4ef7\u6307\u6807\uff1aSpearman Rank Correlation</li> <li>\u6570\u636e\u96c6\uff1a\uff08\u4e00\u5f20\u8868\uff0c\u6bd4 2021 \u7684\u6709\u589e\u52a0\uff09</li> <li>\u5b9e\u9a8c\u7ed3\u679c\u5bf9\u6bd4\uff1a\u628a\u5404\u4e2a\u65b9\u6cd5\u7684 Spearman Rank Corr. \u5217\u5728\u4e86\u540c\u4e00\u5f20\u8868\u91cc\uff0c\u7136\u540e\u5bf9\u6bd4\u5206\u6790</li> </ul> </li> <li> <p>\u4ee5 \u7b49\u7ea7\u7c7b\u522b/ \u8d28\u91cf\u7b49\u7ea7\u6392\u5e8f \u4e3a\u8bc4\u4f30\u7ed3\u679c\uff08\u4e8c\u5408\u4e00\uff09</p> <ul> <li> <p>\u8bc4\u4ef7\u6307\u6807\uff1a</p> <ul> <li>\u7b49\u7ea7\u7c7b\u522b\uff1a\\(\\text{accuracy} = \\frac{n_{\u5206\u7c7b\u6b63\u786e}}{N}\\)</li> <li> <p>\u8d28\u91cf\u7b49\u7ea7\u6392\u5e8f\uff1a</p> <ul> <li>\u8003\u8651\u4e00\u5bf9\u6837\u672c \\((p_i,p_j)\\)\uff0c\u5982\u679c\u9884\u6d4b\u503c\u7684\u76f8\u5bf9\u5927\u5c0f\u7b26\u5408\u771f\u5b9e\u503c\u7684\u76f8\u5bf9\u5927\u5c0f\u5173\u7cfb\uff0c\u5219\u8ba4\u4e3a\u5206\u7c7b\u6b63\u786e <code>m+=1</code></li> <li>\u5bf9\u4e8e\u6574\u4e2a\u6570\u636e\u96c6\uff0c\u82e5\u5b58\u5728 \\(N\\) \u5bf9\u6837\u672c\uff0c\u5219 \\(\\text{accuracy}_{rank} = \\frac{m}{N}\\)</li> </ul> </li> <li> <p>\uff08\u65b0\uff09NDCG\uff1a\u5e38\u7528\u4e8e\u641c\u7d22\u7b97\u6cd5\u8bc4\u4f30\uff0c\u8bc4\u4f30\u9884\u6d4b\u6392\u540d\u5e8f\u5217\u4e0e\u5b9e\u9645\u6392\u540d\u5e8f\u5217\u7684\u76f8\u4f3c\u7a0b\u5ea6</p> </li> </ul> </li> <li> <p>\u6570\u636e\u96c6</p> </li> <li>\u5b9e\u9a8c\u7ed3\u679c\u5bf9\u6bd4\uff1a\u628a accuracy \u5217\u4e86\u4e00\u4e0b</li> </ul> </li> </ol>"},{"location":"sum/survey/#3","title":"3 \u9650\u5236\u4e0e\u5c55\u671b","text":"<ol> <li> <p>\u4ec5\u9762\u5411\u7279\u5b9a\u52a8\u4f5c</p> <p>\u5bf9\u6bcf\u79cd\u52a8\u4f5c\u5355\u72ec\u8bad\u7ec3\u6a21\u578b\u8fdb\u884c\u8bc4\u4f30\uff0c\u4e0d\u80fd\u5bf9\u590d\u6742\u52a8\u4f5c\u8fdb\u884c\u8bc4\u4f30</p> </li> <li> <p>\u5bf9\u5206\u6570\u7684\u9884\u6d4b\u5206\u6790\u8fc7\u4e8e\u5355\u4e00\uff1a\u53ea\u6709\u56de\u5f52\u6a21\u578b</p> </li> <li> <p>\u867d\u7136\u5b58\u5728 \u201c\u5206\u9636\u6bb5\u8bc4\u4f30\u201d \u7684\u65b9\u6cd5\uff0c\u4f46\u5207\u5272\u65b9\u5f0f\u7b80\u5355</p> <ul> <li>\u5927\u90e8\u5206\u65b9\u6cd5\u76f4\u63a5 \u7b49\u5206 \u89c6\u9891</li> <li>\u90e8\u5206\u91c7\u7528 \u65f6\u5e8f\u5206\u5272\uff0c\u4f46\u53ef\u80fd\u5bfc\u81f4\u65f6\u95f4\u4fe1\u606f\u4e22\u5931\uff0c\u5206\u5272\u51c6\u786e\u7387\u4e5f\u4f1a\u5f71\u54cd\u540e\u7eed\u56de\u5f52\u51c6\u786e\u7387</li> </ul> </li> <li> <p>\u5bf9\u4e8e\u4ee5 \u6392\u5e8f \u4f5c\u4e3a\u8bc4\u4f30\u4f9d\u636e\u7684\u65b9\u6cd5\uff0c\u53ea\u6709 pair-wise \u7684\u76f8\u5bf9\u6392\u5e8f</p> <p>\u800c\u5728 \u6392\u5e8f\u5b66\u4e60\u7b97\u6cd5 \u4e2d\uff0c\u8fd8\u6709 Point-Wise Ranking / ListWise Sorting \u4e24\u79cd</p> <p>\u7f3a\u5c11\u4f7f\u7528\u5176\u4ed6\u6392\u5e8f\u601d\u60f3\u8fdb\u884c\u8bc4\u4f30\u7684\u6307\u6807</p> </li> <li> <p>\u6570\u636e\u96c6\u4ecd\u7136\u6b20\u7f3a</p> <p>\u5bf9\u7279\u5b9a\u9886\u57df\u8fdb\u884c\u4e13\u5bb6\u7ea7\u6ce8\u91ca\u7684\u6210\u672c\u5de8\u5927\uff0c\u800c \u590d\u6742\u52a8\u4f5c\u7684\u00b7\u591a\u9636\u6bb5 \u8bc4\u4ef7\u66f4\u662f\u5982\u6b64</p> <p>\u5927\u90e8\u5206\u6a21\u578b\u4ee5\u57fa\u4e8e image/video \u7684\u5927\u89c4\u6a21\u6570\u636e\u8fdb\u884c\u9884\u8bad\u7ec3\uff0c\u6025\u9700\u57fa\u4e8e \u5f31\u76d1\u7763/\u534a\u76d1\u7763 \u7684\u65b9\u6cd5</p> </li> </ol>"},{"location":"sum/survey/#new-datasets","title":"New Datasets","text":""},{"location":"sum/survey/#2023-logo","title":"2023: LOGO","text":"<p>A Long-Form Video Dataset for Group Action Quality Assessment</p> <ul> <li>\u9886\u57df\uff1aArtistic Swimming\uff08\u82b1\u6837\u6e38\u6cf3\uff09</li> <li>\u5e73\u5747\u65f6\u957f\uff1a204.2s</li> <li> <p>\u5177\u4f53\u63cf\u8ff0\uff1a</p> <ul> <li>\u6765\u81ea 26 \u573a\u6bd4\u8d5b\u7684 200 \u4e2a\u89c6\u9891</li> <li>\u6bcf\u4e2a\u89c6\u9891\u5305\u542b 8 \u540d\u8fd0\u52a8\u5458</li> <li>\u5305\u62ec\u5bf9\u8fd0\u52a8\u5458\u56e2\u4f53\u4fe1\u606f &amp; \u52a8\u4f5c\u8fc7\u7a0b\u7684\u8be6\u7ec6\u6807\u6ce8</li> </ul> </li> <li> <p>\u4eae\u70b9\uff1a</p> <p>\u9664\u57fa\u672c\u7684 <code>score &amp; action</code> \u6807\u6ce8\u5916\uff0cLOGO \u8fd8\u5305\u542b\u4e86 <code>formation</code> \u6807\u7b7e</p> <p>=&gt; \u4f7f\u7528 Graph \u6807\u6ce8\u4e86 8 \u540d\u8fd0\u52a8\u5458\u7ec4\u6210\u7684\u961f\u5f62\u5173\u7cfb</p> </li> </ul> <pre><code>S. Zhang et al., \"LOGO: A Long-Form Video Dataset for Group Action Quality Assessment,\" 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), Vancouver, BC, Canada, 2023, pp. 2405-2414, doi: 10.1109/CVPR52729.2023.00238.\n</code></pre>"},{"location":"sum/survey/#pose","title":"Pose","text":"<p>\u5176\u5b9e z \u8f74\u4f4d\u79fb\u8fd8\u662f\u4f1a\u4e22\u6389</p>"},{"location":"sum/survey/#mmpose","title":"MMPose","text":"<p>MMPose is an open-source toolbox for pose estimation based on PyTorch. </p>"},{"location":"sum/survey/#2019-videopose-3d","title":"2019: VideoPose 3D","text":"<ul> <li> <p>Video-based</p> <p>\u57fa\u4e8e\u89c6\u9891\u7684\u65b9\u6cd5\u5c31\u662f\u5728\u4ee5\u4e0a\u4e24\u7c7b\u65b9\u6cd5\u7684\u57fa\u7840\u4e0a\u5f15\u5165\u65f6\u95f4\u7ef4\u5ea6\u4e0a\u7684\u4fe1\u606f\u3002\u76f8\u90bb\u5e27\u63d0\u4f9b\u7684\u4e0a\u4e0b\u6587\u4fe1\u606f\u53ef\u4ee5\u5e2e\u52a9\u6211\u4eec\u66f4\u597d\u5730\u9884\u6d4b\u5f53\u524d\u5e27\u7684\u59ff\u6001\u3002\u5bf9\u4e8e\u906e\u6321\u60c5\u51b5\uff0c\u4e5f\u53ef\u4ee5\u6839\u636e\u524d\u540e\u51e0\u5e27\u7684\u59ff\u6001\u505a\u4e00\u4e9b\u5408\u7406\u63a8\u6d4b\u3002\u53e6\u5916\uff0c\u7531\u4e8e\u5728\u4e00\u6bb5\u89c6\u9891\u4e2d\u540c\u4e00\u4e2a\u4eba\u7684\u9aa8\u9abc\u957f\u5ea6\u662f\u4e0d\u53d8\u7684\uff0c\u56e0\u6b64\u8fd9\u7c7b\u65b9\u6cd5\u901a\u5e38\u4f1a\u5f15\u5165\u9aa8\u9abc\u957f\u5ea6\u4e00\u81f4\u6027\u7684\u7ea6\u675f\u9650\u5236\uff0c\u6709\u52a9\u4e8e\u8f93\u51fa\u66f4\u52a0\u7a33\u5b9a\u7684 3D pose\u3002</p> </li> <li> <p>\u4ee5 2D pose \u5e8f\u5217\u4f5c\u4e3a\u8f93\u5165\uff0c\u5229\u7528 Temporal Convolutional Network (TCN) \u5904\u7406\u5e8f\u5217\u4fe1\u606f\u5e76\u8f93\u51fa 3D pose\u3002</p> <p>TCN \u7684\u672c\u8d28\u662f\u5728\u65f6\u95f4\u57df\u4e0a\u7684\u5377\u79ef\uff0c\u5b83\u76f8\u5bf9\u4e8e RNN \u7684\u6700\u5927\u4f18\u52bf\u5728\u4e8e\u80fd\u591f\u5e76\u884c\u5904\u7406\u591a\u4e2a\u5e8f\u5217\uff0c\u4e14 TCN \u7684\u8ba1\u7b97\u590d\u6742\u5ea6\u8f83\u4f4e\uff0c\u6a21\u578b\u53c2\u6570\u91cf\u8f83\u5c11\u3002</p> </li> <li> <p>\u5728 VideoPose3D \u4e2d\uff0c\u4f5c\u8005\u8fdb\u4e00\u6b65\u5229\u7528 dilated convolution \u6269\u5927 TCN \u7684\u611f\u53d7\u91ce\u3002\u5177\u4f53\u7684\u7f51\u7edc\u7ed3\u6784\u4e0e SimpleBaseline3D \u7c7b\u4f3c\uff0c\u91c7\u7528\u4e86\u6b8b\u5dee\u8fde\u63a5\u7684\u5168\u5377\u79ef\u7f51\u7edc\u3002</p> </li> <li> <p>\u9664\u6b64\u4ee5\u5916\uff0cVideoPose3D \u8fd8\u5305\u542b\u4e86\u4e00\u79cd\u534a\u76d1\u7763\u7684\u8bad\u7ec3\u65b9\u6cd5\uff0c\u4e3b\u8981\u601d\u8def\u662f\u6dfb\u52a0\u4e00\u4e2a\u8f68\u8ff9\u9884\u6d4b\u6a21\u578b\u7528\u4e8e\u9884\u6d4b\u6839\u5173\u8282\u7684\u7edd\u5bf9\u5750\u6807\uff0c\u5c06\u76f8\u673a\u5750\u6807\u7cfb\u4e0b\u7edd\u5bf9\u7684 3D pose \u6295\u5f71\u56de 2D \u5e73\u9762\uff0c\u4ece\u800c\u5f15\u5165\u91cd\u6295\u5f71\u635f\u5931\u3002\u534a\u76d1\u7763\u65b9\u6cd5\u5728 3D label \u6709\u9650\u7684\u60c5\u51b5\u4e0b\u80fd\u591f\u66f4\u597d\u5730\u63d0\u5347\u7cbe\u5ea6\u3002</p> </li> </ul>"},{"location":"sum/survey/#2022-mhformer","title":"2022: MHFormer","text":"<p>Multi-Hypothesis Transformer\uff08MHFormer\uff09\u7684\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u65e8\u5728\u5b66\u4e60\u591a\u4e2a\u53ef\u80fd\u7684\u59ff\u52bf\u5047\u8bbe\u7684\u65f6\u7a7a\u8868\u793a\u3002</p> <p>\u4e3a\u4e86\u6709\u6548\u6355\u6349\u591a\u5047\u8bbe\u4e4b\u95f4\u7684\u4f9d\u8d56\u5173\u7cfb\uff0c\u5e76\u5728\u5047\u8bbe\u7279\u5f81\u4e4b\u95f4\u5efa\u7acb\u5f3a\u5927\u7684\u5173\u8054\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5c06\u4efb\u52a1\u5206\u89e3\u4e3a\u4e09\u4e2a\u9636\u6bb5\uff1a</p> <ul> <li> <p>\u751f\u6210\u591a\u4e2a\u521d\u59cb\u5047\u8bbe\u8868\u793a\uff1a\u8fd9\u4e2a\u9636\u6bb5\u6d89\u53ca\u521b\u5efa\u591a\u4e2a\u521d\u59cb\u59ff\u52bf\u5047\u8bbe\u7684\u8868\u793a\u3002</p> </li> <li> <p>\u5efa\u6a21\u81ea\u5047\u8bbe\u901a\u4fe1\uff1a\u5728\u8fd9\u4e00\u9636\u6bb5\uff0c\u8be5\u65b9\u6cd5\u6d89\u53ca\u5c06\u591a\u4e2a\u5047\u8bbe\u5408\u5e76\u4e3a\u5355\u4e2a\u6c47\u805a\u8868\u793a\uff0c\u7136\u540e\u5c06\u5176\u5206\u6210\u51e0\u4e2a\u5206\u6563\u7684\u5047\u8bbe\uff0c\u4ee5\u4fc3\u8fdb\u5047\u8bbe\u4e4b\u95f4\u7684\u901a\u4fe1\u3002</p> </li> <li> <p>\u5b66\u4e60\u8de8\u5047\u8bbe\u901a\u4fe1\u548c\u7279\u5f81\u805a\u5408\uff1a\u8fd9\u4e00\u9636\u6bb5\u4fa7\u91cd\u4e8e\u5b66\u4e60\u8de8\u5047\u8bbe\u901a\u4fe1\uff0c\u5e76\u805a\u5408\u591a\u5047\u8bbe\u7279\u5f81\u4ee5\u5408\u6210\u6700\u7ec8\u76843D\u59ff\u52bf\u3002</p> </li> </ul>"},{"location":"sum/survey/#2024-hot","title":"2024: HoT","text":"<p>Hourglass Tokenizer for Efficient Transformer-Based 3D Human Pose Estimation</p> <ul> <li> <p>\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5373\u63d2\u5373\u7528\u7684\u4fee\u526a\u548c\u6062\u590d\u6846\u67b6\uff0c\u79f0\u4e3aHourglass Tokenizer\uff08HoT\uff09\uff0c\u7528\u4e8e\u4ece\u89c6\u9891\u4e2d\u9ad8\u6548\u8fdb\u884c\u57fa\u4e8eTransformer\u7684\u4e09\u7ef4\u4eba\u4f53\u59ff\u52bf\u4f30\u8ba1\u3002</p> </li> <li> <p>HoT\u4ece\u4fee\u526a\u5197\u4f59\u5e27\u7684\u59ff\u52bf\u6807\u8bb0\u5f00\u59cb\uff0c\u5e76\u4ee5\u6062\u590d\u5b8c\u6574\u957f\u5ea6\u7684\u6807\u8bb0\u7ed3\u675f\uff0c\u4ece\u800c\u5728\u4e2d\u95f4Transformer\u5757\u4e2d\u4ea7\u751f\u5c11\u91cf\u59ff\u52bf\u6807\u8bb0\uff0c\u4ece\u800c\u63d0\u9ad8\u6a21\u578b\u7684\u6548\u7387\u3002</p> <p>\u4e3a\u4e86\u6709\u6548\u5b9e\u73b0\u8fd9\u4e00\u70b9\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u6807\u8bb0\u4fee\u526a\u805a\u7c7b\uff08TPC\uff09\uff0c\u5b83\u52a8\u6001\u9009\u62e9\u4e00\u4e9b\u5177\u6709\u9ad8\u8bed\u4e49\u591a\u6837\u6027\u7684\u4ee3\u8868\u6027\u6807\u8bb0\uff0c\u540c\u65f6\u6d88\u9664\u89c6\u9891\u5e27\u7684\u5197\u4f59\u3002</p> <p>\u6b64\u5916\uff0c\u6211\u4eec\u5f00\u53d1\u4e86\u4e00\u4e2a\u6807\u8bb0\u6062\u590d\u6ce8\u610f\u529b\uff08TRA\uff09\uff0c\u6839\u636e\u6240\u9009\u6807\u8bb0\u6062\u590d\u8be6\u7ec6\u7684\u65f6\u7a7a\u4fe1\u606f\uff0c\u4ece\u800c\u5c06\u7f51\u7edc\u8f93\u51fa\u6269\u5c55\u5230\u539f\u59cb\u7684\u5168\u957f\u5ea6\u65f6\u95f4\u5206\u8fa8\u7387\uff0c\u4ee5\u4fbf\u8fdb\u884c\u5feb\u901f\u63a8\u65ad\u3002</p> </li> </ul>"},{"location":"sum/survey/#segmentation","title":"Segmentation","text":""},{"location":"sum/survey/#2017-ed-tcn","title":"2017: ED-TCN","text":"<ul> <li>\u6211\u4eec\u7684\u7f16\u7801\u5668-\u89e3\u7801\u5668TCN\u5229\u7528\u6c60\u5316\u548c\u4e0a\u91c7\u6837\u6765\u6709\u6548\u6355\u83b7\u957f\u7a0b\u65f6\u95f4\u6a21\u5f0f\uff0c\u800c\u6211\u4eec\u7684\u6269\u5f20TCN\u4f7f\u7528\u6269\u5f20\u5377\u79ef\u3002</li> <li>TCNs\u80fd\u591f\u6355\u83b7\u52a8\u4f5c\u7ec4\u5408\u3001\u6bb5\u6301\u7eed\u65f6\u95f4\u548c\u957f\u7a0b\u4f9d\u8d56\u5173\u7cfb\uff0c\u5e76\u4e14\u8bad\u7ec3\u901f\u5ea6\u6bd4\u57fa\u4e8e\u7ade\u4e89\u7684\u57fa\u4e8eLSTM\u7684\u9012\u5f52\u795e\u7ecf\u7f51\u7edc\u5feb\u4e00\u4e2a\u6570\u91cf\u7ea7\u3002</li> </ul>"},{"location":"sum/survey/#2020-ms-tcn","title":"2020: MS-TCN++","text":"<p>\u76f8\u6bd4\u4e8eMS-TCN\u7684:</p> <ol> <li>\u5c06\u539f\u59cb\u751f\u6210\u7684\u521d\u59cb\u7279\u5f81\u4e2d\u5229\u7528\u5e76\u884c\u7684\u7a7a\u6d1e\u5377\u79ef\u6765\u6355\u83b7\u591a\u5c3a\u5ea6\u7279\u5f81</li> <li>\u4ece\u7279\u5f81\u5197\u4f59\u7684\u89d2\u5ea6\u5165\u624b\uff0c\u8ba4\u4e3a\u540e\u9762\u51e0\u5c42\u7684\u7279\u5f81\u76f8\u4f3c\u6027\u8f83\u9ad8\uff0c\u6240\u4ee5\u53c2\u6570\u5171\u4eab\u7684\u65b9\u6cd5\u6765\u964d\u4f4e\u53c2\u6570\u91cf\u548c\u8026\u5408\u5ea6</li> </ol>"},{"location":"sum/survey/#2021-acm-net","title":"2021: ACM-Net","text":"<p>Action Context Modeling Network for Weakly-Supervised Temporal Action Localization</p>"},{"location":"sum/survey/#_2","title":"\u591a\u6a21\u6001","text":""},{"location":"sum/survey/#2023-skating-mixer","title":"2023: Skating-Mixer \u97f3\u9891","text":"<p>Long-Term Sport Audio-Visual Modeling with MLPs</p> <ul> <li> <p>Previous work</p> <ol> <li>\u82b1\u6837\u6ed1\u51b0\u4e2d\u7684\u6bcf\u4e2a\u52a8\u4f5c\u53d8\u5316\u8fc5\u901f\uff0c\u56e0\u6b64\u7b80\u5355\u5730\u5e94\u7528\u4f20\u7edf\u7684\u5e27\u91c7\u6837\u4f1a\u4e22\u5931\u5f88\u591a\u5b9d\u8d35\u7684\u4fe1\u606f\uff0c\u7279\u522b\u662f\u57283\u52305\u5206\u949f\u7684\u89c6\u9891\u4e2d</li> <li>\u5148\u524d\u7684\u65b9\u6cd5\u5f88\u5c11\u8003\u8651\u5176\u6a21\u578b\u4e2d\u7684\u5173\u952e audio-visual \u5173\u7cfb</li> </ol> </li> <li> <p>Skating-Mixer: \u5b83\u5c06MLP\u6846\u67b6\u6269\u5c55\u4e3a\u591a\u6a21\u6001\u65b9\u5f0f\uff0c\u5e76\u901a\u8fc7\u6211\u4eec\u8bbe\u8ba1\u7684\u8bb0\u5fc6\u5faa\u73af\u5355\u5143\uff08MRU\uff09\u6709\u6548\u5730\u5b66\u4e60\u957f\u671f\u8868\u793a\u3002</p> </li> <li> <p>Neo Dataset: FS1000</p> <p>\u5176\u4e2d\u5305\u542b\u4e868\u79cd\u8282\u76ee\u7c7b\u578b\u76841000\u591a\u4e2a\u89c6\u9891\uff0c\u6db5\u76d6\u4e867\u79cd\u4e0d\u540c\u7684\u8bc4\u5206\u6307\u6807\uff0c\u5728\u6570\u91cf\u548c\u591a\u6837\u6027\u4e0a\u8d85\u8fc7\u4e86\u5176\u4ed6\u6570\u636e\u96c6\u3002</p> </li> <li> <p>benchmarks: Fis-V, FS1000</p> </li> </ul>"}]}