
<!doctype html>
<html lang="zh" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
        <link rel="canonical" href="https://triplepiers.github.io/AQA/sum/">
      
      
        <link rel="prev" href="..">
      
      
        <link rel="next" href="../rep/">
      
      
      <link rel="icon" href="../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.5.3, mkdocs-material-9.5.3">
    
    
      
        <title>Summary - Action Quality Assessment</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/main.50c56a3b.min.css">
      
        
        <link rel="stylesheet" href="../assets/stylesheets/palette.06af60db.min.css">
      
      
  
  
    
    
  
    
    
  
    
    
  
    
    
  
    
    
  
    
    
  
    
    
  
    
    
  
    
    
  
    
    
  
    
    
  
    
    
  
  
  <style>:root{--md-admonition-icon--note:url('data:image/svg+xml;charset=utf-8,<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 16 16"><path d="M1 7.775V2.75C1 1.784 1.784 1 2.75 1h5.025c.464 0 .91.184 1.238.513l6.25 6.25a1.75 1.75 0 0 1 0 2.474l-5.026 5.026a1.75 1.75 0 0 1-2.474 0l-6.25-6.25A1.752 1.752 0 0 1 1 7.775Zm1.5 0c0 .066.026.13.073.177l6.25 6.25a.25.25 0 0 0 .354 0l5.025-5.025a.25.25 0 0 0 0-.354l-6.25-6.25a.25.25 0 0 0-.177-.073H2.75a.25.25 0 0 0-.25.25ZM6 5a1 1 0 1 1 0 2 1 1 0 0 1 0-2Z"/></svg>');--md-admonition-icon--abstract:url('data:image/svg+xml;charset=utf-8,<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 16 16"><path d="M2.5 1.75v11.5c0 .138.112.25.25.25h3.17a.75.75 0 0 1 0 1.5H2.75A1.75 1.75 0 0 1 1 13.25V1.75C1 .784 1.784 0 2.75 0h8.5C12.216 0 13 .784 13 1.75v7.736a.75.75 0 0 1-1.5 0V1.75a.25.25 0 0 0-.25-.25h-8.5a.25.25 0 0 0-.25.25Zm13.274 9.537v-.001l-4.557 4.45a.75.75 0 0 1-1.055-.008l-1.943-1.95a.75.75 0 0 1 1.062-1.058l1.419 1.425 4.026-3.932a.75.75 0 1 1 1.048 1.074ZM4.75 4h4.5a.75.75 0 0 1 0 1.5h-4.5a.75.75 0 0 1 0-1.5ZM4 7.75A.75.75 0 0 1 4.75 7h2a.75.75 0 0 1 0 1.5h-2A.75.75 0 0 1 4 7.75Z"/></svg>');--md-admonition-icon--info:url('data:image/svg+xml;charset=utf-8,<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 16 16"><path d="M0 8a8 8 0 1 1 16 0A8 8 0 0 1 0 8Zm8-6.5a6.5 6.5 0 1 0 0 13 6.5 6.5 0 0 0 0-13ZM6.5 7.75A.75.75 0 0 1 7.25 7h1a.75.75 0 0 1 .75.75v2.75h.25a.75.75 0 0 1 0 1.5h-2a.75.75 0 0 1 0-1.5h.25v-2h-.25a.75.75 0 0 1-.75-.75ZM8 6a1 1 0 1 1 0-2 1 1 0 0 1 0 2Z"/></svg>');--md-admonition-icon--tip:url('data:image/svg+xml;charset=utf-8,<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 16 16"><path d="M8 1.5c-2.363 0-4 1.69-4 3.75 0 .984.424 1.625.984 2.304l.214.253c.223.264.47.556.673.848.284.411.537.896.621 1.49a.75.75 0 0 1-1.484.211c-.04-.282-.163-.547-.37-.847a8.456 8.456 0 0 0-.542-.68c-.084-.1-.173-.205-.268-.32C3.201 7.75 2.5 6.766 2.5 5.25 2.5 2.31 4.863 0 8 0s5.5 2.31 5.5 5.25c0 1.516-.701 2.5-1.328 3.259-.095.115-.184.22-.268.319-.207.245-.383.453-.541.681-.208.3-.33.565-.37.847a.751.751 0 0 1-1.485-.212c.084-.593.337-1.078.621-1.489.203-.292.45-.584.673-.848.075-.088.147-.173.213-.253.561-.679.985-1.32.985-2.304 0-2.06-1.637-3.75-4-3.75ZM5.75 12h4.5a.75.75 0 0 1 0 1.5h-4.5a.75.75 0 0 1 0-1.5ZM6 15.25a.75.75 0 0 1 .75-.75h2.5a.75.75 0 0 1 0 1.5h-2.5a.75.75 0 0 1-.75-.75Z"/></svg>');--md-admonition-icon--success:url('data:image/svg+xml;charset=utf-8,<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 16 16"><path d="M13.78 4.22a.75.75 0 0 1 0 1.06l-7.25 7.25a.75.75 0 0 1-1.06 0L2.22 9.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018L6 10.94l6.72-6.72a.75.75 0 0 1 1.06 0Z"/></svg>');--md-admonition-icon--question:url('data:image/svg+xml;charset=utf-8,<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 16 16"><path d="M0 8a8 8 0 1 1 16 0A8 8 0 0 1 0 8Zm8-6.5a6.5 6.5 0 1 0 0 13 6.5 6.5 0 0 0 0-13ZM6.92 6.085h.001a.749.749 0 1 1-1.342-.67c.169-.339.436-.701.849-.977C6.845 4.16 7.369 4 8 4a2.756 2.756 0 0 1 1.637.525c.503.377.863.965.863 1.725 0 .448-.115.83-.329 1.15-.205.307-.47.513-.692.662-.109.072-.22.138-.313.195l-.006.004a6.24 6.24 0 0 0-.26.16.952.952 0 0 0-.276.245.75.75 0 0 1-1.248-.832c.184-.264.42-.489.692-.661.103-.067.207-.132.313-.195l.007-.004c.1-.061.182-.11.258-.161a.969.969 0 0 0 .277-.245C8.96 6.514 9 6.427 9 6.25a.612.612 0 0 0-.262-.525A1.27 1.27 0 0 0 8 5.5c-.369 0-.595.09-.74.187a1.01 1.01 0 0 0-.34.398ZM9 11a1 1 0 1 1-2 0 1 1 0 0 1 2 0Z"/></svg>');--md-admonition-icon--warning:url('data:image/svg+xml;charset=utf-8,<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 16 16"><path d="M6.457 1.047c.659-1.234 2.427-1.234 3.086 0l6.082 11.378A1.75 1.75 0 0 1 14.082 15H1.918a1.75 1.75 0 0 1-1.543-2.575Zm1.763.707a.25.25 0 0 0-.44 0L1.698 13.132a.25.25 0 0 0 .22.368h12.164a.25.25 0 0 0 .22-.368Zm.53 3.996v2.5a.75.75 0 0 1-1.5 0v-2.5a.75.75 0 0 1 1.5 0ZM9 11a1 1 0 1 1-2 0 1 1 0 0 1 2 0Z"/></svg>');--md-admonition-icon--failure:url('data:image/svg+xml;charset=utf-8,<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 16 16"><path d="M2.344 2.343h-.001a8 8 0 0 1 11.314 11.314A8.002 8.002 0 0 1 .234 10.089a8 8 0 0 1 2.11-7.746Zm1.06 10.253a6.5 6.5 0 1 0 9.108-9.275 6.5 6.5 0 0 0-9.108 9.275ZM6.03 4.97 8 6.94l1.97-1.97a.749.749 0 0 1 1.275.326.749.749 0 0 1-.215.734L9.06 8l1.97 1.97a.749.749 0 0 1-.326 1.275.749.749 0 0 1-.734-.215L8 9.06l-1.97 1.97a.749.749 0 0 1-1.275-.326.749.749 0 0 1 .215-.734L6.94 8 4.97 6.03a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018Z"/></svg>');--md-admonition-icon--danger:url('data:image/svg+xml;charset=utf-8,<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 16 16"><path d="M9.504.43a1.516 1.516 0 0 1 2.437 1.713L10.415 5.5h2.123c1.57 0 2.346 1.909 1.22 3.004l-7.34 7.142a1.249 1.249 0 0 1-.871.354h-.302a1.25 1.25 0 0 1-1.157-1.723L5.633 10.5H3.462c-1.57 0-2.346-1.909-1.22-3.004L9.503.429Zm1.047 1.074L3.286 8.571A.25.25 0 0 0 3.462 9H6.75a.75.75 0 0 1 .694 1.034l-1.713 4.188 6.982-6.793A.25.25 0 0 0 12.538 7H9.25a.75.75 0 0 1-.683-1.06l2.008-4.418.003-.006a.036.036 0 0 0-.004-.009l-.006-.006-.008-.001c-.003 0-.006.002-.009.004Z"/></svg>');--md-admonition-icon--bug:url('data:image/svg+xml;charset=utf-8,<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 16 16"><path d="M4.72.22a.75.75 0 0 1 1.06 0l1 .999a3.488 3.488 0 0 1 2.441 0l.999-1a.748.748 0 0 1 1.265.332.75.75 0 0 1-.205.729l-.775.776c.616.63.995 1.493.995 2.444v.327c0 .1-.009.197-.025.292.408.14.764.392 1.029.722l1.968-.787a.75.75 0 0 1 .556 1.392L13 7.258V9h2.25a.75.75 0 0 1 0 1.5H13v.5c0 .409-.049.806-.141 1.186l2.17.868a.75.75 0 0 1-.557 1.392l-2.184-.873A4.997 4.997 0 0 1 8 16a4.997 4.997 0 0 1-4.288-2.427l-2.183.873a.75.75 0 0 1-.558-1.392l2.17-.868A5.036 5.036 0 0 1 3 11v-.5H.75a.75.75 0 0 1 0-1.5H3V7.258L.971 6.446a.75.75 0 0 1 .558-1.392l1.967.787c.265-.33.62-.583 1.03-.722a1.677 1.677 0 0 1-.026-.292V4.5c0-.951.38-1.814.995-2.444L4.72 1.28a.75.75 0 0 1 0-1.06Zm.53 6.28a.75.75 0 0 0-.75.75V11a3.5 3.5 0 1 0 7 0V7.25a.75.75 0 0 0-.75-.75ZM6.173 5h3.654A.172.172 0 0 0 10 4.827V4.5a2 2 0 1 0-4 0v.327c0 .096.077.173.173.173Z"/></svg>');--md-admonition-icon--example:url('data:image/svg+xml;charset=utf-8,<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 16 16"><path d="M5 5.782V2.5h-.25a.75.75 0 0 1 0-1.5h6.5a.75.75 0 0 1 0 1.5H11v3.282l3.666 5.76C15.619 13.04 14.543 15 12.767 15H3.233c-1.776 0-2.852-1.96-1.899-3.458Zm-2.4 6.565a.75.75 0 0 0 .633 1.153h9.534a.75.75 0 0 0 .633-1.153L12.225 10.5h-8.45ZM9.5 2.5h-3V6c0 .143-.04.283-.117.403L4.73 9h6.54L9.617 6.403A.746.746 0 0 1 9.5 6Z"/></svg>');--md-admonition-icon--quote:url('data:image/svg+xml;charset=utf-8,<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 16 16"><path d="M1.75 2.5h10.5a.75.75 0 0 1 0 1.5H1.75a.75.75 0 0 1 0-1.5Zm4 5h8.5a.75.75 0 0 1 0 1.5h-8.5a.75.75 0 0 1 0-1.5Zm0 5h8.5a.75.75 0 0 1 0 1.5h-8.5a.75.75 0 0 1 0-1.5ZM2.5 7.75v6a.75.75 0 0 1-1.5 0v-6a.75.75 0 0 1 1.5 0Z"/></svg>');}</style>



    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=LXGW+WenKai+Screen:300,300i,400,400i,700,700i%7CJetBrains+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"LXGW WenKai Screen";--md-code-font:"JetBrains Mono"}</style>
      
    
    
      <link rel="stylesheet" href="https://gcore.jsdelivr.net/npm/lxgw-wenkai-screen-webfont@1.1.0/style.css">
    
      <link rel="stylesheet" href="https://gcore.jsdelivr.net/npm/lxgw-wenkai-webfont@1.1.0/style.css">
    
      <link rel="stylesheet" href="../termynal.css">
    
    <script>__md_scope=new URL("..",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="teal" data-md-color-accent="purple">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#_1" class="md-skip">
          跳转至
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="页眉">
    <a href=".." title="Action Quality Assessment" class="md-header__button md-logo" aria-label="Action Quality Assessment" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Action Quality Assessment
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Summary
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="default" data-md-color-primary="teal" data-md-color-accent="purple"  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_0">
    
      <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_1" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a4 4 0 0 0-4 4 4 4 0 0 0 4 4 4 4 0 0 0 4-4 4 4 0 0 0-4-4m0 10a6 6 0 0 1-6-6 6 6 0 0 1 6-6 6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12 20 8.69Z"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="slate" data-md-color-primary="deep-purple" data-md-color-accent="light-green"  aria-label="Switch to light mode"  type="radio" name="__palette" id="__palette_1">
    
      <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_0" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 18c-.89 0-1.74-.2-2.5-.55C11.56 16.5 13 14.42 13 12c0-2.42-1.44-4.5-3.5-5.45C10.26 6.2 11.11 6 12 6a6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12 20 8.69Z"/></svg>
      </label>
    
  
</form>
      
    
    
      <script>var media,input,key,value,palette=__md_get("__palette");if(palette&&palette.color){"(prefers-color-scheme)"===palette.color.media&&(media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']"),palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent"));for([key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



<nav class="md-nav md-nav--primary" aria-label="导航栏" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href=".." title="Action Quality Assessment" class="md-nav__button md-logo" aria-label="Action Quality Assessment" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    Action Quality Assessment
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href=".." class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Home
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  <span class="md-ellipsis">
    Summary
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  <span class="md-ellipsis">
    Summary
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="目录">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      目录
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#1-tsa-net" class="md-nav__link">
    <span class="md-ellipsis">
      1 TSA-Net
    </span>
  </a>
  
    <nav class="md-nav" aria-label="1 TSA-Net">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1-1-abstract" class="md-nav__link">
    <span class="md-ellipsis">
      1-1 Abstract
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#1-2-approach" class="md-nav__link">
    <span class="md-ellipsis">
      1-2 Approach
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#2-core" class="md-nav__link">
    <span class="md-ellipsis">
      2 CoRe
    </span>
  </a>
  
    <nav class="md-nav" aria-label="2 CoRe">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#2-1-abstract" class="md-nav__link">
    <span class="md-ellipsis">
      2-1 Abstract
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2-2-relative-works" class="md-nav__link">
    <span class="md-ellipsis">
      2-2 Relative Works
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2-3-approach" class="md-nav__link">
    <span class="md-ellipsis">
      2-3 Approach
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2-4-evaluation-protocol" class="md-nav__link">
    <span class="md-ellipsis">
      2-4 Evaluation Protocol
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#3-pecop" class="md-nav__link">
    <span class="md-ellipsis">
      3 PECoP
    </span>
  </a>
  
    <nav class="md-nav" aria-label="3 PECoP">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#3-1-abstract" class="md-nav__link">
    <span class="md-ellipsis">
      3-1 Abstract
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#3-2-relative-works" class="md-nav__link">
    <span class="md-ellipsis">
      3-2 Relative Works
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#3-3-approach" class="md-nav__link">
    <span class="md-ellipsis">
      3-3 Approach
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#4-video-based-aqa" class="md-nav__link">
    <span class="md-ellipsis">
      4 Video-based AQA (综述)
    </span>
  </a>
  
    <nav class="md-nav" aria-label="4 Video-based AQA (综述)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#4-1-definition-challenges" class="md-nav__link">
    <span class="md-ellipsis">
      4-1 Definition &amp; Challenges
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#4-2-datasets-evaluation" class="md-nav__link">
    <span class="md-ellipsis">
      4-2 Datasets &amp; Evaluation
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#4-3-models-2021" class="md-nav__link">
    <span class="md-ellipsis">
      4-3 Models (截至 2021)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#4-4" class="md-nav__link">
    <span class="md-ellipsis">
      4-4 发展前景
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#5-tpt" class="md-nav__link">
    <span class="md-ellipsis">
      5 TPT
    </span>
  </a>
  
    <nav class="md-nav" aria-label="5 TPT">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#5-1-abstract" class="md-nav__link">
    <span class="md-ellipsis">
      5-1 Abstract
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#5-2-approach" class="md-nav__link">
    <span class="md-ellipsis">
      5-2 Approach
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#5-3-optimization" class="md-nav__link">
    <span class="md-ellipsis">
      5-3 Optimization
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#6-fspn" class="md-nav__link">
    <span class="md-ellipsis">
      6 FSPN
    </span>
  </a>
  
    <nav class="md-nav" aria-label="6 FSPN">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#6-1-abstract" class="md-nav__link">
    <span class="md-ellipsis">
      6-1 Abstract
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#6-2-relative-works" class="md-nav__link">
    <span class="md-ellipsis">
      6-2 Relative Works
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#6-3-approach" class="md-nav__link">
    <span class="md-ellipsis">
      6-3 Approach
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#6-4-optimization" class="md-nav__link">
    <span class="md-ellipsis">
      6-4 Optimization
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#7-iris" class="md-nav__link">
    <span class="md-ellipsis">
      7 IRIS
    </span>
  </a>
  
    <nav class="md-nav" aria-label="7 IRIS">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#7-1-abstract" class="md-nav__link">
    <span class="md-ellipsis">
      7-1 Abstract
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#7-2-relative-works" class="md-nav__link">
    <span class="md-ellipsis">
      7-2 Relative Works
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#7-3-approach" class="md-nav__link">
    <span class="md-ellipsis">
      7-3 Approach
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#7-4-evaluation" class="md-nav__link">
    <span class="md-ellipsis">
      7-4 Evaluation
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../rep/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Repitition
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_4" >
        
          
          <label class="md-nav__link" for="__nav_4" id="__nav_4_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    References
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4">
            <span class="md-nav__icon md-icon"></span>
            References
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_4_1" >
        
          
          <label class="md-nav__link" for="__nav_4_1" id="__nav_4_1_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    2021
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_4_1_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4_1">
            <span class="md-nav__icon md-icon"></span>
            2021
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../ref/2021/CoRe.pdf" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    CoRe 组感知对比回归
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../ref/2021/TSA-Net.pdf" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    TSA-Net 管自注意力网络
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../ref/2021/Video-based.pdf" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Video-based
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_4_2" >
        
          
          <label class="md-nav__link" for="__nav_4_2" id="__nav_4_2_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    2022
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_4_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4_2">
            <span class="md-nav__icon md-icon"></span>
            2022
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../ref/2022/TPA.pdf" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    TPT 时序转译 transformer
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_4_3" >
        
          
          <label class="md-nav__link" for="__nav_4_3" id="__nav_4_3_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    2023
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_4_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4_3">
            <span class="md-nav__icon md-icon"></span>
            2023
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../ref/2023/PECoP.pdf" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    PECoP 连续预训练
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../ref/2023/STPN.pdf" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    FSPN 细粒度时空解析网络
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../ref/2023/IRIS.pdf" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    IRIS 评分标准可解释分割
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="目录">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      目录
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#1-tsa-net" class="md-nav__link">
    <span class="md-ellipsis">
      1 TSA-Net
    </span>
  </a>
  
    <nav class="md-nav" aria-label="1 TSA-Net">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1-1-abstract" class="md-nav__link">
    <span class="md-ellipsis">
      1-1 Abstract
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#1-2-approach" class="md-nav__link">
    <span class="md-ellipsis">
      1-2 Approach
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#2-core" class="md-nav__link">
    <span class="md-ellipsis">
      2 CoRe
    </span>
  </a>
  
    <nav class="md-nav" aria-label="2 CoRe">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#2-1-abstract" class="md-nav__link">
    <span class="md-ellipsis">
      2-1 Abstract
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2-2-relative-works" class="md-nav__link">
    <span class="md-ellipsis">
      2-2 Relative Works
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2-3-approach" class="md-nav__link">
    <span class="md-ellipsis">
      2-3 Approach
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2-4-evaluation-protocol" class="md-nav__link">
    <span class="md-ellipsis">
      2-4 Evaluation Protocol
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#3-pecop" class="md-nav__link">
    <span class="md-ellipsis">
      3 PECoP
    </span>
  </a>
  
    <nav class="md-nav" aria-label="3 PECoP">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#3-1-abstract" class="md-nav__link">
    <span class="md-ellipsis">
      3-1 Abstract
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#3-2-relative-works" class="md-nav__link">
    <span class="md-ellipsis">
      3-2 Relative Works
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#3-3-approach" class="md-nav__link">
    <span class="md-ellipsis">
      3-3 Approach
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#4-video-based-aqa" class="md-nav__link">
    <span class="md-ellipsis">
      4 Video-based AQA (综述)
    </span>
  </a>
  
    <nav class="md-nav" aria-label="4 Video-based AQA (综述)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#4-1-definition-challenges" class="md-nav__link">
    <span class="md-ellipsis">
      4-1 Definition &amp; Challenges
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#4-2-datasets-evaluation" class="md-nav__link">
    <span class="md-ellipsis">
      4-2 Datasets &amp; Evaluation
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#4-3-models-2021" class="md-nav__link">
    <span class="md-ellipsis">
      4-3 Models (截至 2021)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#4-4" class="md-nav__link">
    <span class="md-ellipsis">
      4-4 发展前景
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#5-tpt" class="md-nav__link">
    <span class="md-ellipsis">
      5 TPT
    </span>
  </a>
  
    <nav class="md-nav" aria-label="5 TPT">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#5-1-abstract" class="md-nav__link">
    <span class="md-ellipsis">
      5-1 Abstract
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#5-2-approach" class="md-nav__link">
    <span class="md-ellipsis">
      5-2 Approach
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#5-3-optimization" class="md-nav__link">
    <span class="md-ellipsis">
      5-3 Optimization
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#6-fspn" class="md-nav__link">
    <span class="md-ellipsis">
      6 FSPN
    </span>
  </a>
  
    <nav class="md-nav" aria-label="6 FSPN">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#6-1-abstract" class="md-nav__link">
    <span class="md-ellipsis">
      6-1 Abstract
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#6-2-relative-works" class="md-nav__link">
    <span class="md-ellipsis">
      6-2 Relative Works
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#6-3-approach" class="md-nav__link">
    <span class="md-ellipsis">
      6-3 Approach
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#6-4-optimization" class="md-nav__link">
    <span class="md-ellipsis">
      6-4 Optimization
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#7-iris" class="md-nav__link">
    <span class="md-ellipsis">
      7 IRIS
    </span>
  </a>
  
    <nav class="md-nav" aria-label="7 IRIS">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#7-1-abstract" class="md-nav__link">
    <span class="md-ellipsis">
      7-1 Abstract
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#7-2-relative-works" class="md-nav__link">
    <span class="md-ellipsis">
      7-2 Relative Works
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#7-3-approach" class="md-nav__link">
    <span class="md-ellipsis">
      7-3 Approach
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#7-4-evaluation" class="md-nav__link">
    <span class="md-ellipsis">
      7-4 Evaluation
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  


<h1 id="_1">汇总</h1>
<h2 id="1-tsa-net">1 TSA-Net</h2>
<h3 id="1-1-abstract">1-1 Abstract</h3>
<ul>
<li>
<p>现有工作的不足：</p>
<ol>
<li>
<p>直接迁移 Action Recognition，忽略了最本质的特征 —— foreground &amp; background info</p>
<ul>
<li>HAR 的重点在于 —— 区分 <em>不同的动作</em></li>
<li>QAQ 的重点在于 —— 对于 <em>特定动作</em> 的优缺点进行评估</li>
</ul>
<p>=&gt; 对两者采用相同的 特征提取 方式显然是不对的</p>
</li>
<li>
<p>Feature Aggregasion 效率低</p>
<p>受卷积操作的感受野大小限制，导致损失了 long-range dependencies (?)</p>
<blockquote>
<p>看起来是缺少时间信息</p>
</blockquote>
</li>
<li>
<p>RNN 可以通过隐藏态存储上下文，但是 <em>不能并行计算</em></p>
</li>
</ol>
<p><span style="color:red;">所以，AQA 需要一个 <u>高效的 Feature Aggregation 算法</u> ！</span></p>
</li>
<li>
<p>创新点：</p>
<ol>
<li>
<p>引入 single object tracker</p>
</li>
<li>
<p>提出 Tube Self-Attention Module(TSA)</p>
<ul>
<li>
<p>基于 tube 机制 和 self-attention 机制，实现高效的 feature aggregation</p>
</li>
<li>
<p>通过 adopting sparse feature interactions 生成丰富的时空上下文信息</p>
</li>
</ul>
</li>
</ol>
</li>
</ul>
<h4 id="attention-based-feature-aggregation">Attention-Based Feature Aggregation</h4>
<ol>
<li>
<p>Non-Local Opertaion</p>
<p>每一个 position 会和 <strong>所有 feature</strong> 做 Dense Correlation</p>
</li>
<li>
<p>Tube Self-Attention Operation</p>
<p>仅对各时刻下同一 spatio-temporal tube 内的每一个 postion 做 Sparse Correlation</p>
<ul>
<li>tube 使模型专注于 feature map 的一个子集 —— 运动员（忽略背景）</li>
<li>self-attention 生成了 时间维度 上的 上下文信息</li>
</ul>
</li>
</ol>
<h3 id="1-2-approach">1-2 Approach</h3>
<p><img alt="" src="../assets/TSA%20Pipline.png" /></p>
<ol>
<li>
<p>包含两个步骤：</p>
<ul>
<li>
<p>使用已有的 Visual Object Tacking 模型生成 tracking box，然后通过 feature selection 生成 tube（ST-Tube）</p>
<p>假设输入的视频共有 <span class="arithmatex">\(L\)</span> 帧，<span class="arithmatex">\(b_l\)</span> 表示第 <span class="arithmatex">\(l\)</span> 帧中的 Bounding-Box</p>
<div class="arithmatex">\[
    V:\text{L-frames Video} \rightarrow VOT_{SiamMask} \rightarrow B:\text{BBoxs}\{b_l\}_{l=1}^L
\]</div>
</li>
<li>
<p>将视频划分为 N-Clip 交给 I3D-s1（立体卷积）生成 Feature Map</p>
<p>假设 <span class="arithmatex">\(L=MN\)</span>，我们将视频划分为 <span class="arithmatex">\(N\)</span> 个包含 <span class="arithmatex">\(M\)</span> 个连续帧的 Clip</p>
<p>使用 I3D 算法可以从中生成 <span class="arithmatex">\(N\)</span> 个 features</p>
<div class="arithmatex">\[
    V:\text{N-fold Video} \rightarrow I3D \rightarrow X:\text{features}\{x_n\}_{n=1}^N
\]</div>
</li>
</ul>
</li>
<li>
<p>对 Feature Map 中 tube 框定的部分应用 self-attention 机制，从而实现 feature aggregation</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-0-1" name="__codelineno-0-1" href="#__codelineno-0-1"></a>    B: BBoxs    \
<a id="__codelineno-0-2" name="__codelineno-0-2" href="#__codelineno-0-2"></a>                 |--&gt; TSA Module --&gt; 包含上下文时空信息的 X&#39;: features (N个)
<a id="__codelineno-0-3" name="__codelineno-0-3" href="#__codelineno-0-3"></a>    X: features /
</code></pre></div>
<blockquote>
<p>X 与 X' 等大：TSA Module 没有改变 Feature Map 的 shape</p>
<p>=&gt; 这一特性允许我们<u>堆叠 TSA</u>以获得更丰富的时空信息</p>
</blockquote>
</li>
<li>
<p>将 Aggregated Feature 交给 I3D-Stage2 生成 <span class="arithmatex">\(H\)</span></p>
<div class="arithmatex">\[
    X' \rightarrow I3D_{s2} \rightarrow H: \text{features}\{h_n\}_{n=1}^N
\]</div>
</li>
<li>
<p>Network Head</p>
<ol>
<li>
<p>使用 AvgPooling 在 Clip 纬度上进行 fuse</p>
<div class="arithmatex">\[
    \overline{h} = \frac{1}{N} \sum_{n=1}^N h_n
\]</div>
</li>
<li>
<p>迁移训练 MLP_Block 对 <span class="arithmatex">\(H\)</span> 进行打分</p>
<div class="arithmatex">\[
    \overline{h} \rightarrow \text{MLP-Block} \rightarrow Score
\]</div>
</li>
</ol>
</li>
</ol>
<h4 id="tsa-module">TSA Module</h4>
<p>根据 Bounding-Box 对 I3D 生成的 feature-map 进行过滤，减少 self-attention 处理的数据量</p>
<blockquote>
<p>他说这能去除背景噪声的干扰 =&gt; Non-local Module 会使用整个 Feature-Map，从而引入背景噪声</p>
</blockquote>
<ol>
<li>
<p>Tube 生成</p>
<ul>
<li>
<p>由于 I3D-s1 使用了 2 * temporal Pooling (<span class="arithmatex">\(/2^2\)</span>)，Bounding-Box:feature-map X = 4:1</p>
</li>
<li>
<p>问题：SiamMask 会产生 <em>平行四边形</em> 边框 -&gt; 我们不好直接在 feature-map 里框方格</p>
<p>解决： 使用如下的 alignment 策略生成 mask</p>
<ol>
<li>将连续的 4 个 BBox 缩放到和 feature-map 一样的大小</li>
<li>分别生成 4 个 Mask：覆盖面积超过阈值 <span class="arithmatex">\(\tau = 0.5\)</span> 记 1，否则为 0</li>
<li>使用并集作为最终的 Mask' <span class="arithmatex">\(M_{c,t}^{l\rightarrow l+3}\)</span></li>
</ol>
</li>
<li>
<p>最终参与 self-attention 是所有 mask=1 的 feature: <span class="arithmatex">\(\Omega_{c,t}\)</span></p>
<div class="arithmatex">\[
    \Omega_{c,t} = \{(i,j)|M_{c,t}^{l\rightarrow l+3}(i,j)=1\}
\]</div>
</li>
</ul>
</li>
<li>
<p>Self-Attention</p>
<p>生成了和 feature-map <span class="arithmatex">\(x\)</span> 等大的 <span class="arithmatex">\(y\)</span>，输出两者的 residual <span class="arithmatex">\(x' = Wy + x\)</span></p>
<p><center><img alt="" src="../assets/TSA%20Module.png" /></center>
<center>本文中的 TSA 算法</center></p>
</li>
<li>
<p>Nertwork Head</p>
<blockquote>
<p>通过修改 MLP_Block 的输出大小 &amp; Loss Func，Network Head 就可以兼容不同的任务</p>
</blockquote>
<ul>
<li>Classification：输出由 n_class 决定 + BCE Loss</li>
<li>Regression：输出 1 维向量 + MSE Loss</li>
<li>
<p>Score distribution prediction：</p>
<ul>
<li>
<p>在 USDL 中嵌入 TSA Module</p>
</li>
<li>
<p>Loss = Kullback-Leibler (KL) divergence of predicted score distribution and ground-truth (GT) score distribution</p>
</li>
</ul>
</li>
</ul>
</li>
</ol>
<h2 id="2-core">2 CoRe</h2>
<h3 id="2-1-abstract">2-1 Abstract</h3>
<div class="admonition info">
<p class="admonition-title">对比学习 Constrastive Learning</p>
<p>学习一个 representation space，通过比较两个（same class）样本在 representation space 中的距离来衡量其 语义联系 (semantic relationship)</p>
<blockquote>
<p>因为只有同组的才能直接对比，所以需要 Group-aware</p>
</blockquote>
<p>本文训练模型用于 <u>回归预测 relative score</u> 并对两个 video 的 score 进行对比，从而学习其 diff 用于最终分数估计</p>
</div>
<ul>
<li>
<p>现有工作：</p>
<p>从 single video 回归得到一个 quality score（针对单一视频的回归任务），面临三个挑战：</p>
<ol>
<li>
<p>Score Label 是由人类裁判给出的，其主观评判会给分数估计造成极大困难</p>
<p>suffter a lot from <u>large inter-video score variation</u></p>
</li>
<li>
<p>video 之间的差异是十分微小的：运动员往往在相似的环境下作出类似的动作</p>
</li>
<li>
<p>大多数解决方案使用 Spaerman's Rank 进行评估，这种方法可能不能正确反应评估表现</p>
</li>
</ol>
</li>
<li>
<p>创新点：</p>
<ul>
<li>
<p>基于 reference to another video that has shared attributes 进行分数预测（不是直接从单个视频下手）</p>
</li>
<li>
<p>提出了 CoRe：</p>
<p>通过 pare-wise comparision，强调视频之间的 <em>diff</em>，并引导模型学习评估的 key hint</p>
<div class="arithmatex">\[
    \text{Video} \rightarrow Model \rightarrow \text{Relative Score Space}
\]</div>
</li>
<li>
<p>通过 GART 将传统的分数回归预测划分为两个子问题：</p>
<blockquote>
<p>group-aware regression tree</p>
</blockquote>
<ol>
<li>
<p>coarse-to-fine classification: 判断 “好坏” 的二分类</p>
<p>将 relative score 空间划分成多个相离区间，随后通过二叉树将 relavtive score 划分到对应的 group</p>
</li>
<li>
<p>small interval regression</p>
<p><u>在 relative score 被预测的 class 内</u> 预测 final score</p>
</li>
</ol>
</li>
<li>
<p>提出 Relative L2-distance，在模型评估中考虑组间差异</p>
</li>
</ul>
</li>
</ul>
<h3 id="2-2-relative-works">2-2 Relative Works</h3>
<ul>
<li>
<p>Gordan：使用骨架轨迹评估体操跳跃动作质量</p>
</li>
<li>
<p>Pirsiavash：</p>
<ul>
<li>
<p>离散余弦变换（DCT）被用于从 body pose 图像到 input feature 的编码</p>
</li>
<li>
<p>SVR 被用于从 feature 到 final score 的映射</p>
</li>
</ul>
</li>
<li>
<p>Parmar：使用 C3D 对视频数据进行编码以获得 spatio-temporal features</p>
</li>
<li>
<p>Xu：使用两个 LSTM 来学习 multi-scale feature</p>
</li>
<li>
<p>Pan：</p>
<ul>
<li>
<p>使用 spatial &amp; temporal relation graphs，在其相交处建模（？）</p>
</li>
<li>
<p>使用 I3D 提取 spatio-temporal features</p>
</li>
</ul>
</li>
<li>
<p>Tang：提出 USDL 来降低人类裁判打分造成的歧义</p>
<blockquote>
<p>uncertainty-aware score distribution learning</p>
</blockquote>
</li>
</ul>
<h3 id="2-3-approach">2-3 Approach</h3>
<p><img alt="" src="../assets/CoRe%20Pipline.png" /></p>
<h4 id="1-contrastive-regression">1 Contrastive Regression</h4>
<ul>
<li>
<p>Problem Formulation</p>
<ul>
<li>
<p>大多数方法将 AQA 视为一个回归问题：输入包含目标动作的 video，输出对应的 quality score</p>
</li>
<li>
<p>部分方法提出了 difficulty-score（已知），最终结果变为 qulity * difficulty</p>
</li>
</ul>
<div class="admonition bug">
<p class="admonition-title">由于视频往往在相似的环境下拍摄，模型很难从微小的变化中学习巨大的分数差异</p>
</div>
<hr />
<ul>
<li>
<p>重新定义问题为：</p>
<p>Regress relative score between the Input Video <span class="arithmatex">\(v_m=\{F_m^i\}_{i=1}^{L_m}\)</span> &amp; an Exemplar <span class="arithmatex">\(v_n=\{F_n^i\}_{i=1}^{L_n}\)</span> with Score Label <span class="arithmatex">\(s_n\)</span></p>
</li>
<li>
<p>我们可以将回归问题写为如下形式：</p>
<div class="arithmatex">\[
\hat{s}_m = R_{\Theta}(F_W(v_m), F_W(v_n)) + s_n
\]</div>
<p><center>其中 <span class="arithmatex">\(R_{\Theta}\)</span> 为回归模型，<span class="arithmatex">\(F_W\)</span> 为特征提取模型（参数相同的I3D）</center></p>
<blockquote>
<p>计算 CurInput &amp; Exampler 之间的偏差值，然后基于 Exampler 的分数进行生成评分</p>
</blockquote>
</li>
</ul>
</li>
<li>
<p>Exemplar-Based Score Regression</p>
<div class="admonition question">
<p class="admonition-title">如何挑选具有可比性的 Exampler</p>
</div>
<ol>
<li>
<p>使用 I3D 模型对 input &amp; exampler 进行特征提取，得到 <span class="arithmatex">\(f_m, f_n\)</span></p>
</li>
<li>
<p>将两者特征与 exampler score 进行聚合：<span class="arithmatex">\(f_{(n,m)} = concat([f_n, f_m, \frac{s_n}{\epsilon}])\)</span></p>
<p><span class="arithmatex">\(\epsilon\)</span> 是一个 norm constant，用于确保 <span class="arithmatex">\(\frac{s_n}{\epsilon} \in [0,1]\)</span></p>
</li>
<li>
<p>使用回归模型对 <span class="arithmatex">\(pair(n,m)\)</span> 的 score-diff 进行预测：<span class="arithmatex">\(\Delta s = R_{\Theta}(f_{(n,m)})\)</span></p>
</li>
</ol>
</li>
</ul>
<h4 id="2-group-aware-regression-tree">2 Group-Aware Regression Tree</h4>
<ul>
<li>
<p>contrastive regression 存在的问题</p>
<p>比较回归可以预测 relative score <span class="arithmatex">\(\Delta s\)</span>，但通常具有较大的值域</p>
<p>=&gt; 直接预测 <span class="arithmatex">\(\Delta s\)</span> 具有一定困难</p>
</li>
<li>
<p>GART 实现分治</p>
<ol>
<li>
<p>将 <span class="arithmatex">\(\Delta s\)</span> 的值域划分为 2<sup>d</sup> 个 non-overlapping class</p>
</li>
<li>
<p>建立 <span class="arithmatex">\(d-1\)</span> 层的 二叉回归树（有 2<sup>d</sup> 个 leaf nodes）</p>
</li>
<li>
<p>分类</p>
<p>整个过程遵循 coarse-to-fine manner：第一层决定 input 比 exampler 好/坏，并在后续具体量化 better/worse 的程度</p>
</li>
</ol>
</li>
<li>
<p>Tree Architecture</p>
<ul>
<li>
<p>Input：</p>
<ul>
<li>
<p>Root Node: 将聚合特征 <span class="arithmatex">\(f_{(n,m)}\)</span> 放进多层神经网络 MLP，并将 <span class="arithmatex">\(\text{MLP}(f_{(n,m)})\)</span> 作为 GART 的输入</p>
</li>
<li>
<p>Other Nodes: 父节点的输出</p>
</li>
</ul>
</li>
<li>
<p>Output:</p>
<ul>
<li>
<p>Internal Nodes: 产生通向左右子节点的概率</p>
<p>通向某个具体 leaf node 的概率 = <span class="arithmatex">\(\prod p_{\text{nodes in path}}\)</span></p>
</li>
<li>
<p>Leaf Nodes: <span class="arithmatex">\(sigmoid(P_{\text{leaf}}) \in [0,1]\)</span> 代表了 Input 和对应 class 的 score-diff</p>
</li>
</ul>
</li>
</ul>
</li>
<li>
<p>边界划分策略</p>
<blockquote>
<p>等长划分 class 会导致 unbalance</p>
</blockquote>
<ol>
<li>
<p>收集测试集中所有 pair 的 score-diff： <span class="arithmatex">\(\delta = [\delta_1, ... ,\delta_T]\)</span>，对其进行升序排列后得到 <span class="arithmatex">\(\delta^*\)</span></p>
</li>
<li>
<p>给定 n_class = <span class="arithmatex">\(R\)</span>，遵循以下策略得到各组边界 <span class="arithmatex">\(G^r = (min^r, max^r)\)</span>:</p>
<div class="arithmatex">\[
\begin{align*}
    min^r &amp;= \delta^*\left[\lfloor(T-1) \times \frac{r-1}{R}\rfloor\right] \\
    max^r &amp;= \delta^*\left[\lfloor(T-1) \times \frac{r}{R}\rfloor\right]
\end{align*}
\]</div>
<p><center>其中，<span class="arithmatex">\(\delta^*[i]\)</span> 表示数组中的第 i 个元素</center></p>
</li>
</ol>
</li>
<li>
<p>优化策略</p>
<blockquote>
<p>GART 包含 classification &amp; in-class regression，所以总的 Cost 要拆成两部分。</p>
</blockquote>
<p>当 pair <span class="arithmatex">\(\delta\)</span> 的 score-diff 被分类至 Class i 时:</p>
<ul>
<li>
<p>将 One-Hot 分类标签的对应位置置 1: <span class="arithmatex">\(l[i] = 1\)</span> </p>
</li>
<li>
<p>另 regression 标签 <span class="arithmatex">\(\sigma_i = \frac{\delta - min^i}{max^i - min^i}\)</span></p>
</li>
</ul>
<p>此时，每个 pair 都拥有 One-Hot Classification Label <span class="arithmatex">\(l\)</span> &amp; Regression Label <span class="arithmatex">\(\sigma\)</span>:</p>
<div class="arithmatex">\[
\begin{align*}
J_{cls} &amp;= - \sum_{r=1}^{R}(l_r \log(P_r) + (1 - l_r)\log(1-P_r))\\
J_{reg} &amp;= \sum_{r=0}^R(\hat{\sigma}_r - \sigma_r)^2, \text{where } l_r=1\\
J &amp;= J_{cls} + J_{reg}
\end{align*}
\]</div>
<p><center>其中 <span class="arithmatex">\(P_r, \hat{\sigma}_r\)</span> 分别是 Leaf Probability &amp; Regression Result</center></p>
</li>
<li>
<p>前向推导（仅 GART 部分）</p>
<div class="arithmatex">\[
R_{\Theta}(f_{(n,m)}) = \hat{\sigma}_{r^*} (max^{r^*}-min^{r^*}) + min^{r^*}
\]</div>
<p><center><span class="arithmatex">\(r^*\)</span> 是具有 Max Probability 的 class 编号</center></p>
<div class="admonition info">
<p class="admonition-title">Multi-Exampler Voting Strategy</p>
<p>对于给定输入 <span class="arithmatex">\(v_{\text{test}}\)</span>，选定 <span class="arithmatex">\(M\)</span> 个 exampler <span class="arithmatex">\(\{v^m_{\text{train}}\}_{m=1}^M\)</span> 及其对应分数标签 <span class="arithmatex">\(\{s^m_{\text{train}}\}_{m=1}^M\)</span></p>
<p>Voting 策略可以使用如下公式进行描述：</p>
<div class="arithmatex">\[
    \begin{align*}
        \hat{s}^m_{\text{test}} &amp;= R_{\Theta}(F_W(v_{\text{test}}, v_{\text{train}}^m)) + s_{\text{train}}^m \\
        \hat{s}_{\text{test}} &amp;= \frac{1}{M} \sum \hat{s}_{\text{test}}^m
    \end{align*}
\]</div>
</div>
</li>
</ul>
<h3 id="2-4-evaluation-protocol">2-4 Evaluation Protocol</h3>
<ol>
<li>
<p>Spearman's Rank Correlation </p>
<p>为了能和之前的工作进行比较，此处采用了 Spearman's Rank Correlation 作为评估标准：</p>
<div class="arithmatex">\[
\rho = \frac{\sum(p_i-p)(q_i-q)}{\sqrt{\sum(p_i-\overline{p})^2 \sum(q_i-\overline{q})^2}}
\]</div>
<p>其中 <span class="arithmatex">\(p,q\)</span> 分别表示样本在两个序列中的 ranking</p>
</li>
<li>
<p>Fisher’s z-value</p>
<p>用于评估 across-action 的平均表现</p>
</li>
<li>
<p>Relative L2-Distance</p>
<p>对于特定动作的 max/min score，R-l<sub>2</sub> 遵循如下定义：</p>
<div class="arithmatex">\[
R-l_2(\theta) = \frac{1}{K} \sum\left(\frac{|s_k - \hat{s}_k|}{s_{max} - s_{min}}\right)^2
\]</div>
<p>其中 <span class="arithmatex">\(s_k,\hat{s}_k\)</span> 分别表示第 k 个样本的 ground-truth 和 预测得分</p>
<blockquote>
<p>传统的 l<sub>2</sub> 距离对于从属于不同 class 的 action 是无意义的；</p>
<p>相比于 Spearman's Rank Correlation 注重 RANK，R-l<sub>2</sub> 更加重视具体数值</p>
</blockquote>
</li>
</ol>
<h2 id="3-pecop">3 PECoP</h2>
<h3 id="3-1-abstract">3-1 Abstract</h3>
<ul>
<li>
<p>先前的工作</p>
<ul>
<li>
<p>因为有标签的 AQA 数据很少，先前的工作一般基于在 large-scale domain-general dataset 上预训练的模型进行优化</p>
<p>=&gt; 在存在较大 domain shift 时，模型的 generalisation 较差</p>
</li>
<li>
<p>侧重点不同：</p>
<ul>
<li>
<p>在 Parkinson’s Disease 严重性评估中，运动节奏中的一两次间断都会对 quality score 产生极大影响</p>
</li>
<li>
<p>而在 pretraining task（动作分类）中，轻微（甚至更加严重的）区别并不会影响 action classification</p>
</li>
</ul>
</li>
<li>
<p>Continual Pretraining</p>
<p>主要用于 NLP，以通过 domain-specific unlabeled data 训练任务专精的模型</p>
<p>这种方法要求更新所有参数，同时存储针对所有 separate task 的 params</p>
</li>
<li>
<p>BatchNorm Tuning</p>
<p>支持通过仅调节 BatchNorm Layer 参数获取 domain-specific model</p>
<p>但是在 较小的 / 更加 domain-specific 的 AQA 数据集中不起作用</p>
</li>
</ul>
</li>
<li>
<p>创新点</p>
<p>PECoP 通过增加一个额外的预训练过程来 reduce domain shift（专注于特定 AQA 任务）：</p>
<ul>
<li>
<p>往预训练的 3D CNN 外面包一层 3D-Adapters <u>自监督学习</u> spatiotemporal &amp; in-domain info</p>
</li>
<li>
<p>只更新 Adapter 中的参数，预训练模型参数保持不变</p>
<blockquote>
<p>此处拉踩需要更新 <u>所有参数</u> 的 HPT</p>
</blockquote>
</li>
</ul>
</li>
</ul>
<h3 id="3-2-relative-works">3-2 Relative Works</h3>
<ul>
<li>
<p>AQA：如 Tang(USDL)，Yu（CoRe）</p>
<p>大多数方法将 AQA 视为 监督学习（score）的 回归任务，并尝试降低裁判的主观影响</p>
<div class="admonition bug">
<p class="admonition-title">这些方法都忽视了 base dataset(K400) 和 target dataset 之间的 domain gap</p>
</div>
</li>
<li>
<p>SSL for AQA</p>
<p>近期的一些方法开始尝试 自监督学习（SSL）。在传统的 Regression Loss 外，在迁移学习中增加了 SSL Loss（不需要添加额外的数据标注）</p>
<p>如 Liu 应用了 Self-Supervised Contrasive Loss</p>
</li>
<li>
<p>Continual Pretraining</p>
<p>相比于传统的迁移学习，Continual Pretraining 通过 in-domain SSL 增强了 domain shift 问题</p>
<ul>
<li>
<p>Gururangan：证明了增加 in-domain data pretraining 对于文本分类性能的影响</p>
</li>
<li>
<p>Reed：证明了在更接近 target dataset 的数据集上进行 Continual Pretraining 可以加快收敛、提高鲁棒性</p>
</li>
<li>
<p>Azizi：结合了 supervised pretraining（on ImageNet） &amp; intermediate contrastive SSL（on MedicalImages），得到了具有更佳泛化性的医学图像诊断器</p>
</li>
</ul>
</li>
<li>
<p>Adapters</p>
<p>Transformer 架构的 lightweight bottleneck module，用于实现 parameter-efficient 迁移学习</p>
<ul>
<li>
<p>Chen：</p>
<ul>
<li>
<p>提出 AdaptFormer 用于可量化的 img/video 识别任务</p>
</li>
<li>
<p>提出 Conv-Adapter，使其可以应用于 2D CNN</p>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="3-3-approach">3-3 Approach</h3>
<p><img alt="" src="../assets/PECoP%20Pipline.png" /></p>
<ul>
<li>
<p>Training Set：</p>
<ol>
<li><span class="arithmatex">\(D_g\)</span>: large-scale, labelled, domain-general</li>
<li><span class="arithmatex">\(D_t\)</span>: target video dataset in the AQA domain</li>
</ol>
<p>两个数据集分别对应具有 significant domain discrepancy 的学习任务 <span class="arithmatex">\(T_g, T_t\)</span></p>
</li>
<li>
<p>Test Set: <span class="arithmatex">\(D_q \subseteq D_t\)</span>, unlabelled</p>
</li>
<li>
<p>Target：在 <span class="arithmatex">\(D_g， D_t\)</span> 上调参，以找到一个可用于 <span class="arithmatex">\(D_q\)</span> 的 transferable spatiotemporal feature extractor </p>
</li>
</ul>
<h4 id="domain-general-pretraining">Domain-general pretraining</h4>
<p>这一部分直接使用了 pretrained backbone model —— 在 K400 上经过监督学习得到的 I3D 模型</p>
<h4 id="in-domain-ssl-continual-pretraining">In-domain SSL continual pretraining</h4>
<ul>
<li>
<p>本文提出的 3D-Adapter 和 Transformer &amp; 2D CNN 中的相应模块具有类似的结构，在此基础上添加了 3D Layers 以实现视频数据的 3D CNN 操作</p>
</li>
<li>
<p>在每一个 Inception Module 的 concatenation Layer 后插入一个 3D-Adapter 可以得到显著的性能提升</p>
</li>
</ul>
<p><center><img alt="" src="../assets/ReCoP%20Incept%20with%20Adapter.png" /></center>
<center>添加 3D-Adapter 后的 Inception Module</center></p>
<ul>
<li>
<p>3D-Adapter modules 将被 <strong>随机初始化</strong> </p>
</li>
<li>
<p>每一个 3D-Apater 使用了：</p>
<ul>
<li>可学习参数 <span class="arithmatex">\(\theta_{down}\)</span> —— downsampling · depth-wise · 3D convolution</li>
<li>非线性激活函数 <span class="arithmatex">\(f(.)\)</span>，如 ReLU</li>
<li>可学习参数 <span class="arithmatex">\(\theta_{up}\)</span> —— upsampling · point-wise · 3D convolustion</li>
</ul>
</li>
<li>
<p>相关参数如下：</p>
<ul>
<li><span class="arithmatex">\(C_{in}, C_{out}\)</span>： 输入/输出 的 channel dimensions</li>
<li>compression factor <span class="arithmatex">\(\lambda\)</span>： bottleneck dimension</li>
</ul>
</li>
</ul>
<p>因此，对于给定的 input feture vecor <span class="arithmatex">\(h_{in} \in \mathbb{R}^{C_{in} \times D \times H \times W}\)</span>，3D-Adapter 将输出 <span class="arithmatex">\(h_{out} \in \mathbb{R}^{C_{out} \times D \times H \times W}\)</span></p>
<p>具体变换过程可描述为：</p>
<div class="arithmatex">\[
h_{out} = \alpha \odot (\theta_{up} \otimes f(\theta_{down} \overline{\otimes} h_{in})) + h_{in}
\]</div>
<p>其中：</p>
<ul>
<li><span class="arithmatex">\(\otimes, \overline{\otimes}\)</span> 分别表示 depth-wise / point-wise 卷积操作</li>
<li><span class="arithmatex">\(\alpha \in \mathbb{R}^{C_{out}}\)</span> 是可调超参数，初始化为 ones</li>
<li><span class="arithmatex">\(\odot\)</span> 表示 element-wise 乘法</li>
</ul>
<hr />
<p>Continual Pretraining 阶段：</p>
<div class="admonition tip">
<p class="admonition-title">只有 3D-Adapter 中的参数会被更新</p>
</div>
<ul>
<li>
<p>在集合 <span class="arithmatex">\(D_q\)</span> 中以 SSL 形式进行 —— 视频标签由模型自动生成</p>
</li>
<li>
<p>使用了 Video Segment Pace Prediction (VSPP) 进行了预处理，从而对 “以不同速度完成的动作” 进行对比</p>
</li>
</ul>
<h4 id="supervised-fine-tuning">Supervised fine-tuning</h4>
<div class="admonition tip">
<p class="admonition-title">在这个阶段，<u>所有参数（预训练参数 &amp; 3D-Adapter）</u> 将在 <span class="arithmatex">\(D_t\)</span> 上被微调</p>
</div>
<ul>
<li>
<p>USDL / MUSDL</p>
<ul>
<li>
<p>特征 -&gt; I3D backbone -&gt; temporal Pooling -&gt; SoftMax -&gt; 预测分布</p>
</li>
<li>
<p>使用 <em>预测分布</em> 与 <em>由 ground-truth 产生的高斯分布</em> 之间的 KL loss 进行优化</p>
</li>
</ul>
<blockquote>
<p>MUSDL 是 USDL 的 multi-path 版本，适用于 MTL-AQA / JIGSAWS 这种由多个裁判同时打分的数据集</p>
</blockquote>
</li>
<li>
<p>CoRe</p>
<ul>
<li>
<p>使用添加 3D-Adapter 的 I3D 模型进行特征提取，随后丢回 GART 进行回归</p>
</li>
<li>
<p>最终结果取多个 exampler 的平均</p>
</li>
</ul>
</li>
<li>
<p>TSA</p>
<p>使用添加 3D-Adapter 的 I3D 模型进行特征提取，随后丢回 Attention Module 进行后续操作</p>
</li>
</ul>
<h2 id="4-video-based-aqa">4 Video-based AQA (综述)</h2>
<h3 id="4-1-definition-challenges">4-1 Definition &amp; Challenges</h3>
<ul>
<li>
<p>相关领域：Human Action Recognition &amp; Analysis</p>
<ul>
<li>
<p>现有的技术支持 action classification of short/long-term videos, temporal action segmentation and spatial-temporal action
location</p>
</li>
<li>
<p>在 video sruveillance, vedio retrieval &amp; human-computer interaction 领域有着广泛应用</p>
</li>
</ul>
<div class="admonition bug">
<p class="admonition-title">只能对动作进行 粗粒度的分类/定位，并不能对特定动作的质量进行客观评价</p>
</div>
<ul>
<li>强调识别、捕捉不同种类动作之间的 external differences</li>
</ul>
<blockquote>
<p>AQA 则聚焦于同一种类动作间的 internal differences</p>
</blockquote>
</li>
<li>
<p>AQA 问题目标</p>
<p>得到一个能 <em>自动做出客观评价的智能系统</em>，从而减少在动作评估中投入的人力物力、并降低主观影响。</p>
</li>
</ul>
<h4 id="definition-form">Definition &amp; Form</h4>
<ul>
<li>
<p>Video-based AQA 是一个基于视频数据生成对特定动作质量客观评价的 internal differences 任务</p>
</li>
<li>
<p>用于 AQA 和 HAR 任务的 <strong>模型</strong> 具有一定程度的相似性：</p>
<p>首先进行 feature extraction，随后通过 network head 实现复杂任务</p>
<ul>
<li>
<p>传统方法会采取 <u>DFT/DCT/linear combination</u> 实现 feature aggregation</p>
</li>
<li>
<p>深度学习方法的发展则使得通过 <u>深层卷积网络(DCN)/RNN</u> 进行 video embedding 成为可能</p>
</li>
</ul>
</li>
<li>
<p>AQA 任务大致可被划分为以下三种：</p>
<ol>
<li>
<p>Regression Scoring：常见于运动领域</p>
<ul>
<li>
<p>一般直接使用 <u>SVR/FCN</u> 直接进行预测</p>
</li>
<li>
<p>以 MSE 作为优化目标</p>
</li>
</ul>
</li>
<li>
<p>Grading：常见于对手术操作的评分</p>
<ul>
<li>
<p>实际上是个分类任务，输出的是诸如 <code>novice</code>, <code>medium</code>, <code>expert</code> 的标签</p>
</li>
<li>
<p>一般使用 classification accuracy 进行评估</p>
</li>
</ul>
</li>
<li>
<p>Pairwise Sorting</p>
<ul>
<li>
<p>从测试集里面随手抓两个（一对）视频进行拉踩</p>
</li>
<li>
<p>使用 pairwise sorting accuracy 进行评估</p>
</li>
</ul>
</li>
</ol>
</li>
</ul>
<h4 id="challenges">Challenges</h4>
<ul>
<li>
<p>Area Specifed</p>
<ul>
<li>
<p>Medical Care：由于医疗动作具有较高的时间复杂度、语言信息量和低容忍度，医疗相关的 AQA 解决方案需要有较强的语义理解能力</p>
</li>
<li>
<p>Sports：body distortion &amp; motion blur</p>
</li>
</ul>
</li>
<li>
<p>Common Challenges</p>
<p>计算效率、视角遮挡、模型可解释性  etc.</p>
</li>
</ul>
<h3 id="4-2-datasets-evaluation">4-2 Datasets &amp; Evaluation</h3>
<p><center>Datasets for AQA</center></p>
<table>
<tr>
    <th>类型</th>
    <th>名称</th>
    <th>Desc.</th>
</tr>
<tr>
    <td rowspan="8">Sport</td>
    <td>MIT-Diving</td>
    <td>60fps, 平均每个视频包含 150 帧，分数范围为 [20,100]</td>
</tr>
<tr>
    <td>MIT-Skiing</td>
    <td>24fps, 平均每个视频包含 4200 帧，分数范围为 [0,100]</td>
</tr>
<tr>
    <td>UNLV Dive & UNLV Vault</td>
    <td>平均每个视频包含 75 帧，分数范围为 [0,20]</td>
</tr>
<tr>
    <td>Basketball Performance Assessment Dataset</td>
    <td>24 Train + 24 Test，有 250 + 250 个 pair label</td>
</tr>
<tr>
    <td>AQA-7</td>
    <td>包含7种运动的视频，803 train + 303 test</td>
</tr>
<tr>
    <td>MTL-AQA</td>
    <td>16种不同类别的跳水视频，每个由7位裁判打分</td>
</tr>
<tr>
    <td>FisV-5</td>
    <td>平均时长为 2min50s，由9位裁判评判 TES & PCS</td>
</tr>
<tr>
    <td>Fall Recognition in Figure Skating</td>
    <td>276 顺利落冰 + 141 摔倒</td>
</tr>
<tr>
    <td>Medical Care</td>
    <td>包含缝合、穿针、打结三部分，有部分分和总评分</td>
</tr>
<tr>
    <td rowspan="3">Others</td>
    <td>Epic skills 2018</td>
    <td>包含揉面团、绘画、使用筷子三个子集</td>
</tr>
<tr>
    <td>BEST</td>
    <td>平均时长为 188s，包含5种不同日常活动的视频</td>
</tr>
<tr>
    <td>Infinite grasp dataset</td>
    <td>包含了94个婴儿抓东西的视频，时长在 80-500s，有 pair label</td>
</tr>
</table>

<p><center>Performance Metrics</center></p>
<p><center></p>
<table>
<tr>
    <th>Task</th>
    <th>Metric</th>
</tr>
<tr>
    <td>Regression Scoring</td>
    <td>均方误差 MSE</td>
</tr>
<tr>
    <td>Grading</td>
    <td>Classification Accuracy</td>
</tr>
<tr>
    <td>Pairwise Sorting</td>
    <td>Spearman Correlation Coefficient</td>
</tr>
</table>
<p></center></p>
<div class="admonition info">
<p class="admonition-title">Spearman Correlation Coefficient <span class="arithmatex">\(\rho\)</span></p>
<div class="arithmatex">\[
\rho = \frac{\sum_i(p_i - \overline{p})(q_i - \overline{q})}{\sqrt{\sum_i(p_i - \overline{p})^2 \sum_i(q_i - \overline{q})^2}}
\]</div>
</div>
<h3 id="4-3-models-2021">4-3 Models (截至 2021)</h3>
<ul>
<li>
<p>Medical Skill Evaluation</p>
<p>由于对医疗动作评估问题的研究早于深度学习方法的发展，大多数医疗相关的模型都使用了 <u>traditional feature</u></p>
</li>
<li>
<p>Sport AQA</p>
<p>由于起步相对较晚，体育相关的研究使用 CNN 和 RNN 实现了较好的成果</p>
<ol>
<li>
<p>based on Deep Learning</p>
<ul>
<li>
<p>通常使用 2D-CNN/3D-CNN/LSTM 来进行 feature extract &amp; aggregate</p>
</li>
<li>
<p>通过 Network Head 来适配不同类型的任务</p>
</li>
</ul>
<p>根据关注点不同可划分为: Structure Design / Loss Desgin 两类</p>
</li>
<li>
<p>based on Handcrafted Features (before 2014)</p>
</li>
</ol>
</li>
<li>
<p>Medical Care</p>
<p>由于较强的专业性，医疗领域没有一个可以作为统一 benchmark 的数据集</p>
<ul>
<li>
<p>GIT 聚焦于 OSATS 系统下的外壳手术技能评估</p>
</li>
<li>
<p>JHU 聚焦于 机器人微创手术(RMIS)</p>
</li>
<li>
<p>ASU 聚焦于 腹腔镜手术(laparoscopic surgery)</p>
</li>
</ul>
</li>
</ul>
<h3 id="4-4">4-4 发展前景</h3>
<ul>
<li>
<p>Dataset</p>
<p>目前存在的 AQA 数据集规模较小，且包含的语义信息较少</p>
<p>=&gt; 希望在未来推出更大规模、包含更多语义信息的数据集</p>
</li>
<li>
<p>Model: more Efficient &amp; Accurate</p>
<ul>
<li>
<p>更好的利用 temporal info 对动作进行建模</p>
</li>
<li>
<p>使用 Unsupervised 方法减少数据标注、降低主观影响</p>
</li>
<li>
<p>对 复杂、长期 动作的质量进行评估</p>
</li>
</ul>
</li>
</ul>
<h2 id="5-tpt">5 TPT</h2>
<h3 id="5-1-abstract">5-1 Abstract</h3>
<ul>
<li>
<p>先前的 SOTA 方法</p>
<p>使用 ranking-based pairwise comparison，或 regression-based methods </p>
<p>=&gt; 通过对 backbone 输出做 global Pooling，基于 <em>整个视频 (holistic video)</em> 进行 regression / ranking</p>
<div class="admonition bug">
<p class="admonition-title">限制了对 细粒度·类内差异 (fine-grained intra-class variation) 的捕捉</p>
</div>
</li>
<li>
<p>创新点</p>
<p>regression-based，可以在避免 part-level 监督的情况下学习更细粒度的特征</p>
<ol>
<li>
<p>使用 Temporal Parsing Transfomer 将 "整体特征(holistic)" 拆解为 temporal part-level 形式</p>
<div class="admonition example">
<p class="admonition-title">将 Diving 拆解为 approach -&gt; take off -&gt; flight 等多个阶段</p>
</div>
<ul>
<li>
<p>使用一系列 queries 来表示 <u>特定动作的 atomic temporal patterns</u></p>
</li>
<li>
<p>在 Decoding 阶段，将 frames 转换为 长度固定的·按照时序排列的 part representations</p>
</li>
<li>
<p>基于 (relative pairwise) part representations 和 对比回归 得到最终的 Quality Score</p>
</li>
</ul>
</li>
<li>
<p>提出了两种新的 Loss Function 用于 Decoder</p>
<blockquote>
<p>因为当前的数据集都没有提供 temporal part-level labels / partitions</p>
</blockquote>
<ol>
<li>
<p>Ranking Loss: Cross Attenion 阶段学习的参数符合时序</p>
</li>
<li>
<p>Sparsity Loss: 让学习的 part representation 更具 "判别性(discriminative)"</p>
</li>
</ol>
</li>
</ol>
</li>
</ul>
<div class="admonition info">
<p class="admonition-title">Temporal Action Parsing: 细粒度动作识别</p>
<ul>
<li>Zhang: Temporal Query Network adopted query-response functionality</li>
<li>Dian: TransParser</li>
</ul>
<p>上述工作均聚焦于 "frame-level" 的特征增强，而本文则侧重于提取 "更具备语义信息的·part representation"</p>
</div>
<h3 id="5-2-approach">5-2 Approach</h3>
<p><img alt="" src="../assets/TPT%20Pipeline.png" /></p>
<h4 id="overview">Overview</h4>
<ol>
<li>
<p>使用滑动窗口将 input 划分为 <span class="arithmatex">\(T \times M \text{frames}\)</span> 的 <u>重叠 Clips</u></p>
</li>
<li>
<p>对于分割得到的 Clips，使用 I3D (backbone) 处理得到 <span class="arithmatex">\(V = \{v_t \in \mathbb{R}^D\}_{t=1}^T\)</span></p>
<ul>
<li>
<p><span class="arithmatex">\(D\)</span> 为 feature dimension</p>
</li>
<li>
<p>每个 <span class="arithmatex">\(v_t\)</span> 经由空间上的 Average Pooling 得到（本模型并不关注 spatial patterns）</p>
</li>
</ul>
</li>
<li>
<p>Contrastive Regression</p>
<ol>
<li>
<p><u>Clip-Level</u> <span class="arithmatex">\(V\)</span> <span class="arithmatex">\(\rightarrow\)</span> <u>Temporal Part-Level</u> representations <span class="arithmatex">\(P =\{p_k\in \mathbb{R}^d\}_{k=1}^K\)</span></p>
<blockquote>
<p><span class="arithmatex">\(d\)</span> 为 Part Feature-dimension, <span class="arithmatex">\(K\)</span> 为 query 总数</p>
<p>操作分别对 Input &amp; Exampler 进行，生成 <span class="arithmatex">\(P, P_0\)</span></p>
</blockquote>
</li>
<li>
<p>基于 Part-aware Contrastive Regressor 计算 Part-wise Relative representations</p>
</li>
<li>Fuse Part-aware representation 以预测 Relative Score <span class="arithmatex">\(\Delta s = R(P,P_0)\)</span></li>
<li>Final Score = <span class="arithmatex">\(s + \Delta s\)</span>（<span class="arithmatex">\(s\)</span> 为 exampler 的实际分数）</li>
</ol>
</li>
</ol>
<h4 id="temporal-parsing-transformer">Temporal Parsing Transformer</h4>
<blockquote>
<p>输入：Clip-Level Representation <span class="arithmatex">\(V\)</span>，输出：quries (Part Representation)</p>
</blockquote>
<div class="admonition info">
<p class="admonition-title">区别于 DETR 的 Transformer 架构</p>
<ol>
<li>
<p>因为 Encoder 不能提高本方法的准确性，本方法 <strong>仅包含 Decoder</strong>：</p>
<p>可能是因为：</p>
<ul>
<li>
<p>clip-level self-attention smooths the temporal representations</p>
</li>
<li>
<p>本方法 cannot decode part presentations without part labels</p>
</li>
</ul>
</li>
<li>
<p>在 cross attention block 添加了参数 <span class="arithmatex">\(temperature\)</span> 以控制内积的放大（？）</p>
</li>
<li>
<p>没有把位置信息（clip id）embed 到输入数据中</p>
<p>=&gt; query 用于表示 atomic patterns 而非作为时间锚点</p>
</li>
</ol>
</div>
<p>对于 <span class="arithmatex">\(i^{th}\)</span> Decoder Layer，有：</p>
<ul>
<li>
<p>Decoder Part Feature <span class="arithmatex">\(p^{(i)}_k \in \mathbb{R}^d\)</span></p>
</li>
<li>
<p>Learnable Atomic Patterns <span class="arithmatex">\(q_k \in \mathbb{R}^d\)</span></p>
</li>
<li>
<p>embedded Clip Rrpresentation <span class="arithmatex">\(v_t \in \mathbb{R}^d\)</span></p>
</li>
</ul>
<p><center>先用 <span class="arithmatex">\(p^{(i)}_k + q_k\)</span> 得到 query，随后和 <span class="arithmatex">\(v_t\)</span> 做 cross attention 得到输出 <span class="arithmatex">\(\alpha_{k,t}\)</span></center></p>
<div class="arithmatex">\[
\alpha_{k,t} = \frac{
    \exp{((p_k^{(i)} + q_k)^T \cdot \frac{v_t}{\tau})}
}{
    \sum_{j=1}^T \exp{((p_k^{(i)} + q_k)^T \cdot \frac{v_j}{\tau})}
}
\]</div>
<ul>
<li><span class="arithmatex">\(\alpha_{k,t}\)</span> 表示 query<sup>k</sup> 和 clip<sup>t</sup> 之间的 attention value</li>
<li><span class="arithmatex">\(\tau \in \mathbb{R}\)</span> 是可调参数，用于放大内积以使得 attention 更加 discriminative.</li>
</ul>
<hr />
<p>由于本模块的目的在于：将 Clip Repre 聚合到 Part Repre 中，我们需要根据如下策略对 Part Repre 进行更新：</p>
<div class="arithmatex">\[
p^{(i)}_k += \sum_{j=1}^{T} \alpha_{k,j} v_j + p_k^{(i)}
\]</div>
<h4 id="part-aware-contrastive-regression">Part-Aware Contrastive Regression</h4>
<blockquote>
<p>在 Temporal Parsing Transformer 模块，我们已经成功将输入转化为 Part Repre <span class="arithmatex">\(P = \{p_k\}\)</span>;</p>
<p>此时我们需要对 Input 和 Exampler 的 <span class="arithmatex">\(P,P_0\)</span> 进行比较，并生成 Relative Score <span class="arithmatex">\(\Delta s\)</span></p>
</blockquote>
<div class="admonition tip">
<p class="admonition-title">我们可以分别计算每个 Part 的相对分数，最后把他们 fuse 到一块儿</p>
</div>
<p>对于 <span class="arithmatex">\(k^{th}\)</span> Part，我们通过 <em>多层感知机(MLP)</em> <span class="arithmatex">\(f_r(.)\)</span> 生成对应的 relative pairwise representation <span class="arithmatex">\(r_k \in \mathbb{R}^d\)</span>：</p>
<blockquote>
<p>所有的 Parts <u>共用一个 MLP</u></p>
</blockquote>
<div class="arithmatex">\[
r_k = f_r(\text{concat}([P_k;P_k^0]))
\]</div>
<hr />
<p>为了提高准确性，此处使用 Group-aware Regression strategy 生成 Relative Score <span class="arithmatex">\(\Delta s\)</span>：</p>
<ul>
<li>
<p>对 Training Set 中所有可能的 pair 生成 <span class="arithmatex">\(B\)</span> 个 <span class="arithmatex">\(\Delta s\)</span> 取值区间（类似 CoRe Tree 里的区间非偏区间划分策略）</p>
</li>
<li>
<p>生成 One-Hot Label <span class="arithmatex">\(\{l_n\}\)</span>，表示 <span class="arithmatex">\(\Delta s[i]\)</span> 所处的区间编号</p>
</li>
</ul>
<hr />
<p>对 Input Video 的预测：</p>
<ol>
<li>对 Relative Part Repre <span class="arithmatex">\(\{r_k\}\)</span> 使用 Average Pooling</li>
<li>使用 2 * 2-Layer MLP 对输入视频的 classification label <span class="arithmatex">\(l\)</span> &amp; 回归结果 <span class="arithmatex">\(\gamma\)</span> 进行预测</li>
</ol>
<h3 id="5-3-optimization">5-3 Optimization</h3>
<ul>
<li>
<p>假设：每一类动作都可以按照相同顺序进行阶段切分，并通过 transformer query 进行表示</p>
</li>
<li>
<p>在 Cross Attention 阶段，<span class="arithmatex">\(k^{th}\)</span> query 的 attention center <span class="arithmatex">\(\\overline{\alpha}_k\)</span>：</p>
<div class="arithmatex">\[
\overline{\alpha}_k = \sum_{t=1}^T t \cdot \alpha_{k,t} \in [1,T]
\]</div>
<ul>
<li><span class="arithmatex">\(\{\alpha_{k,t}\}\)</span> 是已经 normalized 的 attention responses</li>
<li><span class="arithmatex">\(k^{th}\)</span> query 对于所有 clip 的 attention 总和为 1，即 <span class="arithmatex">\(\sum_{t=1}^T \alpha_{k,t} = 1\)</span></li>
</ul>
</li>
</ul>
<h4 id="cross-attention-block">Cross Attention Block</h4>
<ol>
<li>
<p>Ranking Loss</p>
<p>为了鼓励各 query 聚焦于 <u>different temporal region</u>，我们对 attention center 使用 Ranking Loss</p>
<div class="admonition tip">
<p class="admonition-title">理想情况下，Part Repre 在（同类）不同视频下有 <u>相同时序</u></p>
</div>
<div class="arithmatex">\[
L_{rank} = \sum_{k=1}^{K-1} \max{(0,\ \overline{\alpha}_k - \overline{\alpha}_{k+1} + m)} + \max{(0,\ 1-\overline{\alpha}_1 + m)} + \max{(0,\ \overline{\alpha}_k - T + m)}
\]</div>
<p><center><span class="arithmatex">\(m\)</span> 是用于控制惩罚力度的超参数</center></p>
<ul>
<li>
<p>第一项用于确保顺序 <span class="arithmatex">\(\overline{\alpha}_k \lt \overline{\alpha}_{k+1}\)</span> 成立</p>
</li>
<li>
<p>后两项分别用于确定首位顺序 <span class="arithmatex">\(\overline{\alpha}_0 = 1\)</span> 和 <span class="arithmatex">\(\overline{\alpha}_{k+1} = T\)</span> 成立
（<span class="arithmatex">\(\overline{\alpha}_k \in [1,T]\)</span>）</p>
</li>
</ul>
</li>
<li>
<p>Sparsity Loss</p>
<p>鼓励每一个 query 聚焦于靠近 center <span class="arithmatex">\(\mu_k\)</span> 的那些切片：</p>
<div class="arithmatex">\[
L_{sparsity} = \sum_{k=1}^K\sum_{t=1}^T|t - \overline{\alpha}_k| \cdot \alpha_{k,t}
\]</div>
</li>
</ol>
<h4 id="contrastive-regressor">Contrastive Regressor</h4>
<p>基于比较学习的回归器需要预测 分组标签 <span class="arithmatex">\(l\)</span> &amp; 相对偏差 <span class="arithmatex">\(\gamma\)</span>，我们：</p>
<ul>
<li>
<p>对各组使用 BCE Loss</p>
<div class="arithmatex">\[
L_{cls} = - \sum_{n=1}^N [l_n \log{(\vec{l}_n)} + (1-l_n) \log{(1 - \vec{l}_n)}]
\]</div>
</li>
<li>
<p>对由 ground-truth 生成的 interval 信息使用 Square Error</p>
<div class="arithmatex">\[
L_{reg} = \sum_{n=1}^N (\gamma_n \ \vec{\gamma}_n)^2, \text{ where } l_n=1
\]</div>
</li>
</ul>
<h4 id="overall-training-loss">Overall Training Loss</h4>
<div class="arithmatex">\[
L = \lambda_{cls} L_{cls} + \lambda_{reg} L_{reg} + \lambda_{rank} \sum_{i=1}^L L^i_{rank} + \lambda_{sparsity} \sum_{i=1}^L L^i_{sparsity}
\]</div>
<h2 id="6-fspn">6 FSPN</h2>
<h3 id="6-1-abstract">6-1 Abstract</h3>
<ul>
<li>
<p>先前的工作</p>
<ul>
<li>
<p>大多数基于 粗粒度(coarse-grained)特征 进行训练、采用 holistic video representations，缺乏对 fine-grained intra-class variations 的捕捉</p>
</li>
<li>
<p>Parmar and Morris 认为所有的 sub-action sequences 对结果具有 <u>相等的贡献</u></p>
</li>
<li>
<p>segmenting action sequences along with their temporal dependence remains a challenging task：</p>
<ol>
<li>
<p>缺少预定义的 标签 &amp; action sequence 之间的关联性</p>
</li>
<li>
<p>sub-action sequences 具有非常细的粒度，动作间的变化十分平滑 =&gt; 难以确定其边界</p>
</li>
<li>
<p>由于动作十分细密、在相似的背景中进行，各 sub-action 之间有较多的共同 attributes</p>
</li>
</ol>
</li>
</ul>
</li>
<li>
<p>创新点</p>
<p>提取 fine-grained sub-action sequence 和它们的 temporal dependencies 有助于做出更准确的估计</p>
<blockquote>
<p>为了降低背景的干扰，本文使用预训练模型从 input video 中提取了 actor-centric regions </p>
</blockquote>
<ol>
<li>
<p>提出了由两部分组成的 FSPN：</p>
<ul>
<li>
<p><u>intra-sequence</u> action parsing module <span style="color:red;"><strong>无监督</strong></span></p>
<p>对更细粒度下的 sub-actions 进行挖掘</p>
<p>实现 semantical sub-action parsing，从而更准确的描述动作序列间的细微差别</p>
</li>
<li>
<p>spatiotemporal multiscale transformer module</p>
<blockquote>
<p>低阶特征缺乏语义信息，高阶特征难以对 sub-action 进行细粒度描述</p>
</blockquote>
<p>学习 <u>motion-oriented</u> action features、挖掘其在不同时间范围内的 long-range 依赖关系</p>
</li>
</ul>
</li>
<li>
<p>提出了一个 group contrastive loss</p>
</li>
</ol>
<p>此外，由于整个动作序列可能存在组件重复 ABBBBCC，模型使用了 1D Temporal Convolution + Transformer Network 来提取 single-scale feature</p>
<p>最终，各阶段特征会通过 multiscale temporal fusion 聚合生成 unified feature represen- tation，并用于最终的预测</p>
</li>
</ul>
<h3 id="6-2-relative-works">6-2 Relative Works</h3>
<ol>
<li>
<p>AQA</p>
<ul>
<li>
<p>Regression Formulation</p>
</li>
<li>
<p>Pairwise Ranking Formulation</p>
</li>
</ul>
</li>
<li>
<p>(Fine-grained) Action Parsing </p>
<ul>
<li>
<p>Zhang: Temporal Query Networks =&gt; 通过 query 找出相关的 segments</p>
</li>
<li>
<p>Dian: TransParser =&gt;  对 sub-action 进行挖掘（无监督）</p>
</li>
</ul>
</li>
<li>
<p>Vision Transformer</p>
<p>从 低分辨率图片 &amp; 较小的通道数量 开始，逐渐增加通道并减少 spatial resolution</p>
</li>
</ol>
<h3 id="6-3-approach">6-3 Approach</h3>
<p><img alt="" src="../assets/FSPN%20Pipeline.png" /></p>
<h4 id="_2">问题定义</h4>
<p>对于给定 input video <span class="arithmatex">\(x_i \in \mathbb{R}^{T \times H \times W \times C}\)</span> 及其对应的分数标签 <span class="arithmatex">\(y_i\)</span>，AQA 问题可以认为是一个回归问题：</p>
<blockquote>
<p>T, H, W, C 分别为 clip 长度、视频宽高、通道数</p>
</blockquote>
<ol>
<li>
<p>使用预训练模型 <span class="arithmatex">\(D(.)\)</span> 得到运动员所在的 BBox <span class="arithmatex">\(x_a\)</span></p>
<div class="arithmatex">\[
    x_a = D(x_i) \in \mathbb{R}^{T \times H \times W \times C}
\]</div>
</li>
<li>
<p>将 原始输入 和 BBox 都输入 (相同的)I3D 来提取 spatiotemporal 特征 <span class="arithmatex">\((f_i, f_a)\)</span></p>
<div class="arithmatex">\[
f_i = E_v(x_i),\ f_a = E_v(x_a)
\]</div>
</li>
<li>
<p>使用 FSPN <span class="arithmatex">\(\mathbb{F}_\Theta(.)\)</span> 提取特征，并最终进行回归运算</p>
<div class="arithmatex">\[
\overline{y}_i = R_{\theta}(\mathbb{F}_\Theta(E_v(x_i)),\mathbb{F}_\Theta(E_v(x_a)))
\]</div>
</li>
</ol>
<h4 id="intra-sequence-action-parsing">Intra-Sequence Action Parsing</h4>
<h5 id="1-intra-sequence-action-parsing-iap">1 Intra-Sequence Action Parsing (IAP)</h5>
<div class="admonition tip">
<p class="admonition-title">确定每个 sub-action 的 起始帧 &amp; 结束帧</p>
</div>
<ul>
<li>
<p>给出的 Parser 可以对 <span class="arithmatex">\(S\)</span> 个 sub-action 的分布概率进行预测，同时指出 “转变” 发生的具体帧编号 <span class="arithmatex">\(f^{th}\)</span>：</p>
<p>features -&gt; probability vec <span class="arithmatex">\(A_s\)</span> (对 <span class="arithmatex">\(s^{th}\)</span> sub-action 的 middle-level 表示)</p>
<div class="arithmatex">\[
[A_1, ..., A_s] = IAP(f_i,f_a)
\]</div>
</li>
<li>
<p>使用 up-sampling decoder + MLP layers projection head 构建 “分布概率预测器”</p>
<ul>
<li>
<p>上采样包含四个 spatial-temporal dimensions 分别为：(1024, 12), (512, 24), (256, 48), and (128, 96) 的子块</p>
<ul>
<li>
<p>temporal axis 会被卷积操作扩充</p>
</li>
<li>
<p>spatial dimensions 会被 Max Pooling 削减</p>
</li>
</ul>
</li>
<li>
<p>使用了 3 Layer MLP</p>
</li>
</ul>
</li>
<li>
<p><span class="arithmatex">\(A_s(\vec{t})\)</span> 表示 <span class="arithmatex">\(t^{th}\)</span> 帧可能对应的 sub-action 概率分布；<span class="arithmatex">\(\vec{t}_s\)</span> 是对 <span class="arithmatex">\(s^{th}\)</span> 跳 action sequence 的预测结果</p>
<blockquote>
<p><span class="arithmatex">\(\text{argmax } A_s(\vec{t})\)</span> 即为该帧最可能对应的 sub-action 类型</p>
<p>此时 t 与 t+1 必然对应不同的 sub-aciton =&gt; 新的 sub-action instance 从 <span class="arithmatex">\((t+1)^{th}\)</span> 帧开始</p>
</blockquote>
<div class="arithmatex">\[
\vec{t}_s = \text{argmax } A_s(\vec{t}),\ \frac{T}{S}(s-1) \leq \vec{t} \leq \frac{T}{S}s
\]</div>
<p>上式保证了 <span class="arithmatex">\(\vec{t}_1 \leq ... \leq \vec{t}_s\)</span></p>
</li>
</ul>
<h5 id="2-group-contrastive-learning">2 Group Contrastive Learning</h5>
<ul>
<li>
<p>上一步得到的 <span class="arithmatex">\((f_i,f_a)\)</span> 共享了较多的语义信息，直接对其进行比较学习会导致 <u>对具有相同语义的动作序列学习得到不同的表示</u></p>
<p>=&gt; 使用 Group Contrastive Learning，对具有相似 sub-action seq 的视频进行对比</p>
</li>
<li>
<p>输入的 <span class="arithmatex">\((f_i,f_a)\)</span> 会被</p>
<ul>
<li>赋予与其动作、语义具有最大相似度组别的 Pseudo Label <span class="arithmatex">\(p\)</span></li>
<li>最终生成 sub-action sequence <span class="arithmatex">\((\overline{f}_i,\overline{f}_a)\)</span></li>
</ul>
</li>
<li>
<p>具有相同 Pseudo Label 的 feature 将组成如下的 group：</p>
<div class="arithmatex">\[
G^p_k = \frac{\sum_{i=1}^B g(A_s^k)}{T_B},\ \text{where } p=A_s^k
\]</div>
<ul>
<li>
<p><span class="arithmatex">\(g(.)\)</span> 是序列 <span class="arithmatex">\(A_s^k\)</span> 的 logits</p>
</li>
<li>
<p><span class="arithmatex">\(T_B\)</span> 是 Group <span class="arithmatex">\(B\)</span> 的序列总量</p>
</li>
</ul>
</li>
<li>
<p>我们使用 Group 的 average representation <span class="arithmatex">\(G_f^g\)</span>，定义：</p>
<ul>
<li>
<p>positive-pair：<span class="arithmatex">\(G_a^p, G_i^p\)</span> （上标组别相同）</p>
</li>
<li>
<p>negative-pair：<span class="arithmatex">\(G_a^p, G_k^q\)</span> （上标组别不同）</p>
<blockquote>
<p>Action Seq 相同，但 Sub-action Seq 不同</p>
</blockquote>
</li>
</ul>
</li>
</ul>
<h4 id="spatio-temporal-multiscale-transformer">Spatio-Temporal Multiscale Transformer</h4>
<div class="admonition tip">
<p class="admonition-title">输入 sub-action sequence <span class="arithmatex">\((\overline{f}_i,\overline{f}_a)\)</span>，并在不同 scale 上挖掘 long-range dependencies</p>
</div>
<h5 id="1-actor-centric-multiscale-transformer">1 Actor-Centric Multiscale Transformer</h5>
<ul>
<li>
<p>本文提出的 Transformer 各阶段具有各异的 channel resolution =&gt; channel &amp; scale 逐渐增加</p>
</li>
<li>
<p>该 Multiscale Transformer，由 3 * stage（结构相同）构成</p>
<ul>
<li>
<p>每个 stage </p>
<ul>
<li>
<p>在 early Layer 处理粗粒度特征，并在更深层处理细粒度特征</p>
</li>
<li>
<p>各包含了 3 个 transformer block + 8 attention head，用于处理相同 scale 的信息并生成 Attention 值</p>
<div class="arithmatex">\[
\hat{f}_i += MLP(LN(attention)), attention = Multihead(LN(\frac{Q_a(K_i)^T}{\sqrt{d_k}})V_i) + \overline{f}_i
\]</div>
<p><center><span class="arithmatex">\(Ln(.)\)</span> 为 Layer Norm 操作，<span class="arithmatex">\(MLP\)</span> 为两层使用 GELU 激活函数</center></p>
<p><center><img src="../assets/FSPN%20block.png" style="max-height:250px"></center></p>
<p><center>block 结构：<span class="arithmatex">\(\overline{f}_a = query, \overline{f}_i=memory\)</span></center></p>
</li>
</ul>
</li>
<li>
<p>对于输入特征 <span class="arithmatex">\(f \in \mathbb{R}^{T' \times C'}\)</span>:</p>
<ol>
<li>
<p>进行一次 1D 卷积 (kernel=3, stride=1)</p>
</li>
<li>
<p>使用包含 <span class="arithmatex">\(L\)</span> 个 block 的 Multiscale Transformer，其中 stage n 的 output_shape = <span class="arithmatex">\(T' \times 2^nC'\)</span>（放大）</p>
<blockquote>
<p>channel dimesion 会在 stage 切换时通过 MLP 扩大 2 倍</p>
</blockquote>
</li>
</ol>
</li>
</ul>
</li>
</ul>
<h5 id="2-multiscale-temporal-fusion">2 Multiscale Temporal Fusion</h5>
<p><center><img src="../assets/FSPN%20fusion.png" style="max-width:500px;"></center></p>
<p>对于第 <span class="arithmatex">\(n\)</span> 层的 output shape 为 <span class="arithmatex">\(F_n = T' \times 2^nC'\)</span>，而第 <span class="arithmatex">\(n+1\)</span> 层则为 <span class="arithmatex">\(F_{n+1}= T' \times 2^{n+1}C'\)</span></p>
<ul>
<li>
<p>为了成功进行 aggregate，我们需要进行 upsampling: <span class="arithmatex">\(U_\varphi(F_n) = \text{Upsampling}(R_n\mathcal{W}^n)\)</span></p>
</li>
<li>
<p>随后使用 element-wise addition 更新 <span class="arithmatex">\(F_{n+1} = U_\varphi(F_n) + F_{n+1}\mathcal{W}^{n+1}\)</span></p>
</li>
</ul>
<p>最终通过 fuse 的到的 intergrated feature <span class="arithmatex">\(\mathcal{F} = \text{Concat}(F_1, ..., F_{N})\)</span></p>
<h3 id="6-4-optimization">6-4 Optimization</h3>
<h4 id="overall-training-loss_1">Overall Training Loss</h4>
<p>最终将由 2-Layer MLP 对 <span class="arithmatex">\(\text{MaxPool}(\mathcal{F})\)</span> 预测得到分组标签 <span class="arithmatex">\(\gamma_i\)</span> 和 回归分数 <span class="arithmatex">\(y_i\)</span></p>
<ul>
<li>
<p>对于回归预测，有：</p>
<div class="arithmatex">\[
\begin{align*}
L_{bce} &amp;= - \sum_{i=1}^I(\gamma_i log(\overline{\gamma_i}) + (1-\gamma_i)log(1-\overline{\gamma}_i)) \\
L_{reg} &amp;= \sum_{i=1}^I \| \overline{y}_i - y_i\|^2,\ \text{where } \gamma_i = 1
\end{align*}
\]</div>
</li>
<li>
<p>对 Group Contrastive，有：</p>
<div class="arithmatex">\[
L_{gc} = - \log{\frac{
    h(G_a^p,G_i^p) / \tau
}{
    h(G_a^p,G_i^p) / \tau + \sum_{q=1,k}^S  h(G_a^p,G_k^q) 
}}, \text{ where }p\neq q
\]</div>
<ul>
<li>
<p><span class="arithmatex">\(h(.) = \exp{(\text{cosine similarity})}\)</span> </p>
</li>
<li>
<p><span class="arithmatex">\(\tau\)</span> 是 teperature 超参数</p>
</li>
</ul>
</li>
</ul>
<h2 id="7-iris">7 IRIS</h2>
<div class="admonition info">
<p class="admonition-title">本文聚焦的问题：(花滑)单人短节目</p>
<ul>
<li>
<p>节目时长约为 3min</p>
</li>
<li>
<p>相比于自由滑，节目编排受制于更多的规则</p>
</li>
</ul>
</div>
<h3 id="7-1-abstract">7-1 Abstract</h3>
<ul>
<li>
<p>创新点</p>
<blockquote>
<p>XAI 开发者应该学习 <u>具体应用场景（运动）的评分标准(rubrics)</u></p>
</blockquote>
<p>本文提出的 Interpretable Rubric- Informed Segmentation：</p>
<ul>
<li>
<p>其给出评分的过程是 <strong>可解释</strong> 的，依照 rubric 决定 what to consider</p>
<p>使用 4 个 key feature 进行 预测&amp;解释: Score, Sequence, Segments, and Subscores</p>
</li>
<li>
<p>对输入进行分割，以定位对应特定 criteria 的特殊 section</p>
</li>
<li>
<p>本文仅对 <em>figure skating</em> 进行测定，但方法同样适用于其他 Video-based AQA 问题</p>
</li>
</ul>
</li>
</ul>
<h3 id="7-2-relative-works">7-2 Relative Works</h3>
<ol>
<li>
<p>Action Quality Assessment</p>
<p>目前已经有许多针对 Sport 及其他领域的 AQA AI 解决方案被提出，通过训练一个 Regression Model 对最终分数进行预测。</p>
<ul>
<li>
<p>这些方法往往通过 3D CNN (C3D / I3D) 进行特征提取 —— 通过在 2D(spatial) &amp; 1D(temporal) 进行卷积，将<strong>较短的</strong>视频切片转换为 feature vector</p>
</li>
<li>
<p>为了处理<strong>更长的</strong>输入视频：</p>
<ul>
<li>
<p>Parmar 将 3DCNN 和 LSTM 结合，提出了 C3D-LSTM</p>
</li>
<li>
<p>Zheng 基于 Graph Convolutional Network 提出了 Context-aware 的模型</p>
<p>对 Static Posture &amp; Dynamic Movement 进行建模来捕捉不同时间跨度上的联系</p>
</li>
<li>
<p>Nekoui 提出了一种 CNN-based 的方法同时对粗细粒度的 temporal dependencies 进行捕捉</p>
<p>使用 video feature &amp; pose estimation heatmap，并堆叠不同 kernel size 的 CNN 模块来捕捉不同时间跨度的 pattern 进行捕捉</p>
</li>
<li>
<p>Xu 针对 <em>figure skating</em> 场景提出了 multi-scale and skip-connected CNN-LSTM</p>
<p>通过不同大小的 CNN kernel 对短期 temporal dependencies 进行捕捉 &amp; 通过 LSTM + self-attention 对长期 temporal dependencies 进行捕捉</p>
</li>
</ul>
</li>
</ul>
</li>
<li>
<p>Explaining Action Quality Assessment</p>
<p>虽然上述方法卷赢了准确性（预测结果和裁判评分之间的 Spearman’s rank correlation），但却忽略了用户应该如何应用和解释这些预测结果</p>
<blockquote>
<p>老师 chua 的一下给你的卷子批了个总评分，又不告诉你扣哪了</p>
</blockquote>
<ul>
<li>
<p>Yu 通过 Grad-CAM saliency maps 对 <em>diving</em> 问题的评分过程进行解释</p>
<p>=&gt; 但 saliency maps 的本意是拿来 debug 的 orz</p>
</li>
<li>
<p>Pirsiavash 通过计算 由姿态估计得到的 relative score 的梯度 来对 <em>diving</em> 问题的评分过程进行解释</p>
<p>=&gt; 基于数据（而非基于人类裁判如何做出裁决）</p>
</li>
<li>
<p>使用数据传感器 + 浅层模型的方法</p>
<ul>
<li>
<p>Khan 借鉴了电子板球游戏，使用可穿戴设备收集数据并对板球击球数据进行分析，并人为规定了若干 low-level sub-actions</p>
</li>
<li>
<p>Thompson 在量化评分标准后，提出了对 “盛装舞步” 项目的可视化方法</p>
</li>
</ul>
</li>
</ul>
</li>
<li>
<p>Rubrics for Figure Skating</p>
<ul>
<li>
<p>“评分标准” 的两要素：</p>
<ol>
<li>
<p><span class="arithmatex">\(\geq 1\)</span> * trait / dimension + 对应的解释案例</p>
</li>
<li>
<p>各 dimension 的 评分范围 / 分段标准</p>
</li>
</ol>
</li>
<li>
<p>ISU Judging System: 对每个维度给 sub-score，最后折算总分</p>
<ol>
<li>
<p>TES（技术得分）</p>
<p>运动员的技术动作序列将被提前列出，每个动作的得分 = Base Value(难度分) + GOE(偏差值)</p>
<blockquote>
<p><span class="arithmatex">\(GOE \in [-5,5]\)</span></p>
</blockquote>
<ul>
<li>
<p>动作序列将由：Jump, Spin, Step Seq 及之间的过渡动作组成</p>
</li>
<li>
<p>Jump 可被分为六种：Toe Loop (T), Salchow (S), Loop (Lo), Flip (F), Lutz (Lz), and Axel (A)</p>
<p>根据起跳、落冰动作进行区分，不同的圈数会给不同的分</p>
</li>
<li>
<p>Rotation 可被分为三种：Upright (USp), Sit (SSp), and Camel (CSp)</p>
<p>根据难度、流畅度、稳定度进行评分</p>
</li>
<li>
<p>Step Sequence 可被分为三种：Straight line step sequence (SISt), Circular step sequence (CiSt), and Serpentine step sequence (SeSt)</p>
<p>步伐必须与音乐相匹配，并且干净利落的完成shuo ta</p>
</li>
</ul>
</li>
<li>
<p>PCS (Program Component Score) 由五部分组成：</p>
<ul>
<li>
<p>Skating Skills: 对节目内容的丰富程度、技术是否干净、滑速进行评分</p>
</li>
<li>
<p>Tran- sition / Linking Footwork: 对整体步法和姿态转变流畅度进行评分</p>
</li>
<li>
<p>Performance / Execution: 是否从肢体动作和情绪上传达了配乐的情感</p>
</li>
<li>
<p>Choreography / Composition: 节目动作编排是否与音乐契合</p>
</li>
<li>
<p>Interpretation: 用于表达音乐的动作是否具有创新性</p>
</li>
</ul>
</li>
</ol>
</li>
</ul>
</li>
</ol>
<h3 id="7-3-approach">7-3 Approach</h3>
<p><center><img src="../assets/IRIS%20Pipeline.png" style="max-width:500px;"></center></p>
<details class="info">
<summary>事前可解释性建模 Ante-hoc</summary>
<blockquote>
<p>IRIS 在对模型进行训练前，将可解释性结合到模型结构中，以实现模型内置的可解释性</p>
</blockquote>
<ul>
<li>
<p>It is informed by a rubric to determine what to consider when calculating its judgement.</p>
</li>
<li>
<p>It performs seg- mentation to specify moments of a performance to judge specific criteria.</p>
</li>
</ul>
</details>
<p>IRIS 使用了 4 个 Rubric-Feature 以预测和解释其作出的判断：</p>
<ol>
<li>
<p>Score: Final Judgement</p>
</li>
<li>
<p>Sequence: 描述了用于评判的动作 (technical elements)</p>
</li>
<li>
<p>Segments: 每个动作由哪一条规则评判</p>
</li>
<li>
<p>Subscores: 描述 TES &amp; PCS 各项得分</p>
</li>
</ol>
<h4 id="0-data-preparation">0 Data Preparation</h4>
<blockquote>
<p>使用 MIT-Skate 数据集</p>
</blockquote>
<ul>
<li>
<p>将 3min 左右的视频转化为 4D tensor: <code>[x, y, time, colorChannel]</code></p>
</li>
<li>
<p>提取 ISU 发布的 PDF 文件中的 skater name, TES base, GOE, total subscores, PCS subscore。将其与视频数据相对应。</p>
</li>
<li>
<p><span style="color:red;">手动</span> 标记每个动作的起止时间 （train segmentation）</p>
</li>
<li>
<p>为了确保每个类别下的都有充足的数据，这里队 Jump, Spin, Step Sequence, Transition 的划分较为宽泛</p>
<blockquote>
<p>建议 Future Work 能够用更多数据来划分 more specific labels</p>
</blockquote>
</li>
</ul>
<h4 id="1-base-embedding">1 Base Embedding</h4>
<blockquote>
<p>生成 video 的 vector presentation</p>
</blockquote>
<p>训练 3D-CNN（I3D） 模型 <span class="arithmatex">\(M_0\)</span>：</p>
<ul>
<li>
<p>输入: video tensor <span class="arithmatex">\(x\)</span></p>
</li>
<li>
<p>输出: timeseries embedding <span class="arithmatex">\(\hat{z}_t\)</span>: 2D tensor</p>
<p><span class="arithmatex">\(M_0\)</span> 会对每 0.534s 生成一个 vector embedding，每个视频将对应约 356 个（zero-padding）</p>
</li>
</ul>
<h4 id="2-action-segments">2 Action Segments</h4>
<ul>
<li>
<p>训练 multi-stage temporal convolutional network (MS-TCN) <span class="arithmatex">\(M_t\)</span>：</p>
<ul>
<li>
<p>训练集：人工标注的 segments 作为 ground truth (监督学习)</p>
</li>
<li>
<p>输入：<span class="arithmatex">\(\hat{z}_t\)</span></p>
</li>
<li>
<p>输出：action sequence embedding <span class="arithmatex">\(\hat{z}_{\tau}\)</span> (Seq-to-Seq)</p>
<p>标注了每个 action 的起止时间 (zero-padding)</p>
</li>
</ul>
</li>
<li>
<p>由于 TCN 可能预测出 over-segmentation，本文使用了以下方法进行优化</p>
<ol>
<li>
<p>使用 truncated mean squared err 对 loss function 进行平滑操作</p>
<blockquote>
<p>合并较小的 segmentation</p>
</blockquote>
<div class="arithmatex">\[
L_{\mu} = \frac{1}{T} \sum_t^T \max(\varepsilon_t, \varepsilon)
\]</div>
<ul>
<li>
<p><span class="arithmatex">\(\varepsilon_t = (\log\hat{m}(t)- \log\hat{m}(t-1))^2\)</span></p>
</li>
<li>
<p><span class="arithmatex">\(\varepsilon\)</span>: truncation 超参数</p>
</li>
</ul>
</li>
<li>
<p>使用启发式的方法</p>
<ol>
<li>
<p>从裁判表中数处每个动作类别 <span class="arithmatex">\(a\)</span> 包含的动作数量，总量为 <span class="arithmatex">\(n_a\)</span> 个</p>
</li>
<li>
<p>使用 <u>最长的</u> <span class="arithmatex">\(n_a\)</span> 个 segmentation，将其分配给各类别</p>
</li>
<li>
<p>剩下的较短 segmentations 全都打上 <code>transition</code> 标签</p>
</li>
</ol>
</li>
</ol>
</li>
</ul>
<h4 id="3-predict-subscores">3 Predict Subscores</h4>
<p>IRIS 预测的 subscores 分为以下两个部分：</p>
<ol>
<li>
<p>TES (7): 对每个技术得分点进行评估</p>
<ol>
<li>
<p>对 action sequence 中的每一个动作 <span class="arithmatex">\(\tau\)</span> 分别预测 TES（仅 GOE 部分）</p>
<p>为每个 seq step <span class="arithmatex">\(\tau\)</span> 各训练一个传统 CNN 模型 <span class="arithmatex">\(M_{\Delta \tau}\)</span>:</p>
<ul>
<li>
<p>输入：seq embedding <span class="arithmatex">\(\hat{z}_{\tau}\)</span> &amp; 真实动作标签 <span class="arithmatex">\(a_{\tau}\)</span>(可以预先得知)</p>
</li>
<li>
<p>输出：单个动作的 GOE 得分 <span class="arithmatex">\(\hat{y}_{\Delta \tau}\)</span></p>
</li>
</ul>
</li>
<li>
<p>计算单个动作的 TES 得分 <span class="arithmatex">\(\hat{y}_{\tau} = \hat{y}_{\tau_0} + \hat{y}_{\Delta \tau}\)</span> (Base + GOE)</p>
</li>
<li>
<p>计算所有动作的总 TES 得分 <span class="arithmatex">\(\hat{y}_{total} = \sum_r \hat{y}_r\)</span></p>
</li>
</ol>
</li>
<li>
<p>PCS (5): 对节目 <em>整体表现</em> 进行评估</p>
<p>训练 multi-task CNN 模型 <span class="arithmatex">\(M_{\pi}\)</span>：</p>
<blockquote>
<p>因为各项总评之间存在 correlation，训练 multi-task 比训练 multi-indipendent 更准确</p>
</blockquote>
<ul>
<li>
<p>输入： <strong>整个视频</strong> 的 time series embedding <span class="arithmatex">\(\hat{z}_t\)</span></p>
</li>
<li>
<p>输出：预测 PCS 的各分量 <span class="arithmatex">\(\hat{y}_p^i\)</span> (multi-task)，将其和 <span class="arithmatex">\(\hat{y}_{\pi}\)</span> 作为总 PCS</p>
</li>
</ul>
</li>
</ol>
<h4 id="4-predict-final-score">4 Predict Final Score</h4>
<p>简单的将 TES &amp; PCS 总分相加即可：</p>
<div class="arithmatex">\[
\hat{y} = \hat{y}_{total} + \hat{y}_p
\]</div>
<p><center><img src="../assets/IRIS%20visual.png" style="max-width:500px;"></center>
<center>IRIS 评分过程可视化</center></p>
<h3 id="7-4-evaluation">7-4 Evaluation</h3>
<ol>
<li>
<p>计算 Final Score 的 Spearman Rank Correlation</p>
</li>
<li>
<p>(New) 分别计算 TES &amp; PCS 的 Spearman Rank Correlation</p>
</li>
<li>
<p>Dice Coefficient: 计算 segmentation 的准确性</p>
<p>对预测分割序列 <span class="arithmatex">\(\hat{a}\)</span> 和真实动作序列 <span class="arithmatex">\(a\)</span>，计算其交叠程度 <span class="arithmatex">\(2(a \cdot \hat{a})/(|a|^2 + |\hat{a}|^2)\)</span></p>
</li>
</ol>












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
    
      
      <nav class="md-footer__inner md-grid" aria-label="页脚" >
        
          
          <a href=".." class="md-footer__link md-footer__link--prev" aria-label="上一页: Home">
            <div class="md-footer__button md-icon">
              
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
            </div>
            <div class="md-footer__title">
              <span class="md-footer__direction">
                上一页
              </span>
              <div class="md-ellipsis">
                Home
              </div>
            </div>
          </a>
        
        
          
          <a href="../rep/" class="md-footer__link md-footer__link--next" aria-label="下一页: Repitition">
            <div class="md-footer__title">
              <span class="md-footer__direction">
                下一页
              </span>
              <div class="md-ellipsis">
                Repitition
              </div>
            </div>
            <div class="md-footer__button md-icon">
              
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11H4Z"/></svg>
            </div>
          </a>
        
      </nav>
    
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
    <div class="md-copyright__highlight">
      Copyright © 2023 SeaBee
    </div>
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    <script id="__config" type="application/json">{"base": "..", "features": ["navigation.footer", "navigation.expand", "content.code.copy", "content.code.annotate"], "search": "../assets/javascripts/workers/search.f886a092.min.js", "translations": {"clipboard.copied": "\u5df2\u590d\u5236", "clipboard.copy": "\u590d\u5236", "search.result.more.one": "\u5728\u8be5\u9875\u4e0a\u8fd8\u6709 1 \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.more.other": "\u5728\u8be5\u9875\u4e0a\u8fd8\u6709 # \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.none": "\u6ca1\u6709\u627e\u5230\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.one": "\u627e\u5230 1 \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.other": "# \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.placeholder": "\u952e\u5165\u4ee5\u5f00\u59cb\u641c\u7d22", "search.result.term.missing": "\u7f3a\u5c11", "select.version": "\u9009\u62e9\u5f53\u524d\u7248\u672c"}}</script>
    
    
      <script src="../assets/javascripts/bundle.d7c377c4.min.js"></script>
      
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.min.js"></script>
      
        <script src="../termynal.js"></script>
      
    
  </body>
</html>